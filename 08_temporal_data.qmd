# Temporal Data {#sec-ch08}

```{python}
#| include: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from plotnine import *
from datetime import datetime, date, time
import pytz
from dateutil import parser
import warnings
warnings.filterwarnings('ignore')

# Set locale for consistent date formatting
import locale
try:
    locale.setlocale(locale.LC_TIME, 'en_US.UTF-8')
except:
    pass  # Fallback if locale not available
```

## Introduction

For the second edition of this book, we chose to add this chapter on temporal data.
Being able to analyze change over time is key to many kinds of humanities data. 
For example, we might have a series of letters with dates,
photographs with time stamps, and a film with shot and scene times that we are excited
to study. These kinds of data animate fields such as history and media studies.

In this chapter, we discuss techniques for working with data that has some
temporal component. This includes any kind of data that has one or more variables
that record dates or times, as well as any dataset that has general meaningful
ordering of its rows. For example, the annotation object that we created
for textual data in Chap. 6 has a meaningful ordering to it and can
be treated as having a temporal ordering even if it is not associated
specifically with fixed timestamps. We will start by focusing specifically on
datasets that contain explicit information about dates and times. In the later
sections we will illustrate window functions and range joins, both of which 
have a wider set of applications to all ordered datasets.

As we saw in Chap. 5, it is possible to store information about dates
and times in a tabular dataset. There are many different formats for storing
this information; we recommend that most users start by recording these with
separate columns for each numeric component of the date or time. This makes it
easier to avoid errors and to record partial information, the latter being a
common complication of many humanities datasets. We will begin by looking at a
dataset related to the Wikipedia pages we saw in the previous two chapters that
has date information stored in such a format.

In showing the application of line graphs in Chap. 2 and again in
Chap. 5, we saw how to visualize the dataset of food prices over 
a 140-year period. This visualization was fairly straightforward. There was
exactly one row for each year. We were able to treat the year variable
has any other continuous measurement, with the only change being that it made
sense to connect dots with a line when building the visualization. Here we 
will work with a slightly more complex example corresponding to the Wikipedia
pages from the preceding chapters.

## Temporal Data and Ordering

Let's start by loading some data with a temporal component in Python. Below, we will read in
data related to the 75 Wikipedia pages from a selection of British authors.
Here, we have a different set of information about the pages than we used in
the text and network analysis chapters. For each page, we have grabbed page
view statistics for a 60-day period from Wikipedia. In other words, we have a
record of how many people looked at a particular page each day, for each 
author. The data are organized with one row for each combination of item and
day.

```{python}
page_views = pd.read_csv("data/wiki_uk_page_views.csv")
page_views
```

The time variables are given the way we recommended in Chap. 5, with
individual columns for year, month, and day. Here, our dataset is already
ordered (within each item type) from the earliest records to the latest. If
this were not the case, because all of our variables are stored as numbers, we 
could use the `sort_values()` function to sort by year, followed by month, followed
by day, to get the same ordering.

How could we show the change in page views over time for a particular variable?
One approach is to add a numeric column running down the dataset using the index.
Below is an example of the code to create a line plot using the row number approach:

```{python}
# Filter for Geoffrey Chaucer and create row numbers
chaucer_data = (page_views
    .query("doc_id == 'Geoffrey Chaucer'")
    .reset_index(drop=True)
    .reset_index()
    .rename(columns={'index': 'row_number'})
)

p = (ggplot(chaucer_data, aes(x='row_number', y='views')) +
     geom_line(color='red') +
     labs(title="Page Views for Geoffrey Chaucer (by Day Number)",
          x="Day Number", y="Views"))
p
```

In this case, our starting plot is not a bad place to begin. The x-axis
corresponds to the day number, and in many applications that may be exactly
what we need. We can clearly see that the number of page views for Chaucer has
a relatively stable count, possibly with some periodic swings over the course of
the week. There is one day about two-thirds of the way through the plot in which
the count spikes. Notice, though, that it is very hard to tell anything from the
plot about exactly what days of the year are being represented. We cannot easily
see which day has the spike in views, for example. Also, note that the
correspondence between the row number and day only works because the data
are uniformly sampled (one observation each day) and there is no missing data.

Another way to work with dates is to convert the data to a fractional year
format. Here, the months and days are added to form a fractional day. A quick
way to do this is to compute the following fractional year:

$$ year_{frac} = year + \frac{month - 1}{12} + \frac{day - 1}{12 \cdot 31}$$

We are subtracting one from the month and day so, for example, on a date such
as 1 July 2020 (halfway through the year) we have the fractional year equal to
`2020.5`. We could make this even more exact by accounting for the fact that
some months have fewer than 31 days, but as a first pass this works relatively
well. We can see the output with the following code:

```{python}
# Create fractional year representation
chaucer_fractional = (page_views
    .query("doc_id == 'Geoffrey Chaucer'")
    .assign(year_frac = lambda df: df['year'] + (df['month'] - 1) / 12 + (df['day'] - 1) / (12 * 31))
)

p = (ggplot(chaucer_fractional, aes(x='year_frac', y='views')) +
     geom_line(color='red') +
     labs(title="Page Views for Geoffrey Chaucer (Fractional Year)",
          x="Fractional Year", y="Views"))
p
```

This revised visualization improves on several aspects of the original plot.
For one thing, we can roughly see exactly what dates correspond to each data
point. Also, the code will work fine regardless of whether the data are sorted,
evenly distributed, or contain any missing values. As a down-side, the axis
labels take some explaining. We can extend the same approach to working with
time data. For example, if we also had the (24-hour) time of our data points
the formula would become:

$$ year_{frac} = year + \frac{month - 1}{12} + \frac{day - 1}{12 \cdot 31} + \frac{hour - 1}{24 \cdot 12 \cdot 31}$$

If we are only interested in the time since a specific event, say the start of
an experiment, we can use the same approach but take the difference relative to
a specific fractional year.

Fractional times have a number of important applications. Fractional times are
convenient because they can represent an arbitrarily precise date or date-time
with an ordinary number. This means that they can be used in other models and
applications without any special treatment. They may require different model
assumptions, but at least the code should work with minimal effort. 
This is a great way to explore our data. However,
particularly when we want to create nice publishable visualizations, it can be
useful to work with specific functions for manipulating dates and times.

## Date Objects

Most of the variables that we have worked with so far are either strings or numbers. 
Dates are in some ways like numbers: they have a
natural ordering, we can talk about the difference between two numbers, and it
makes sense to color and plot them on a continuous scale. However, they do have
some unique properties, particularly when we want to extract information such
as the day of the week from a date, that require a unique data type. To create
a date object in Python, we can use pandas' `to_datetime()` function or the
`datetime` module directly.

```{python}
# Create date objects using pandas
chaucer_with_dates = (page_views
    .query("doc_id == 'Geoffrey Chaucer'")
    .assign(date = lambda df: pd.to_datetime(df[['year', 'month', 'day']]))
)

chaucer_with_dates
```

Notice that the new column has a special data type: `datetime64[ns]`. If we build a
visualization using a date object, plotnine is able to make helpful built-in choices about
how to label the axis. For example, the following code will make a line plot 
that has nicely labeled values on the x-axis.

```{python}
p = (ggplot(chaucer_with_dates, aes(x='date', y='views')) +
     geom_line(color='red') +
     labs(title="Page Views for Geoffrey Chaucer",
          x="Date", y="Views"))
p
```

The output shows that the algorithm decided to label the dates appropriately. We can manually change the
frequency of the labels using `scale_x_datetime()` and setting the `date_breaks`
option. For example, the code below will display one label for each week:

```{python}
p = (ggplot(chaucer_with_dates, aes(x='date', y='views')) +
     geom_line(color='red') +
     scale_x_datetime(date_breaks='1 week', date_labels='%Y-%m-%d') +
     theme(axis_text_x=element_text(angle=45, hjust=1)) +
     labs(title="Page Views for Geoffrey Chaucer (Weekly Labels)",
          x="Date", y="Views"))
p
```

Once we have a date object, we can also extract useful information from it. For
example, we can extract the weekday of the date using pandas' datetime accessor. 
Here, we will compute the weekday and then calculate the average number of page views for
each day of the week:

```{python}
# Extract weekday and compute average views by day of week
weekday_analysis = (chaucer_with_dates
    .assign(weekday = lambda df: df['date'].dt.day_name())
    .groupby('weekday')['views']
    .mean()
    .reset_index()
    .sort_values('views', ascending=False)
)

weekday_analysis
```

Here we see the average number of page views by day of the week. We can also use the date
objects to filter the dataset. For example, we can filter the dataset to only
include those dates after 15 January 2020, about two-thirds of the way through
our dataset:

```{python}
# Filter by date
filtered_data = chaucer_with_dates.query("date > '2020-01-15'")
filtered_data
```

Note that we can use string representations of dates in filtering operations,
and pandas will automatically convert them to the appropriate datetime objects.

## Datetime Objects

The `page_views` dataset records the date of each observation. Sometimes we have
data that describes the time of an event more specifically in terms of hours,
minutes, and even possibly seconds. We will use the term *datetime* to describe
an object that stores the precise time that an event occurs. The idea is that
to describe the time that something happens we need to specify a date and a
time. Later in the chapter, we will see an object that stores time without
reference to a particular day. Whereas dates have a natural precision (a single
day), we might desire to work with datetime objects of different levels of 
granularity. In some cases we might have just hours of the day and in others
we might have access to records at the level of a millisecond such as data 
from radio and TV. In Python,
internally all datetime objects are stored with nanosecond precision, but 
we can regard the precision as whatever granularity we have
given in our data for all practical purposes.

Datetime objects largely function the same as date objects.
Let's grab another dataset from Wikipedia that has precise timestamps. Below,
we read in a dataset consisting of the last 500 edits made to each of the 75
British author pages in our collection.

```{python}
page_revisions = pd.read_csv("data/wiki_uk_page_revisions.csv")
# Convert datetime column to pandas datetime
page_revisions['datetime'] = pd.to_datetime(page_revisions['datetime'])

page_revisions
```

Notice that each row has a record in the column `datetime` that provides
a precise datetime object giving the second at which the page was modified.
The data were stored using the ISO-8601 format ("YYYY-MM-DD HH:MM:SS"), which
pandas can automatically parse.

Our `page_revisions` dataset includes several pieces of information about each
of the edits. We have a username for the person who made the edit (recall that
anyone can edit a Wikipedia page), the size in bytes of the page after the edit
was made, and a short comment describing what was done in the change. Looking
at the page size over time shows when large additions and deletions were made
to each record. The code below yields a temporal visualization:

```{python}
# Filter for two authors and create plot
selected_authors = ['Geoffrey Chaucer', 'Emily BrontÃ«']
revision_subset = page_revisions[page_revisions['doc_id'].isin(selected_authors)]

p = (ggplot(revision_subset, aes(x='datetime', y='size', color='doc_id')) +
     geom_line() +
     scale_color_manual(values=['red', 'blue']) +
     labs(title="Wikipedia Page Size Over Time",
          x="Date", y="Page Size (bytes)",
          color="Author"))
p
```

Looking at the plot, we can see that there are a few very large edits (both
deletions and additions), likely consisting of large sections added and
subtracted from the page. If we want to visualize when these large changes
occurred, it would be useful to include a more granular set of labels on the
x-axis. We can do this using `scale_x_datetime()` with custom formatting:

```{python}
p = (ggplot(revision_subset, aes(x='datetime', y='size', color='doc_id')) +
     geom_line() +
     scale_color_manual(values=['red', 'blue']) +
     scale_x_datetime(date_breaks='6 months', date_labels='%b %Y') +
     theme(axis_text_x=element_text(angle=90, hjust=1)) +
     labs(title="Wikipedia Page Size Over Time (Custom Labels)",
          x="Date", y="Page Size (bytes)",
          color="Author"))
p
```

We can also filter our dataset by a particular range of dates or times. This is
useful to zoom into a specific region of our data to investigate patterns that 
may be otherwise lost. For example, if we wanted to see all of the page sizes
for two authors from 2021 onward:

```{python}
# Filter by datetime
recent_revisions = (page_revisions
    .query("doc_id in @selected_authors")
    .query("datetime > @pd.Timestamp('2021-01-01', tz='UTC')")
)

recent_revisions
```

Notice that the filter includes data from 2021, even though we use a strictly
greater than condition. The reason for this is that `'2021-01-01'` is interpreted
as the exact time corresponding to 1 January 2021 at 00:00. Any record that
comes at any other time during the year of 2021 will be included in the filter.

## Language and Time Zones

So far, we have primarily worked with numeric summaries of the date and datetime
objects. In the previous sections, notice that our example of working with the
days of the week and the names of the months all had Python create automatically the
names of these objects in English. Depending on our audience, it may be
desirable to show plots using the names of weekdays and months in another
language. This can be controlled by setting the system locale or using
custom formatting. Below, for example, are the days of the weeks provided in
different languages using pandas datetime functionality:

```{python}
# Extract various datetime components
datetime_analysis = (page_revisions
    .head(10)
    .assign(
        weekday_en = lambda df: df['datetime'].dt.day_name(),
        month_en = lambda df: df['datetime'].dt.month_name(),
        hour = lambda df: df['datetime'].dt.hour,
        date_only = lambda df: df['datetime'].dt.date
    )
    .loc[:, ['datetime', 'weekday_en', 'month_en', 'hour', 'date_only']]
)

datetime_analysis
```

For non-English locales, you would typically set the system locale before running
the analysis. However, since locale support varies by system, we'll focus on
the English representation here.

Another regional issue that arises when working with dates and times are
time zones. While seemingly not too difficult a concept, getting time zones to
work correctly with complex datasets can be incredibly complicated. A wide
range of programming bugs have been attributed to all sorts of edge-cases
surrounding the processing of time zones.

All times stored in pandas can be timezone-aware or timezone-naive. By default,
times are stored as timezone-naive (no timezone information). The data can be
localized to a specific timezone using pandas timezone functionality. All of the
times recorded in the dataset `page_revisions` are given in UTC. This
is not surprising; most technical sources with a global focus will use this
convention.

We can convert between time zones using pandas and pytz. The `tz_localize()` method
assigns a timezone to naive datetime data, while `tz_convert()` converts from one
timezone to another. Let's convert our UTC times to New York time:

```{python}
import pytz

# Convert UTC times to New York timezone
revision_with_tz = (page_revisions
    .head(10)
    .assign(
        datetime_utc = lambda df: df['datetime'],  # Already UTC timezone-aware
        datetime_nyc = lambda df: df['datetime'].dt.tz_convert('America/New_York')
    )
)

# Extract hour information for comparison
revision_with_tz = revision_with_tz.assign(
    hour_utc = lambda df: df['datetime_utc'].dt.hour,
    hour_nyc = lambda df: df['datetime_nyc'].dt.hour
)

revision_with_tz[['datetime_utc', 'datetime_nyc', 'hour_utc', 'hour_nyc']]
```

We can use the timezone information to display data
in a useful way to a local audience. For example, the code below displays the
frequency of updates as a function of the hour of the day in New York City:

```{python}
# Analyze edits by hour in New York timezone
hourly_edits = (page_revisions
    .assign(
        datetime_nyc = lambda df: df['datetime'].dt.tz_convert('America/New_York'),
        hour_nyc = lambda df: df['datetime'].dt.tz_convert('America/New_York').dt.hour
    )
    .groupby('hour_nyc')
    .size()
    .reset_index(name='count')
)

p = (ggplot(hourly_edits, aes(x='hour_nyc', y='count')) +
     geom_col() +
     labs(title="Wikipedia Edits by Hour (New York Time)",
          x="Hour of Day", y="Number of Edits"))
p
```

While certainly many editors are living in other English-speaking cities
(London, Los Angeles, or Mumbai), it is generally easier for people to do the
mental math for what times correspond with relative to their own time zone than
relative to UTC.

## Dates and Datetimes

We have shown above how to create date and datetime objects using pandas
`to_datetime()` function. Also, we have seen
how to extract the components from date and datetime objects with
the pandas datetime accessor (`.dt`). There are a variety of other functions that
help us create and manipulate these temporal objects. For example, pandas can
parse string representations in common formats automatically. While we will not 
give an entire list of all the available functions in pandas datetime functionality,
let's look at a few of the most useful and representative examples for converting 
between different ways of representing information about time.

The `page_revisions` dataset has revisions recorded with the precision of a
second. This is likely overly granular for many applications; it might be better
to have the data in a format that is only at the level of an hour, for example.
We can truncate any date or datetime object by using pandas' `.dt.floor()` method
along with a specific frequency. Setting the frequency to "H" (hour), for example,
will remove all of the minutes and seconds from the time:

```{python}
# Truncate datetime to hours
revision_hourly = (page_revisions
    .head(10)
    .assign(
        datetime_hour = lambda df: df['datetime'].dt.floor('H'),
        datetime_day = lambda df: df['datetime'].dt.floor('D')
    )
    .loc[:, ['datetime', 'datetime_hour', 'datetime_day']]
)

revision_hourly
```

The benefit of using the `.dt.floor()` method is that we could then group, join, or
summarize the data in a way that treats each value of `datetime` the same as
long as they occur during the same hour. There is also a `.dt.round()` method
for rounding the datetime object to the nearest desired unit. In the special case
in which we want to extract just the date part of a datetime, we can use the
`.dt.date` accessor. The code below illustrates this process, as well as showing
how reducing the temporal granularity can be a useful first step before grouping
and summarizing:

```{python}
# Count edits by date
daily_edit_counts = (page_revisions
    .assign(date = lambda df: df['datetime'].dt.date)
    .groupby('date')
    .size()
    .reset_index(name='count')
    .sort_values('date', ascending=False)
)

daily_edit_counts
```

Above, we effectively remove the time component of the datetime object and
treat the variable as having only a date element. Occasionally, we might
want to do the opposite. That is, considering only the time component of a
datetime object without worrying about the specific date. For example, we might
want to summarize the number of edits that are made based on the time of the
day. We can do this by extracting the time component:

```{python}
# Extract time component and aggregate by hour
hourly_edit_pattern = (page_revisions
    .assign(
        hour = lambda df: df['datetime'].dt.floor('H').dt.time,
        hour_numeric = lambda df: df['datetime'].dt.hour
    )
    .groupby('hour_numeric')
    .size()
    .reset_index(name='count')
    .sort_values('hour_numeric')
)

hourly_edit_pattern
```

This creates a variable that stores time without any date information, which is
useful for analyzing patterns that repeat daily.

## Window Functions

At the start of this chapter, we considered time series to be a sequence of
events without too much focus on the specific dates and times. This viewpoint
can be a useful construct when we want to look at changes over time. For
example, we have the overall size of each Wikipedia page after an edit. A
measurement that would be useful is the difference in page size made by an
edit. To add a variable to a dataset, we usually use the `assign()` method or
direct assignment, and that will again work here. However, in this case we need to reference
values that come before or after a certain value. This requires the use of
window functions.

A *window function* transforms a variable in a dataset into a new variable with
the same length in a way that takes into account the
entire ordering of the data. Two examples of window functions that are useful
when working with time series data are `shift()` with positive and negative values,
which give access to rows preceding or following a row, respectively. Let's apply this to our
dataset of page revisions to get the previous and next values of the page
size variable.

```{python}
# Apply window functions to get lagged and leading values
revision_with_lag = (page_revisions
    .assign(
        size_last = lambda df: df['size'].shift(1),  # lag
        size_next = lambda df: df['size'].shift(-1)  # lead
    )
    .loc[:, ['doc_id', 'datetime', 'size', 'size_last', 'size_next']]
)

revision_with_lag
```

Notice that the first value of `size_last` is missing because there is no
*last* value for the first item in our data. Similarly, the variable
`size_next` will have a missing value at the end of the dataset. As
written above, the code incorrectly crosses the time points at the boundary of
each page. That is, for the first row of the second page (Geoffrey Chaucer) it
thinks that the size of the last page is the size of the final page of the Marie de France
record. To fix this, we can group the dataset by item prior to applying the
window functions. Window functions respect the grouping of the data:

```{python}
# Apply window functions within groups
revision_grouped_lag = (page_revisions
    .sort_values(['doc_id', 'datetime'])  # Ensure proper ordering
    .groupby('doc_id', group_keys=False)
    .apply(lambda group: group.assign(
        size_last = group['size'].shift(1),
        size_next = group['size'].shift(-1)
    ))
    .reset_index(drop=True)
)

# Show the boundary between groups
boundary_data = revision_grouped_lag.iloc[495:505]
boundary_data[['doc_id', 'datetime', 'size', 'size_last', 'size_next']]
```

Notice that now, correctly, the dataset has a missing `size_next` for the
final Marie de France record and a missing `size_last` for the first 
Geoffrey Chaucer record. Now, let's use this to compute the change in the page
sizes for each of the revisions:

```{python}
# Compute size differences
revision_with_diff = (page_revisions
    .sort_values(['doc_id', 'datetime'])
    .groupby('doc_id', group_keys=False)
    .apply(lambda group: group.assign(
        size_diff = group['size'] - group['size'].shift(1)
    ))
    .reset_index(drop=True)
    .loc[:, ['doc_id', 'datetime', 'size', 'size_diff']]
)

revision_with_diff
```

In the above output, we can see the changes in page sizes. If we wanted to find
reversions in the dataset, we could apply the `shift()` function several times.
As an alternative, we can also give a parameter to `shift()` to indicate that we want to go back (or
forward) more than one row. Let's put this together to indicate which commits
seem to be a reversion (the page size exactly matches the page size from two
commits prior) as well as the overall size of the reversion:

```{python}
# Identify reversions
revision_with_reversion = (page_revisions
    .sort_values(['doc_id', 'datetime'])
    .groupby('doc_id', group_keys=False)
    .apply(lambda group: group.assign(
        size_diff = group['size'] - group['size'].shift(1),
        size_two_back = group['size'].shift(2),
        is_reversion = group['size'] == group['size'].shift(2)
    ))
    .reset_index(drop=True)
    .query('is_reversion == True')
    .loc[:, ['doc_id', 'datetime', 'size_diff', 'is_reversion']]
)

revision_with_reversion
```

These reversions can be studied to see the nature of the Wikipedia editing
process. For example, how long do these reversions tend to take? Are
certain pages more likely to undergo reversions? Do these take place during a
certain time of the day? These are all questions that we should now be able
to address using this dataset and the tools described above.

## Range Joins

We will finish this chapter by looking at *range joins*, which allow for 
combining datasets based on inequalities between keys contained in two different
datasets. Range joins are functionality that can greatly simplify some operations that arise when working with
temporal data. Recall that all of the join functions that we saw in
Chap. 5 work by finding a correspondence where keys from one dataset
equal the values of keys in another dataset. In some cases it happens that we
want to join two tables on inequalities rather than exact values.

Take, for example, the metadata table for the 75 authors in our Wikipedia collection.
Recall that this dataset contains the years that each author was born and the
years each author died. What if we wanted to make a dataset by joining the metadata
to itself, matching each author with other authors that would have been alive
in overlapping years? We can do this using pandas' `merge()` function with some
custom logic:

```{python}
# Load metadata
meta = pd.read_csv("data/wiki_uk_meta.csv.gz")

# Create overlap pairs using cross join and filtering
# This creates all combinations and then filters for overlaps
meta_cross = meta.assign(key=1).merge(meta.assign(key=1), on='key', suffixes=('', '2'))

overlap_pairs = (meta_cross
    .query('born <= born2 and died > born2')  # First author's life overlaps with second's birth
    .loc[:, ['doc_id', 'doc_id2', 'born', 'died', 'born2', 'died2']]
    .query('doc_id != doc_id2')  # Remove self-matches
)

overlap_pairs
```

The resulting dataset would make an interesting type of network, showing
temporal overlap of authors in the dataset. In fact, it is such a nice example
we could create a visualization by bringing together
Chap. 7 on networks with our temporal data:

```{python}
# Create a simple network visualization of temporal overlaps
# For demonstration, we'll create a subset and simple plot
overlap_sample = overlap_pairs.head(50)  # Sample for clarity

# Count overlaps per author
overlap_counts = (overlap_pairs
    .groupby('doc_id')
    .size()
    .reset_index(name='overlap_count')
    .merge(meta, on='doc_id')
    .sort_values('overlap_count', ascending=False)
)

overlap_counts
```

The output shows that certain authors have many temporal overlaps with others in our collection,
which creates interesting network structures. Adding this chapter on temporal data allows us to explore time and date data. 
When combined with other chapters such as networks, we can further layer our analysis.
Given the calls to add context when working with humanities data, exploring in multiple ways
and combining methods offers a way to add nuance. Of course, drawing on other scholarship,
archives, and primary sources will also shape our understandings, and provide further insights.
No computational method sits in a vacuum. Yet, adding important avenues such as time and date
to understand data through a temporal lens is one more way we can work with data in a way that
the humanities call for.

## Extensions

A large portion of the date and datetime operations in pandas have been 
introduced throughout this chapter. Pandas contains many more functions for 
converting strings in other formats into date and datetime objects, all of which
can be found along with examples in the pandas documentation. The `shift()` 
window function shown above is a very useful tool for any data that has a fixed 
order, which can include temporal data but also sources such as text.

Additional window functions that operate over an arbitrary number of rows at once
are provided by pandas' `.rolling()` and `.expanding()` methods. These are
excellent for computing moving averages, rolling statistics, and cumulative
calculations over time series data.

Beyond these programming tools, there is a wide array of specialized techniques 
for modeling time series data. Some useful Python libraries include:

- **statsmodels**: Comprehensive statistical modeling including time series analysis
- **scikit-learn**: Machine learning algorithms that can be applied to time series
- **prophet**: Facebook's time series forecasting library
- **pyflux**: Bayesian time series modeling
- **arch**: Econometric modeling for financial time series

For theoretical background, Shumway and Stoffer offer a relatively accessible 
introduction to time-series modeling [@shumway2017time]. Fumio Hayashi's
*Econometrics* provides more theoretical details [@hayashi2011econometrics]. 
The canonical and encyclopedic reference for temporal analysis is
James Hamilton's *Time series analysis* [@hamilton2020time].

The Python ecosystem provides rich and growing support for temporal data analysis,
with particularly strong capabilities in areas like financial time series,
econometrics, and forecasting that continue to evolve rapidly.

## References {-}
