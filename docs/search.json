[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Humanities Data in Python",
    "section": "",
    "text": "Welcome\nThis book is currently under development. It will be extended and updated throughout the 2025-2026 academic year. It is being used for our year-long undergraduate sequence of data science courses (DSST289 and DSST389).\nThis is the digital version of the text Humanities Data in Python. The first part of the text provides an introduction to the core data science principles using the Python programming language. The second part explores techniques for working with a variety of different kinds of data that are common in the humanities and social sciences. These include spatial, textual, and temporal data. Example datasets were chosen to be interesting without requiring deep specialized knowledge of a subject domain while at the same time being engaging enough to illustrate the power of the techniques under consideration. While originally designed with the needs of students coming from the humanities, the text offers an approach that should be useful to anyone looking to learn how to explore rich datasets with the Python programming language.\nThe authors use this site for teaching courses in data science and the digital humanities. Unlike the physical book, this website will be continually expanded and updated over time. We currently have this version of the text password protected though hope to eventually open it to the public. Please use the menu on the left (or above on the mobile site) to navigate the text. If you would like to follow along, please see the instructions in the setup section for getting Python and all of the datasets installed on your machine. As always, please contact us if you have any questions or comments about the text. We hope that you find it helpful!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Introduction\nIn this book, we focus on tools and techniques for exploratory data analysis, or EDA. Initially described in John Tukey’s classic text by the same name, EDA is a general approach to examining data through visualizations and broad summary statistics [1] [2]. It prioritizes studying data directly in order to generate hypotheses and ascertain general trends prior to, and often in lieu of, formal statistical modeling. The growth in both data volume and complexity has further increased the need for a careful application of these exploratory techniques. In the intervening 40 years, techniques for EDA have enjoyed great popularity within statistics, computer science, and many other data-driven fields and professions.\nThe histories of the R programming language and EDA are deeply entwined. Concurrent with Tukey’s development of EDA, Rick Becker, John Chambers, and Allan Wilks of Bell Labs began developing software designed specifically for statistical computing. By 1980, the ‘S’ language was released for general distribution outside Labs. It was followed by a popular series of books and updates, including ‘New S’ and ‘S-Plus’ [3] [4] [5] [6]. In the early 1990s, Ross Ihaka and Robert Gentleman produced a fully open-source implementation of S called ‘R’. It is called ‘R’ for it is both the “previous letter in the alphabet” and the shared initial in the authors’ names. Their implementation has become the de-facto tool in the field of statistics and is often cited as being amongst the Top-20 used programming languages in the world. Without the interactive console and flexible graphics engine of a language such as R, modern data analysis techniques would be largely intractable. Conversely, without the tools of EDA, R would likely still have been a welcome simplification to programming in lower-level languages, but would have played a far less pivotal role in the development of applied statistics.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#data-science-ecosystem",
    "href": "01_intro.html#data-science-ecosystem",
    "title": "1  Introduction",
    "section": "1.2 Data Science Ecosystem",
    "text": "1.2 Data Science Ecosystem\nWhile R pioneered many concepts in statistical computing and EDA, these ideas have been successfully adopted and extended by the Python programming language through a rich ecosystem of data science libraries. Python, originally created by Guido van Rossum in 1991, has evolved into one of the most popular programming languages for data analysis, machine learning, and scientific computing.\nThe core concepts developed for EDA in R—interactive data manipulation, powerful visualization capabilities, and seamless integration between analysis steps—have been implemented in Python through several key libraries. Pandas, developed by Wes McKinney starting in 2008, brought R-like data structures and manipulation capabilities to Python [7]. Matplotlib and later Seaborn provided comprehensive plotting capabilities, while plotnine specifically implemented the grammar of graphics approach pioneered by ggplot2 in R. More recently, Polars has emerged as a high-performance alternative to pandas for large-scale data manipulation.\nThis Python ecosystem maintains the same philosophy that made R successful for EDA: prioritizing interactive exploration, readable code, and the ability to seamlessly move between data manipulation, visualization, and analysis. The historical context of EDA’s development in R underscores the motivation for studying these concepts in Python as well. We see this book as contributing to efforts to bring new communities to learn from and to help shape data analysis by offering the humanities and humanistic social sciences powerful tools for data-driven inquiry. A visual summary of the steps of EDA are shown in Fig. 1.1. We will see that the core chapters in this text map onto the steps outlined in the diagram.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#setup",
    "href": "01_intro.html#setup",
    "title": "1  Introduction",
    "section": "1.3 Setup",
    "text": "1.3 Setup\nWhile it is possible to read this book as a conceptual text, we expect that the majority of readers will eventually want to follow along with the code and examples that are given throughout the text. The first step in doing so is to obtain a working copy of Python with the necessary data science libraries.\nWe recommend installing Python through the Anaconda distribution, which includes Python along with many of the most commonly used data science packages: https://www.anaconda.com/download. Anaconda provides download instructions according to a user’s operating system (i.e., Mac, Windows, Linux). Alternative installation methods exist for advanced users, including Miniconda for a lighter installation or installing Python directly and managing packages separately. We make no assumptions throughout this text regarding which operating system or method of obtaining Python readers have chosen. In the rare cases where differences exist based on these options, they will be explicitly addressed.\nFor working with Python code, we recommend using either Jupyter notebooks or an integrated development environment (IDE). Jupyter Lab or Jupyter notebooks provide an excellent interactive environment for data analysis and are included with Anaconda. Alternatively, VS Code with the Python extension or PyCharm provide full-featured IDE experiences. We will show examples using Jupyter notebooks, which provide a convenient way of running Python code and seeing the output in a single interface.\nIn addition to the Python software, walking through the examples in this text requires access to the datasets we explore. Care has been taken to ensure that these are all in the public domain so as to make it easy for us to redistribute to readers. The materials and download instructions can be found at https://humanitiesdata.org/. A complete copy of the code from the book is also provided to make replicating (and extending) the results as easy as possible.\nA major selling point of Python is its extensive collection of user-contributed libraries, available through the Python Package Index (PyPI). Details of how to install packages are included in the supplemental materials. Specifically, the supplemental materials have a document called setup.md or a requirements.txt file. These provide instructions for installing all the packages that are needed throughout this book using either pip or conda. Like Python itself, all the packages used here are free and open-source software, thanks to a robust community dedicated to developing and expanding Python’s data science capabilities.\nAs mentioned in the preface, we make heavy use in this text of several key Python packages: pandas for data manipulation, plotnine for visualization using the grammar of graphics, numpy for numerical computing, and matplotlib for additional plotting capabilities. We may also use specialized packages like scipy for statistical functions and scikit-learn for machine learning tasks.\nLearning to program is hard and invariably questions and issues will arise in the process (even the most experienced users require help with surprisingly high frequency). As a first source of help, searching a question or error message online will often pull up one of the many third-party question and answer sites, such as http://stackoverflow.com/, which are heavily frequented by new and advanced Python users alike. If we cannot find an immediate answer to a question, the next best step is to find some local, in-person help. While we have done our best with this static text to explain the concepts for working with Python, nothing beats talking to a real-life person. As a final step, we could post questions directly on third-party sites. It may take a few days to get a response, but usually someone helpful from the Python community will answer. We invite everyone to participate in the community by being active on forums, contributing packages, and supporting colleagues and friends. There are also great groups like PyLadies (pyladies.com) and local Python user groups that can provide further connections.\n\n\n\n\n\n\nFigure 1.1: Diagram of the process of exploratory data analysis.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#working-with-notebooks",
    "href": "01_intro.html#working-with-notebooks",
    "title": "1  Introduction",
    "section": "1.4 Working with Notebooks",
    "text": "1.4 Working with Notebooks\nThe supplemental materials for this book include all the data and code needed to replicate all of the analyses and visualizations in this book. We include the exact same code that will be printed in the book. We have used Jupyter notebooks (with an .ipynb extension) to store this code, with a file corresponding to each chapter in the text. Jupyter notebooks are an excellent choice for data analysis because they allow us to mix code, visualizations, and explanations within the same file [8]. In fact, the entire data science workflow—from initial exploration through final presentation—can be contained within a single notebook.\nThe Jupyter environment offers a convenient format for viewing and editing notebooks. When we open a Jupyter notebook, we see an interface with cells that can contain either code or markdown text. Running a code cell executes the Python code and displays the output directly below the cell. This interactive approach is ideal for exploratory data analysis because we can experiment with different approaches and immediately see the results.\nLooking at a Jupyter notebook, we’ll see that it consists of cells that contain either code or formatted text (markdown). Code cells have a gray background and can be executed by clicking the run button or pressing Shift+Enter. When we run code to read or create a new dataset, we can examine the data by simply typing the variable name in a cell, or by using methods like .head() or .info() to get summary information.\nAs with any digital file, it is a good idea to save the notebook frequently. Jupyter notebooks save both the code and the output, including plots and tables. If we would like to share our results, we can export the notebook to various formats including HTML, PDF, or even convert it to a Python script.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#running-python-code",
    "href": "01_intro.html#running-python-code",
    "title": "1  Introduction",
    "section": "1.5 Running Python Code",
    "text": "1.5 Running Python Code\nNow, let’s see some examples of how to run Python code. In this book, we will show snippets of Python code and the output. Though, know that we should think of each of the snippets as occurring inside of a code cell in a Jupyter notebook. In one of its most basic forms, Python can be used as a fancy calculator. We can add 1 and 1 by typing 1+1 into a code cell. Running the cell will display the output (2) below. In the book, we will write this code and output using a black box with the Python code written inside of it. Any output will be shown below. An example is given below.\n\n1 + 1\n\n2\n\n\nIn addition to just returning a value, running Python code can also result in storing values through the creation of new variables. Variables in Python are used to store anything—such as numbers, datasets, functions, or models—that we want to use again later. Each variable has a name associated with it that we can use to access it in future code. To create a variable, we use the = (equals) symbol with the name on the left-hand side of the equals sign and code that produces the value on the right-hand side. For example, we can create a new variable called mynum with a value of 8 by running the following code.\n\nmynum = 3 + 5\n\nNotice that the code here did not print any results because the result was saved as a new variable. We can now use our new variable mynum exactly the same way that we would use the number 8. For example, adding it to 1 to get the number nine:\n\nmynum + 1\n\n9\n\n\nVariable names must start with a letter or underscore, but can also use numbers after the first character. We recommend using only lowercase letters and underscores. That makes it easier to read the code later on without needing to remember if and where we used capital letters.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#functions-in-python",
    "href": "01_intro.html#functions-in-python",
    "title": "1  Introduction",
    "section": "1.6 Functions in Python",
    "text": "1.6 Functions in Python\nA function in Python is something that takes a set of input values and returns an output value. Generally, a function will have a format similar to that given in the code here:\n\nfunction_name(arg1=input1, arg2=input2)\n\nWhere arg1 and arg2 are the names of the inputs to the function (they are fixed) and input1 and input2 are the values that we will assign to them. The number of arguments is not always two, however. There may be any number of arguments, including zero. Also, there may be additional optional arguments that have default values that can be modified. Let us look at an example function: range. This function returns a sequence of numbers. We can give the function a starting point and an ending point.\n\nlist(range(1, 101))\n\n[1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99,\n 100]\n\n\nThe function returns a sequence of numbers starting from 1 and ending at 100 (note that the end point is exclusive in Python). We used list() to convert the range object to a list so we can see all the values. In addition to specifying arguments by name, we can also pass arguments by position. When specifying arguments by position we need to know and use the default ordering of the arguments. Below is an example of another equivalent way to write the code to produce a sequence of integers from 1 to 100, this time using a different approach with numpy.\n\nimport numpy as np\nnp.arange(1, 101)\n\nHow did we know the inputs to each function and what they do? In this text, we will explain the names and usage of the required inputs to new functions as they are introduced. In order to learn more about all of the possible inputs to a function, we can look at a function’s documentation. Python has excellent built-in documentation that can be accessed using the help() function or, in Jupyter notebooks, by using a question mark after the function name.\n\nhelp(range)\n# or in Jupyter: range?\n\nMany Python libraries also have extensive online documentation. For example, pandas has comprehensive documentation at https://pandas.pydata.org/docs/.\nAs shown in the documentation, the range function also has an optional step argument that controls the spacing between each of the numbers. By default the step is equal to 1, but we can change it to create sequences with different intervals. For example, below are the even numbers between 2 and 20.\n\nlist(range(2, 21, 2))\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n\n\nWe will learn how to use numerous functions in the coming chapters, each of which will help us in exploring and understanding data. In order to do this, we need to first load our data into Python, which we will show in the next section.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#loading-data-in-python",
    "href": "01_intro.html#loading-data-in-python",
    "title": "1  Introduction",
    "section": "1.7 Loading Data in Python",
    "text": "1.7 Loading Data in Python\nIn this book we will be working with data that is stored in a tabular format. Fig. 1.2 shows an example of a tabular dataset consisting of information about metropolitan regions in the United States supplied by the US Census Bureau. These regions are called core-based statistical areas, or CBSA. In the figure we can see rows and columns. Each row of the dataset represents a particular metropolitan region. We call each of the rows an observation. The columns in a tabular dataset represent the measurements that we record for each observation. These measurements are called variables.\nIn our example dataset, we have five variables which record the name of the region, the quadrant of the country that the region exists in, the population of the region in millions of people, the density given in tens of thousands of people per square kilometer, and the median age of all people living in the region. More details are given in the following section.\n\n\n\n\n\n\nFigure 1.2: Example of a tabular dataset.\n\n\n\nA larger version of this dataset, with more regions and variables, is included in the book’s supplemental materials as a comma separated value (CSV) file. We will make extensive use of this dataset in the following chapters as a common example for creating visualizations and performing data manipulation. In order to read in the dataset we use the function pd.read_csv() from the pandas package [9]. In order make the functions from pandas available, we need to import it. We will use the standard convention of importing pandas as pd and numpy as np.\n\nimport pandas as pd\nimport numpy as np\n\nWe call the pd.read_csv() function with the path to where the file is located relative to where this script is stored. If we are running the Jupyter notebooks from the supplemental materials, the data will be called acs_cbsa.csv and will be stored in a folder called data. The following code will load the CBSA dataset into Python, save it as a variable called cbsa, and print out the first several rows. The output dataset is stored as a type of Python object called a DataFrame.\n\ncbsa = pd.read_csv(\"data/acs_cbsa.csv\")\ncbsa\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\n\n\n0\nNew York\n35620\nNE\n-74.101056\n40.768770\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\nMiddle Atlantic\n\n\n1\nLos Angeles\n31080\nW\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\nPacific\n\n\n2\nChicago\n16980\nNC\n-87.958820\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\nEast North Central\n\n\n3\nDallas\n19100\nS\n-96.970508\n32.849480\n7.543340\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\nWest South Central\n\n\n4\nHouston\n26420\nS\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\nWest South Central\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n929\nZapata\n49820\nS\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\nWest South Central\n\n\n930\nKetchikan\n28540\nO\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\nPacific\n\n\n931\nCraig\n18780\nW\n-108.207523\n40.618749\n0.013240\n1.077471\n44.5\n58583\n72.8\n485\n29.0\nMountain\n\n\n932\nVernon\n46900\nS\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\nWest South Central\n\n\n933\nLamesa\n29500\nS\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\nWest South Central\n\n\n\n\n934 rows × 13 columns\n\n\n\nNotice that the display shows that there are a total of 934 rows and 13 columns. Or, with our terms defined above, there are 934 observations and 13 variables. Only the first few and last few observations are shown in the output, along with information about the shape of the DataFrame. We can use various methods to explore the data, such as cbsa.head() to see the first few rows, cbsa.info() to get information about the data types, or cbsa.describe() to get summary statistics.\nThe data types shown by pandas tell us the types of data stored in each column. The type object typically indicates text data (strings), such as the name, quad (quadrant), and division columns. String data can consist of any sequence of letters, numbers, spaces, and punctuation marks. String variables are often used to represent fixed categories, such as the quadrant and division of each CBSA region. They can also provide unique identifiers and descriptions for each row, such as the name of the CBSA region in our example.\nThe other data types we see are numeric types like int64 and float64, which indicate that a column contains integer or floating-point numeric data. While there are technical differences between these types (integers are whole numbers, floats can have decimal places), we will refer to any variable of either type as numeric data for our purposes.\nKnowing the types of data for each column is important because, as we will see throughout the book, they will affect the kinds of visualizations and analysis that can be applied. The data types in the DataFrame are automatically determined by the pd.read_csv() function. Optional arguments like dtype can be set to specify alternatives, or we can modify data types after the DataFrame has been created using techniques shown in Chapter 3. The string and numeric data types are by far the most common. Other possible options are explored in Chapter 7 (dates and times), Chapter 9 (spatial variables), and Chapter 11 (lists and logical values).",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#datasets",
    "href": "01_intro.html#datasets",
    "title": "1  Introduction",
    "section": "1.8 Datasets",
    "text": "1.8 Datasets\nThroughout this book, we will use multiple datasets to illustrate different concepts and show how each approach can be used across multiple application domains. We draw on data that animates humanities inquiry in areas such as American Studies, history, literary studies, and visual culture studies. While we will briefly reintroduce new datasets as they appear, for readers making their way selectively through the text, we offer a somewhat more detailed description of the main datasets that we will use in this section.\nTo introduce the concept of EDA, we will make sustained use of the CBSA dataset in Chapters 2-5 to demonstrate new concepts in data visualization and manipulation. As described above, the data comes from an annual survey conducted by the United States Census Bureau called the American Community Survey (ACS). The survey consists of data collected from a sample of 3.5 million households in the United States. Outside of the constitutionally mandated decennial census, this is the largest survey completed by the Census Bureau. It asks several dozen questions covering topics such as gender, race, income, housing, education, and transportation. Aggregated data are released on a regular schedule, with summaries over one-, three-, and five-year periods. Our data comes from the five-year summary from the most recently published version (2021) at the time of writing. We selected a small set of measurements that we felt did not require extensive background knowledge, while capturing variations across the county. As seen in the table above, we have selected the median age, median household income (USD), the percentage of households owning their housing, the median rent for a one-bedroom apartment (USD), and the median household spending on rent.\nThe American Community Survey aggregates data to a variety of different geographic regions. Most regions correspond to political boundaries, such as states, counties, and cities. One particularly interesting geographic region are the core-based statistical areas, or CBSA. These regions, of which there are nearly a thousand, are defined by the United States Office of Management and Budget. Regions are defined in the documentation as “an area containing a large population nucleus and adjacent communities that have a high degree of integration with that nucleus.” We chose these regions for our dataset because their social, rather than political, definition makes them particularly well suited for humanities research questions. Our dataset includes a short, common name for each CBSA, as well as a unique identifier (geoid), and several geographic categorizations derived from spatial data provided by the Census Bureau.\nThe core chapters of the book also make use of a dataset illustrating the relative change in the price of various food items for over 140 years in the United States. This collection was published as-is by Davis S. Jacks for his publication “From boom to bust: a typology of real commodity prices in the long run” [10]. The data is organized with one observation per year, and variables capturing the relative price of each of thirteen food commodities. We can read this dataset into Python using the same function that we used for the CBSA dataset, shown below.\n\nfood_prices = pd.read_csv(\"data/food_prices.csv\")\nfood_prices\n\n\n\n\n\n\n\n\nyear\ntea\nsugar\npeanuts\ncoffee\ncocoa\nwheat\nrye\nrice\ncorn\nbarley\npork\nbeef\nlamb\n\n\n\n\n0\n1870\n128.845647\n151.406650\n203.422160\n88.067280\n78.817566\n88.102178\n103.393214\n83.478261\n121.373971\n103.241107\n107.813162\n73.540373\n63.257185\n\n\n1\n1871\n131.730769\n167.156863\n221.774193\n108.728711\n66.686484\n118.157238\n105.251996\n84.543919\n88.393285\n130.236487\n68.466144\n86.755952\n73.411017\n\n\n2\n1872\n134.455128\n161.764706\n188.508064\n140.092762\n71.591359\n121.523799\n102.095808\n92.905405\n69.229178\n124.662162\n58.644715\n85.119048\n78.072034\n\n\n3\n1873\n136.337312\n154.104124\n178.826474\n172.869998\n65.752662\n116.056854\n106.028976\n91.034483\n67.136989\n166.034483\n62.871238\n93.563218\n84.444769\n\n\n4\n1874\n146.088815\n153.246661\n230.709444\n187.244900\n69.924808\n113.202047\n125.977031\n99.637681\n127.551727\n174.275362\n87.613692\n89.531573\n77.480963\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n141\n2011\n33.092175\n34.758403\n49.430540\n104.633987\n30.950856\n32.303050\n56.050786\n26.970989\n56.191498\n76.784074\n18.957462\n195.682011\n199.901823\n\n\n142\n2012\n32.169401\n28.216065\n50.480016\n70.519447\n24.339000\n32.329419\n54.278655\n27.791165\n60.997731\n87.240944\n17.266451\n196.471582\n179.915106\n\n\n143\n2013\n31.300000\n22.830000\n31.508720\n51.984920\n24.449543\n30.635729\n55.776368\n24.481270\n55.410171\n72.322420\n17.782412\n190.347418\n164.486798\n\n\n144\n2014\n29.530000\n21.790000\n29.420000\n74.140000\n30.470000\n26.370000\n53.558470\n19.970000\n36.750000\n50.220000\n20.960000\n229.492089\n184.540000\n\n\n145\n2015\n29.174600\n17.087190\n28.121785\n58.715669\n30.967121\n22.009427\n51.521938\n17.670020\n32.904916\n68.492986\n13.747636\n203.667366\n149.682263\n\n\n\n\n146 rows × 14 columns\n\n\n\nAll of the prices are given on a relative scale where \\(100\\) is equal to the price in 1900. We will use this dataset to show how to build data visualizations that show change over time. It will also be useful for our study of table pivots in Chapter 4.\nPart II turns to data types. The first three application chapters focus on text analysis, temporal analysis, and network analysis, respectively. While these three chapters introduce different methods, we will make use of a consistent core dataset across all three that we have created from Wikipedia. Specifically, we have a dataset consisting of the text, links, page views, and change histories of a set of 75 Wikipedia pages sampled from a set of British authors. These data are contained in several different tables, each of which will be introduced as needed. The main metadata for the set of 75 pages is shown in the data loaded by the following code.\n\nmeta = pd.read_csv(\"data/wiki_uk_meta.csv.gz\")\nmeta\n\n\n\n\n\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\n\n\n0\nMarie de France\n1160\n1215\nEarly\nfemale\nMarie_de_France\nMarie d. F.\n\n\n1\nGeoffrey Chaucer\n1343\n1400\nEarly\nmale\nGeoffrey_Chaucer\nChaucer\n\n\n2\nJohn Gower\n1330\n1408\nEarly\nmale\nJohn_Gower\nGower\n\n\n3\nWilliam Langland\n1332\n1386\nEarly\nmale\nWilliam_Langland\nLangland\n\n\n4\nMargery Kempe\n1373\n1438\nEarly\nfemale\nMargery_Kempe\nKempe\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n70\nStephen Spender\n1909\n1995\nTwentieth C\nmale\nStephen_Spender\nSpender\n\n\n71\nChristopher Isherwood\n1904\n1986\nTwentieth C\nmale\nChristopher_Isherwood\nIsherwood\n\n\n72\nEdward Upward\n1903\n2009\nTwentieth C\nmale\nEdward_Upward\nUpward\n\n\n73\nRex Warner\n1905\n1986\nTwentieth C\nmale\nRex_Warner\nWarner\n\n\n74\nSeamus Heaney\n1939\n1939\nTwentieth C\nmale\nSeamus_Heaney\nHeaney\n\n\n\n\n75 rows × 7 columns\n\n\n\nWe decided to use Wikipedia data because it is freely available and can be easily generated in the same format for other collection of pages that correspond to nearly any other topic of interest. Wikipedia is also helpful because it allows us look at pages in other languages, which will allow us to demonstrate how to extend our techniques to texts that are not in English. Finally, we will return to the Wikipedia data in Chapter 12 to demonstrate how to build a dataset (specifically, this one) by calling an API from within Python using the requests library.\nSeveral other datasets will be used throughout the book within a single chapter. For example, Chapter 9 on spatial data makes use of a dataset showing the location of French cities and Parisian metro stops as a source in our study of geographic data. Chapter 10 on image data shows a collection of documentary photographs and associated metadata in our analysis of images. As these datasets are used only a single point in the book, we will introduce them in more detail as they are introduced.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#formatting-python-code",
    "href": "01_intro.html#formatting-python-code",
    "title": "1  Introduction",
    "section": "1.9 Formatting Python Code",
    "text": "1.9 Formatting Python Code\nIt is very important to properly format Python code in a consistent way. Even though the code may run without errors and produce the desired results, keeping the code well-formatted will make it easier to read and debug. We will follow the following guidelines throughout this book (based on PEP 8, Python’s official style guide):\n\none space before and after an equals sign and around operators\none space after a comma, but no space before a comma\none space around mathematical operations (such as + and *)\nif a line of code becomes too long, split it across multiple lines with proper indentation (typically 4 spaces)\nuse lowercase with underscores for variable names (snake_case)\n\nWe have found it makes our life a lot easier if we use these rules right from the start and whenever we are writing Python code.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#extensions",
    "href": "01_intro.html#extensions",
    "title": "1  Introduction",
    "section": "1.10 Extensions",
    "text": "1.10 Extensions\nEach chapter in this book contains a short, concluding section of extensions on the main material. These include references for further study, additional Python packages, and other suggested methods that may be of interest to the study of each specific type of humanities data.\nIn this chapter, we will mention a few standard Python references that might be useful to use in parallel or in sequence with our text. The classic introduction to the Python language is Learning Python by Mark Lutz [11]. For those specifically interested in data science applications, Python for Data Analysis by Wes McKinney (the creator of pandas) provides comprehensive coverage of the core data science libraries [7]. This book covers pandas, numpy, matplotlib, and related tools in great detail.\nFor the specific approach to data analysis that we follow in this book, Python Data Science Handbook by Jake VanderPlas is an excellent reference [12]. It covers the full stack of data science tools in Python, from basic data manipulation through machine learning. The book is also available freely online.\nWhen working through the code in this book’s supplemental materials, as mentioned above, we will be using Jupyter notebooks. More information about Jupyter and what can be done with it can be found in the Jupyter Notebook documentation and various online tutorials. The philosophy behind interactive computing can be found in research on computational notebooks and reproducible research [8].\nFor those interested in the grammar of graphics approach to visualization that we use throughout this book, The Grammar of Graphics by Leland Wilkinson provides the theoretical foundation [13]. The plotnine library implements these concepts in Python, closely following the ggplot2 implementation from R.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#references",
    "href": "01_intro.html#references",
    "title": "1  Introduction",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Tukey, J W (1977 ). Exploratory Data Analysis. Addison-Wesley\n\n\n[2] Brillinger, D R (2002 ). John w. Tukey: His life and professional contributions. Annals of Statistics. JSTOR. 1535–75\n\n\n[3] Becker, R A and Chambers, J M (1984 ). S: An Interactive Environment for Data Analysis and Graphics. CRC Press\n\n\n[4] Becker, R A and Chambers, J M (1985 ). Extending the S System. Wadsworth Advanced Books; Software\n\n\n[5] Becker, R A, Chambers, J M and Wilks, A R (1988 ). The New S Language: A Programming Environment for Data Analysis and Graphics. Wadsworth Advanced Books; Software\n\n\n[6] Chambers, J M and Hastie, T J (1991 ). Statistical Models in S. CRC Press\n\n\n[7] McKinney, W (2012 ). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O’Reilly Media, Inc.\"\n\n\n[8] Pérez, F and Granger, B E (2007 ). IPython: A system for interactive scientific computing. Computing in science & engineering. IEEE. 9 21–9\n\n\n[9] McKinney, W and others (2011 ). Pandas: A foundational python library for data analysis and statistics. Python for high performance and scientific computing. Dallas, TX. 14 1–9\n\n\n[10] Jacks, D S (2019 ). From boom to bust: A typology of real commodity prices in the long run. Cliometrica. Springer. 13 201–20\n\n\n[11] Lutz, M (2013 ). Learning Python: Powerful Object-Oriented Programming. \" O’Reilly Media, Inc.\"\n\n\n[12] VanderPlas, J, Granger, B, Heer, J, Moritz, D, Wongsuphasawat, K, Satyanarayan, A, Lees, E, Timofeev, I, Welsh, B and Sievert, S (2018 ). Altair: Interactive statistical visualizations for python. Journal of open source software. The Open Journal. 3 1057\n\n\n[13] Wilkinson, L (2012 ). The Grammar of Graphics. Springer",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_graphics.html",
    "href": "02_graphics.html",
    "title": "2  EDA I: Visualizing Data",
    "section": "",
    "text": "2.1 Introduction\nAs we outlined in Chapter 1, the concept of exploratory data analysis (EDA) is key to our approach. As a result, data visualization is one of the most important tasks and powerful tools for the analysis of data. We start our study of exploratory data analysis with visualization because it offers the best immediate payoff for how statistical programming can help understand datasets of any size. Visualizations also have the benefit for those new to programming because it is relatively easy to verify that our code is working. We can just look at the output and see if the resulting plot is what we expected. Finally, data visualizations can be useful for even very small collections of data.\nIn this chapter, we will learn and use the plotnine package for building informative graphics [1] [2]. The package makes it easy to build fairly complex graphics in a way that is guided by a general theory of data visualization. The only downside is that, because it is built around a theoretical model rather than many one-off solutions for different tasks, it has a somewhat steeper initial learning curve. The chapter is designed to get us started using the package to make a variety of different data visualizations.\nThe core idea of the grammar of graphics is that visualizations are composed of independent layers. The term “grammar” is used to describe visualizations because the theory builds connections between elements of the dataset to elements of a visualization. It builds up complex elements from smaller ones, much like a grammar provides relations between words in order to generate larger phrases and sentences. To describe a specific layer, we need to specify several elements. First, we need to specify the dataset from which data will be taken to construct the plot. Next, we have to specify a set of mappings called aesthetics that describe how elements of the plot are related to columns in our data. For example, we often indicate which column corresponds to the horizontal axis of the plot and which one corresponds to the vertical axis of the plot. It is also possible to describe elements such as color, shape, and size of elements of the plot by associating these quantities with columns in the data. Finally, we need to provide the geometry that will be used in the plot. The geometry describes the kinds of objects that are associated with each row of the data. A common example is the points geometry, which associates a single point with each observation.\nWe can show how to use the grammar of graphics by starting with the CBSA data that we introduced in the previous chapter, where each row is associated with a particular metropolitan region in the United States. The first plot we will make is a scatter plot that investigates the relationship between the median price of a one-bedroom apartment and the population density of the metropolitan region. In the language of the grammar of graphics, we can start to describe this visualization by providing the variable name in Python of the data (cbsa). Next, we associate the horizontal axis (called the x aesthetic) with the column in the data named density. The vertical axis (the y aesthetic) can similarly be associated with the column named rent_1br_median. We will make a scatter plot, with each point on the plot describing one of our metropolitan regions, which leads us to use a point geometry. Our plot will allow us to understand the relationship between city density and rental prices.\nIn Python, we need to use some special functions to indicate all of this information and to instruct the program to produce a plot. We start by indicating the name of the underlying dataset and using it to create a special object called ggplot that indicates that we want to create a data visualization. The plot itself is created by adding—literally, with the plus sign—the function geom_point. This function indicates that we want to add a points geometry to the plot. Inside of the geometry function, we apply the function aes (short for aesthetics), which indicates that we want to specify the mappings between components of the plot and column names in our dataset. Code to write this using the values described in the previous paragraph is given below.\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median')))\n\ncbsa[['name', 'quad', 'density', 'rent_1br_median']]\n\n\n\n\n\n\n\n\nname\nquad\ndensity\nrent_1br_median\n\n\n\n\n0\nNew York\nNE\n1051.306467\n1430\n\n\n1\nLos Angeles\nW\n1040.647281\n1468\n\n\n2\nChicago\nNC\n508.629406\n1060\n\n\n3\nDallas\nS\n323.181404\n1106\n\n\n4\nHouston\nS\n316.543514\n997\n\n\n5\nWashington\nS\n363.732689\n1601\n\n\n6\nPhiladelphia\nNE\n506.068130\n1083\n\n\n7\nMiami\nS\n430.103162\n1230\n\n\n8\nAtlanta\nS\n263.275821\n1181\n\n\n9\nBoston\nNE\n517.827702\n1390\n\n\n10\nPhoenix\nW\n126.558955\n1065\n\n\n11\nSan Francisco\nW\n712.992772\n1940\n\n\n12\nRiverside\nW\n64.511974\n1120\n\n\n13\nDetroit\nNC\n421.567420\n811\n\n\n14\nSeattle\nW\n256.189989\n1478\n\n\n15\nMinneapolis\nNC\n188.297448\n1035\n\n\n16\nSan Diego\nW\n298.781488\n1518\n\n\n17\nTampa\nS\n452.790117\n1045\n\n\n18\nDenver\nW\n135.070927\n1340\n\n\n19\nBaltimore\nS\n404.661335\n1132\n\n\n20\nSt. Louis\nNC\n134.615515\n778\n\n\n21\nOrlando\nS\n253.043039\n1156\n\n\n22\nCharlotte\nS\n177.393426\n1074\n\n\n23\nSan Antonio\nS\n132.337510\n936\n\n\n24\nPortland\nW\n141.406821\n1262\n\n\n25\nSacramento\nW\n173.268846\n1156\n\n\n26\nPittsburgh\nNE\n171.198592\n743\n\n\n27\nCincinnati\nNC\n187.276368\n714\n\n\n28\nAustin\nS\n201.447732\n1216\n\n\n29\nLas Vegas\nW\n106.878725\n1004\n\n\n\n\n\nPlot of the largest 30 core-based statistical areas in the United States, showing their density and the median price to rent a one-bedroom apartment from the 2021 American community survey.\nRunning the code above from a Quarto document will show the desired visualization right below the block of code. In this plot, each row of our dataset, a CBSA region, is represented as a point in the plot. The location of each point is determined by the density and median rent price for a one-bedroom apartment in the corresponding region. Notice that Python has automatically made several choices for the plot that we did not explicitly indicate in the code. For example, the range of values on the two axes, the axis labels, the grid lines, and the marks along the grid. Python has also automatically picked the color, size, and shape of the points. While the defaults work as a good starting point, it is often useful to modify these values; we will see how to changes these aspects of the plot in later sections of this chapter.\nScatter plots are typically used to understand the relationship between two numeric values. What does our first plot tell us about the relationship between city density and median rent? There is not a clear trend between these two variables. Rather, the plot of these two economic metrics clusters the regions into several groups. We see a couple of regions with a very high density but only moderately large rental prices, one city with unusually high rental prices, and the rest of the regions fairly uniformly distributed in the lower-left corner of the plot. Let’s see if we can give some more context to the plot by adding additional information.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#introduction",
    "href": "02_graphics.html#introduction",
    "title": "2  EDA I: Visualizing Data",
    "section": "",
    "text": "Figure 2.1: Diagram of how the elements of the grammar of graphics correspond to elements of the code and visualization.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#text-geometry",
    "href": "02_graphics.html#text-geometry",
    "title": "2  EDA I: Visualizing Data",
    "section": "2.2 Text Geometry",
    "text": "2.2 Text Geometry\nA common critique of computational methods is that they obscure a closer understanding of each individual object of study in an attempt to search for numeric patterns. This is certainly an important caution; computational analysis of humanities data should always be paired with close analysis. However, it does not always have to be the case that visualizations reduce complex collections to a few numerical summaries. This is particularly so when working with a dataset that has a relatively small number of observations. Looking back at our first scatter plot, how could we recover a closer analysis of individual cities while also looking for general patterns between the two economic variables? One option is to add labels indicating the names of the regions. These names would let anyone looking at the plot to adding their own understanding of the individual regions as an additional layer of information as they interpret the plot.\nAdding the names of the regions can be done by using another type of geometry called a text geometry. This geometry is created with the function geom_text. For each row of a given dataset, this geometry adds a small textual label. As with the point geometry, it requires us to specify which columns of our data correspond to the x and y aesthetics. These values tell the plot where to place the label. Additionally, the text geometry requires an aesthetic called label that indicates the column of the dataset that the label should take its text from. In our case, we will use the column called name to make textual labels on the plot. A reminder that this is a column name from the data that we loaded in. The code block below produces a text label plot by changing the geometry type and adding the additional aesthetic from the previous example.\n\n(ggplot(cbsa) +\n    geom_text(aes(x='density', y='rent_1br_median', label='name')))\n\n\n\n\nPlot of the largest 30 core-based statistical areas in the United States, showing their density and the median price to rent a one-bedroom apartment from the 2021 American Community Survey. Here, short descriptive names of the regions are included.\n\n\n\n\nThe plot generated by the code allows us to now see which region has the highest rents (San Francisco). And, we can identify which regions have the highest density (New York and Los Angeles). We can also identify regions such as Detroit that are relatively dense but inexpensive or regions such as Denver that are not particularly dense but still one of the more expensive regions to rent in. While we have added only a single additional piece of information to the plot, each of the labels uniquely identifies each row of the data. This allows anyone familiar with metropolitan regions in the United States to bring many more characteristics of each data point to the plot through their own knowledge. For example, while the plot does not include any information about overall population, anyone who knows the largest cities in the US can use the plot to see that the two most dense cities (New York and Los Angeles) are also the most populous. And, while the plot does not have information about the location of the regions, if we know the general geography of the country, it is possible to see that many of the cities that are expensive but not particularly dense (Portland, Denver, Seattle, and San Diego) are on the West Coast. These observations point to the power of including labels on a scatter plot.\nWhile the text plot adds additional contextual information compared to the scatter plot, it does have some shortcomings. Some of the labels for points at the edges of the plot fall off and become truncated. Labels for points in the lower-left corner of the plot start to overlap one another and become difficult to read. These issues will only grow if we increase the number of regions in our dataset. Also, it is not entirely clear what part of the label corresponds to the density of the cities. Is it the center of the label, the start of the label, or the end of the label? We could add a note that the value is the center of the label, but that becomes somewhat cumbersome to have to constantly remember and remind ourselves and others about.\nTo start addressing these issues, we can add the points back into the plot with the labels. We could do this in Python by adding the two geometry layers (geom_point and geom_text) one after the other. This will make it more clear where on the x-axis each region is associated to, but at the same time will make the names of the cities even more difficult to read. To fix the second problem, we will replace the text geometry with a different geometry called geom_text_repel. It also places labels on the plot, but has special logic that avoids intersecting labels. Instead, labels are moved away from the data points and connected (when needed) by a line segment. As with the text geometry, the text repel geometry requires specifying x, y, and label aesthetics. Below is the code to make both of these modifications.\n\nfrom adjustText import adjust_text\nimport matplotlib.pyplot as plt\n\n# Note: plotnine doesn't have geom_text_repel, so we'll use a workaround with regular geom_text\n# For production code, you might want to use matplotlib directly or another solution\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median')) +\n    geom_text(aes(x='density', y='rent_1br_median', label='name'), \n              nudge_y=50, size=8))\n\n\n\n\nPlot of the largest 30 core-based statistical areas in the United States, showing their density and the median price to rent a one-bedroom apartment from the 2021 American community survey. Here, short descriptive names of the regions are included but offset from the points to make the plot easier to read.\n\n\n\n\nThe output of the plot with the points and text labels shows that we have attempted to avoid writing labels that intersect one another by nudging them slightly. The points indicate the specific values of the density and median rents, while the labels provide context about which metropolitan area each point represents. Some of the labels do still become a bit busy in the lower left-hand corner; this could be fixed by making the size of the labels slightly smaller, which we will learn how to do later in the chapter. Once the number of points becomes larger, it will eventually not be possible to label all of the points. Several strategies exist for dealing with this, such as only labeling a subset of the points. We will see these techniques as they arise in our examples. The plotnine package and communities online have an entire ecosystem of strategies for increasing interpretability and adding context to plot, providing strategies for using the exploratory and visual power of data visualization to garner insights from humanities data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#lines-and-bars",
    "href": "02_graphics.html#lines-and-bars",
    "title": "2  EDA I: Visualizing Data",
    "section": "2.3 Lines and Bars",
    "text": "2.3 Lines and Bars\nThere a large number of different geometries supplied by the plotnine package, in addition to the even larger collection of extensions by other Python packages. We will look at two other types of geometries in this section that allow us to investigate common relationships in the columns of a dataset. Other geometries will be discussed throughout the book as the need arises, and the full list of geometries can be found in the plotnine package’s documentation.\n\n\n\n\n\n\nFigure 2.2: Examples of common geometries used in the grammar of graphics.\n\n\n\nFor a moment, we will switch gears and look at the food prices dataset, which was introduced in the previous chapter. This data contains one row for every year from 1870 through 2015, with relative prices for thirteen different food items across the United States [3]. Consider a visualization showing the change in the price of tea over the 146 years in the dataset. We could create a scatter plot where each point is a row of the data, the x aesthetic captures the year of each record, and the y aesthetic measures the relative cost of tea. This visualization would be fine and could roughly help us understand the changes in relative prices for this commodity. A common visualization type, however, for data of this format is a line plot, where the price in each year is connected by a line to the price in the subsequent year. To create such a plot, we can use the geom_line geometry. This is most commonly used when the horizontal axis measures some unit of time, but can represent other quantities that we expect to continuously and smoothly change between measurements on the x-axis. The line geometry requires the same aesthetics as the point geometry and can be created with the same syntax, as shown in the following block of code.\n\n(ggplot(food_prices) +\n    geom_line(aes(x='year', y='tea')))\n\n\n\n\nPlot of the price of tea in standardized units (100 is the price in 1900) over time.\n\n\n\n\nThe output of this visualization allows us to see the change over time of the tea prices. Notice that the relative price decreased fairly steadily from 1870 through to 1920. It had a few sudden drops and reversals in the 1920s and 1930s, before increasing again in the 1950s. The relative cost of tea then decreased again fairly steadily from the mid 1950s through to the end of the data range in 2015.\nAnother common usage of a visualization is to see the value of a numeric column of the dataset relative to a character column of the dataset. It is possible to represent such a relationship with a geom_point layer. However, it is often more visually meaningful to use a bar for each category and the height or length of the bar representing the numeric value. This type of plot is most common when showing the counts of different categories, something we will see in the next chapter, but can also be used in any situation where a numeric value is associated with different categories. To create a plot with bars we use the geom_col function, providing both x and y aesthetics. Python will automatically create vertical bars if we have a character variable associated with the x aesthetic and horizontal bars if we have one in the y aesthetic. Putting the character variable on the y-axis usually makes it easier to read the labels, so we recommend it in most cases. In the code block below, we have the commands to create a bar plot of the population in each region from the CBSA dataset.\n\n(ggplot(cbsa) +\n    geom_col(aes(x='pop', y='name')))\n\n\n\n\nPlot of the population of the largest 30 core-based statistical areas in the United States, showing their population from the 2021 American community survey.\n\n\n\n\nOne of the first things that stands out in the output is that the regions are ordered alphabetically from bottom to top. The visualization would be much more useful and readable if we could reorder the categories on the y-axis. This is also something that we will address in the following chapter. For now, we can see how plotnine is offering a range of plot types to see our data from different angles. We can add additional context through additional aesthetics.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#optional-aesthetics",
    "href": "02_graphics.html#optional-aesthetics",
    "title": "2  EDA I: Visualizing Data",
    "section": "2.4 Optional Aesthetics",
    "text": "2.4 Optional Aesthetics\nIn the previous sections, we have shown how visualizations can be built out of geometry layers, where each geometry is associated with a dataset and a collection of variable mappings known as aesthetics. The point, line, and bar geometries require x and y aesthetics; the text geometry also required an aesthetic named label. In addition to the required aesthetics, each geometry type also has a number of optional aesthetics that we can use to add additional information to the plot. For example, most geometries have a color aesthetic. The syntax for describing this is exactly the same as with the required aesthetics: we place the name of the aesthetic followed by the name of the associated variable name. Let’s see what happens when we add a color aesthetic to our scatter plot by relating the column called quad to the aesthetic named color. Below is the corresponding code.\n\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median', color='quad')))\n\n\n\n\nPlot of the largest 30 core-based statistical areas in the United States, showing their density and the median price to rent a one-bedroom apartment from the 2021 American community survey. Here, the points are colored based on the quadrant in which the city is found in the United States.\n\n\n\n\nThe result of associating a column in the dataset with a color produces a new variation of the original scatter plot. We have the same set of points and locations on the plot, as well as the same axes. However, now each color has been automatically associated with a region and every point has been colored according to the region column associated with each row of the data. The mapping between colors and region names is shown in an automatically created legend on the right-hand side of the plot. The ability to add additional information to the plot by specifying a single aesthetic speaks to how powerful the grammar of graphics is in terms of quickly producing informative visualizations of data. In the first edition of this text, which used the built-in graphics system in Python, it was necessary to write nearly a dozen lines of code to produce a similar plot. Now that we are able to use the plotnine package, this process has been greatly simplified.\nIn the previous example we changed the color aesthetic from the fixed default of black to a color that changes with another variable. It is also possible to specify an alternative, fixed value for any aesthetic. We can draw on the color names available in Python. For example, we might want to change all of the points to be a shade of green. This can be done with a small change to the function call. To do this, we set the color aesthetic to the name of a color, such as “red”. However, unlike with variable aesthetics, the mapping needs to be done outside of the aes() function, but still within the geom_* function. Below is an example of the code to redo our plot with a different color; we use a color called “olivedrab”, which in print is much more aesthetically pleasing than its name might at first suggest.\n\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median'), color='olivedrab'))\n\n\n\n\nPlot of the largest 30 core-based statistical areas in the United States, showing their density and the median price to rent a one-bedroom apartment from the 2021 American community survey. The color of the points has been changed to a dark green called ‘olivedrab’.\n\n\n\n\nWhile minor, the changed notation for specifying fixed aesthetics is a common source of confusing errors for users new to the geometry of graphics, so be careful to follow the correct syntax of arguments as in the code above. One can interchange the fixed and variable aesthetic commands, and the relative order should not effect the output. Just be sure to put fixed terms after finishing the aes() command.\nWhile each geometry can have different required and optional aesthetics, the plotnine package tries as much as possible to use a common set of terms for the aesthetics in each geometry. We have already seen the x, y, and label aesthetics in the previous sections, and just introduced the color aesthetic. Color can also be used to change the color of a line plot or the color of the font in a text geometry. For applications such as the bar plot, we might want to modify both the border and interior colors of the bars; these are set separately by the color and fill aesthetics, respectively. The size aesthetic can be used to set the size of the points in a scatter plot or the font-size of the labels in a text geometry. The shape aesthetic is used to modify the shape of the points. An aesthetic named alpha controls the opacity of points, with a value of 1 being the default and 0 being completely invisible. Some of these, such as alpha, are most frequently used with fixed values, but if needed, almost all can be given a variable mapping as well.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#scales",
    "href": "02_graphics.html#scales",
    "title": "2  EDA I: Visualizing Data",
    "section": "2.5 Scales",
    "text": "2.5 Scales\nPython makes many choices for us automatically when creating any plot. In our example above, in which we set the color of the points to follow another variable in the dataset, Python handles the details of how to pick the specific colors and sizes. It has figured how large to make the axes, where to add tick marks, and where to draw grid lines. Letting Python deal with these details is convenient because it frees us up to focus on the data itself. Sometimes, such as when preparing to produce plots for external distribution, or when the default are particularly hard to interpret, it is useful to manually adjust these details. This is exactly what scales were designed for.\nEach aesthetic within the grammar of graphics is associated with a scale. Scales detail how a plot should relate aesthetics to the concrete, perceivable features in a plot. For example, a scale for the x aesthetic will describe the smallest and largest values on the x-axis. It will also code information about how to label the x-axis. Similarly, a color scale describes what colors corresponds to each category in a dataset and how to format a legend for the plot. In order to change or modify the default scales, we add an additional function to the code. The order of the scales relative to the geometries do not effect the output; by convention, scales are usually grouped after the geometries.\nFor example, a popular alternative to the default color palette shown in our previous plot is the function scale_color_cmap_d(). It constructs a set of colors that is color-blind friendly, looks nice when printed in black and white, and displays fine on bad projectors. After specifying that the color of a geometry should vary with a column in the dataset, we specify this color scale by adding the function as an extra line in the plot. An example is shown in the following code.\n\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median', color='quad')) +\n    scale_color_cmap_d())\n\n\n\n\nPlot of the largest 30 core-based statistical areas in the United States, showing their density and the median price to rent a one-bedroom apartment from the 2021 American community survey. Here, the points color based on the quadrant in which the city is found in the United States, with a color-blind friendly color scale.\n\n\n\n\nThe output shows that the colors are now given by a range from dark purple to bright yellow in place of the rainbow of colors in the default plot. As with the categories in the bar plot, the ordering of the unique colors is given by putting the categories in alphabetical order. Changing this requires modifying the dataset before passing it to the plot, something that we will discuss in the next chapter. Note that the _d at the end of the scale function indicates that the colors are used to create a set of mappings for a character variable (it stands for “discrete”). There is also a complimentary function scale_color_cmap() that produces a similar set of colors when making the color of the points change according to a numeric variable. The code below demonstrates the continuous case, where the population is treated as a numeric variable.\n\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median', color='pop')) +\n    scale_color_cmap())\n\nMany other scales exist to control a variety of aesthetics. For example, scale_size_area can be used to make the size of the points proportional to one of the other columns in a dataset. There are also several scales to control the x and y axes. For example, we can add scale_x_log10() and scale_y_log10() to a plot to produce values on a logarithmic scale, which can be very useful when working with heavily skewed datasets. We will use this in later chapters as needed.\nThe default scale for the x-axis is called scale_x_continuous. A corresponding function scale_y_continuous is the default for the y-axis. Adding these to a plot on their own has no visible effect. However, there are many helpful optional arguments that we can provide to these functions that change the way a plot is displayed. Setting breaks within one of these scales tells Python the number of labels to put on the axis. Also, making minor_breaks equal to None turns off the minor grid lines. We can set the value limits to a pair of numbers in order to describe the starting and ending range on a plot. Below is the code to produce a plot which shows the same data as our original scatter plot, but now with modified grid lines, axis labels, and vertical range.\n\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median')) +\n    scale_x_continuous(breaks=np.arange(0, 25000, 2500), minor_breaks=None) +\n    scale_y_continuous(limits=(0, 2000)))\n\n\n\n\nPlot of the largest 30 core-based statistical areas in the United States, showing their density and the median price to rent a one-bedroom apartment from the 2021 American community survey. Here, we adjust the standard breaks and limits on the x- and y-axes.\n\n\n\n\nFinally, there are two special scale types that can be useful for working with colors. In some cases we may already have a column in our dataset that explicitly describes the color of an observation; here, it would make sense to use these colors directly. To do that, we can add the scale scale_color_identity to the plot. Another type of scale that can be useful for colors is scale_color_manual. Here, it is possible to describe exactly which color should be used for each category. Below is the syntax for producing manually defined colors for each region in the CBSA dataset.\n\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median', color='quad')) +\n    scale_color_manual(values={\n        'NC': 'lightblue',\n        'NE': 'navy', \n        'S': 'peachpuff',\n        'W': 'wheat'\n    }))\n\nUsing manual colors is generally advisable in the case where there are well-known colors associated with the groups in the dataset. For example, when plotting data about political parties it may be helpful to use the colors traditionally associated with each party. The documentation for plotnine includes additional ways to customize visualizations using a variety of alternative scales.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#labels-and-themes",
    "href": "02_graphics.html#labels-and-themes",
    "title": "2  EDA I: Visualizing Data",
    "section": "2.6 Labels and Themes",
    "text": "2.6 Labels and Themes\nThroughout this chapter, we have seen a number of ways to create and modify data visualizations. One thing that we did not cover was how to label our axes. While many data visualization guides often stress the importance of labeling axes, it is often best to simply use the default labels provided by Python during the exploratory phase of analysis. These are useful for a number of reasons. First, they require minimal effort and make it easy to tweak axes, variables, and other settings without spending time tweaking with the labels. Secondly, the default labels use the variable names in our dataset. When writing code this is exactly what we need to know about a variable to use it in additional plots, models, and data manipulation tasks. Of course, once we want to present our results to others, it is essential to provide more detailed descriptions of the axes and legends in our plot. Fortunately, these can be added directly using the grammar of graphics.\nIn order to change the labels in a plot, we can use the labs function as an extra part of our plot. Inside the function, we assign labels to the names of aesthetic values that we want to describe. Leaving a value unspecified will keep the default value in place. Labels for the x-axis and y-axis will be placed on the sides of the plot. Labels for other aesthetics such as size and color will be placed in the legend. We can also add title, subtitle, and a caption value to the labs command to add addition information to describe the plot as a whole. Here is an example of our scatter plot with color that includes a variety of different labels.\n\n(ggplot(cbsa) +\n    geom_point(aes(x='density', y='rent_1br_median', color='quad')) +\n    labs(\n        x='Density (people per km^2)',\n        y='Median Rent, 1BD Apartment (USD)',\n        color='Quadrant',\n        title='Relationship of CBSA Density and Housing Costs',\n        subtitle='U.S. Core-based Statistical Areas',\n        caption='Data from the United States Census Bureau'\n    ))\n\n\n\n\nPlot of the largest 30 core-based statistical areas in the United States, showing their density and the median price to rent a one-bedroom apartment from the 2021 American community survey. Here, we add custom labels and titles to the plot.\n\n\n\n\nAnother way to prepare our graphics for distribution is to modify the theme of a plot. Themes effect the way that the plot elements such as the axis labels, ticks, and background look. By default throughout this book, we have set the default plot to theme_minimal. We think that this is a great choice for the exploration of a dataset. As the name implies, it removes most of the clutter of other choices while keeping grid lines and other visual cues to help with interpretation. When presenting information for external publication, we often prefer to use a theme that is based on the work of Edward Tufte, a well-known scholar in the field of data visualization. To set the theme, we can add theme_minimal() or another theme function to our plot, as in the example below.\n\n# Set minimal theme as default\ntheme_set(theme_minimal())\n\n&lt;plotnine.themes.theme_minimal.theme_minimal at 0x10b63fe00&gt;\n\n\nNow, whenever we construct a plot it will use the newly assigned theme. Minimal themes are designed to use as little “ink” as possible, thus focusing the reader on the data [4]. They can be a bit too minimal when first exploring the dataset, but can be a great tool for presenting our results. Again, there are many resources online to customize according to our needs.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#conventions-for-graphics-code",
    "href": "02_graphics.html#conventions-for-graphics-code",
    "title": "2  EDA I: Visualizing Data",
    "section": "2.7 Conventions for Graphics Code",
    "text": "2.7 Conventions for Graphics Code\nAs a final point, note that there is a convention for simplifying the plotting command. Often, each layer will use the same x and y variables. It is possible to specify these just once in the ggplot function, and they will be used by default in all other layers. Also, we can drop the x = and y = if we put these options first. This can greatly simplify the amount of code needed to create a plot, therefore making the code easier to edit and understand. Below is an example of layering together the geom_point and geom_text with this inheritance structure.\n\n(ggplot(cbsa, aes('density', 'rent_1br_median')) +\n    geom_point() +\n    geom_text(aes(label='name'), nudge_y=50, size=8))\n\nThese changes are optional. It is perfectly fine to write them as we did earlier. It is important, though, to be able to recognize these conventions as we will make use of this convention throughout the rest of the book. We will also see this format when searching through documentation or help pages online. Finally, in the first chapter we stressed the importance of following a few style guidelines about our code. Here are three additional formatting rules that apply specifically to building graphics in Python:\n\nindent the ggplot command by four spaces when inside parentheses\nindent every line below ggplot by four spaces\nalways add each layer of a plot as a new line\n\nAs with our original set of style guidelines, we think using these formatting rules will make life a lot easier. We will model their application in the code throughout the book.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#extensions",
    "href": "02_graphics.html#extensions",
    "title": "2  EDA I: Visualizing Data",
    "section": "2.8 Extensions",
    "text": "2.8 Extensions\nWe have covered the most important elements—data, geometries, aesthetics, scales, themes—that make up data visualizations using the grammar of graphics as implemented in plotnine. At the end of Chap. 9, we will show how to make use of facets to show multiple plots next to one another. The three remaining elements are position, statistics, and coordinate systems. These are not as crucial to the basic usage of the grammar of graphics, but can be useful for more advanced applications or in specific situations where the defaults are not ideal. More details can be found in the plotnine documentation [1]. While we have covered the idea behind the main other elements, we have only shown a smaller number of the larger set of available geometries and scales.\nA full set can be found in the (continually updated) documentation pages and tutorials available online.\nIn addition to the programming aspects, there are many great references that can help inspire new approaches to data visualization. Classic and still very relevant references are Edward Tufte’s texts on data visualization [4] [5] [6] [7] [8]. Bremer and Wu’s Data Sketches provides a more in-depth study of several complex datasets and the kinds of modern, often interactive, visualizations they used to study them [9]. A more playful, but just as useful, take on data visualization is given in Dear Data, by Lupi and Posavec [10]. They show over one-hundred hand sketched data visualizations that were originally shared as postcards. These help imagine the possibilities that are not overly constrained to what is computationally easy to write. Finally, Cole Nussbaumer Knaflic’s Storytelling with Data, despite having a subtitle referencing business applications, provides a very humanistic approach to exploratory data analysis through visualizations along with a lot of useful and practical advice [11]. The field of data visualization is large and quickly growing. These are just a few examples that we have personally found to be useful references and sources of great practices and ideas.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "02_graphics.html#references",
    "href": "02_graphics.html#references",
    "title": "2  EDA I: Visualizing Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Lavanya, A, Gaurav, L, Sindhuja, S, Seam, H, Joydeep, M, Uppalapati, V, Ali, W and SD, V S (2023 ). Assessing the performance of python data visualization libraries: A review. Int. J. Comput. Eng. Res. Trends. 10 28–39\n\n\n[2] Wilkinson, L (2012 ). The Grammar of Graphics. Springer\n\n\n[3] Jacks, D S (2019 ). From boom to bust: A typology of real commodity prices in the long run. Cliometrica. Springer. 13 201–20\n\n\n[4] Tufte, E (1987 ). The Visual Display of Quantitative Information. Graphics Press\n\n\n[5] Tufte, E (1990 ). Envisioning Information. Graphics Press\n\n\n[6] Tufte, E (1997 ). Visual Explanations: Images and Quantities, Evidence and Narrative. Graphics Press\n\n\n[7] Tufte, E (2006 ). Beautiful Evidence. Graphics Press\n\n\n[8] Tufte, E (2020 ). Seeing with Fresh Eyes: Meaning, Space, Data, Truth. Graphics Press\n\n\n[9] Bremer, N and Wu, S (2021 ). Data Sketches: A Journey of Imagination, Exploration, and Beautiful Data Visualizations. CRC Press\n\n\n[10] Lupi, G and Posavec, S (2016 ). Dear Data. CRC Press\n\n\n[11] Knaflic, C N (2015 ). Storytelling with Data: A Data Visualization Guide for Business Professionals. John Wiley & Sons",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EDA I: Visualizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html",
    "href": "03_verbs.html",
    "title": "3  EDA II: Organizing Data",
    "section": "",
    "text": "3.1 Introduction\nIn the previous chapter, we learned how to build data visualizations using the grammar of graphics and the plotnine package. We saw how these visual techniques allow us to quickly understand the shape and structure of two datasets that have a relatively small number of rows. In order to go farther, we need to be able to modify data directly within Python. The modified dataset can then be used to create new plots, summary tables, or as the input to modeling techniques. For example, we might want to take a subset of the rows of our data to focus on a specific part of the dataset. Or, we might want to create summaries of some of the columns according to different grouping variables defined in the data.\nWe will use methods called data operations to manipulate datasets in Python. All of the data operations shown in this chapter come from the pandas package [1]. Each operation takes as an input one DataFrame—the object type introduced in Chapter 1 that is used to represents a dataset—and returns a new DataFrame. There are many operations available in pandas, though most are a minor variant or specific application of another operation. In this chapter we will see seven of them, all of which are related to selecting, arranging, and creating rows and columns. First, we will see how to select a subset of rows from the original dataset using boolean indexing and .iloc[]. Then, we use column selection to select a subset of columns from the original dataset. Next, we use the .sort_values() method to sort the rows of our dataset and .assign() to add new columns to the data. Finally, we combine groups of the dataset using the .groupby() method along with aggregation functions. Along with these operations, we will also introduce additional helper functions that assist in making use of their different applications. We will also highlight how each of these operations can be chained together to create new data visualizations.\nThe set of data operations all have a similar syntax. They typically start with the original DataFrame and the output is always a new DataFrame. Additional options control how the function modifies the data. Because the operations always start with a dataset and return a dataset, we can use method chaining (with the dot operator) to pass a dataset from one operation to the next in order to string together a sequence of data transformations. Importantly, operations never modify the original data by default. They operate instead on a copy of the data. To save the changes, it is necessary to make an explicit name for the new dataset and assign it with the equals operator. Often, though, we will directly chain the resulting transformed version of the data directly into the ggplot command, without the need to make a newly named version of the data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#choosing-rows",
    "href": "03_verbs.html#choosing-rows",
    "title": "3  EDA II: Organizing Data",
    "section": "3.2 Choosing Rows",
    "text": "3.2 Choosing Rows\nIt is often useful to take a subset of the rows of an existing dataset. For example, we might want to build a model on a certain sub-population or highlight a particular part of the data in a plot. In the previous chapter we looked at the 30 largest CBSA regions. This dataset was created by taking a subset of the larger, entire set of CBSA regions. A straightforward way to take a subset of rows is to indicate the specific row numbers that we want to extract. In order to select rows by row numbers, we use the .iloc[] indexer, followed by the numbers of the rows we want. The code chunk below shows an example of using the iloc indexer to take the second, fifth, and seventh rows of the data (remembering that Python uses 0-based indexing).\n\ncbsa.iloc[[1, 4, 6]]\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\n\n\n1\nLos Angeles\n31080\nW\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\nPacific\n\n\n4\nHouston\n26420\nS\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\nWest South Central\n\n\n6\nPhiladelphia\n37980\nNE\n-75.302635\n39.905213\n6.215222\n506.068130\n42.6\n79070\n71.1\n1083\n30.0\nMiddle Atlantic\n\n\n\n\n\n\n\nAs mentioned above, the code here does not change the dataset cbsa itself. It still has all 916 rows even after running the code above. If we want to create a new dataset with just these three regions, we need to explicitly name and assign it. For example, below is an example of how we would create a dataset of the first five cities. We have named the new dataset cbsa_first_five.\n\ncbsa_first_five = cbsa.iloc[0:5]\n\nThe most common application of the .iloc[] indexer is to take the first set of rows in a dataset. This can be done, for example, after arranging the rows by another variable, something we will see later in the chapter. There are also convenient methods for selecting the first or last rows: .head() and .tail(). For example, in the code below, we show another way to select the first five rows of the dataset.\n\ncbsa.head(5)\n\nWe can also take a subset of the rows of a dataset by selecting rows based on conditions about the various data columns. To do this we use boolean indexing or the .query() method, which accepts a statement about variables in the dataset. We can write the condition directly in terms of the variable names used in the dataset. Only rows where the statements are true will be returned. For example, below is an example of how we use boolean indexing to select regions that have more than 5 million people living in them.\n\ncbsa[cbsa['pop'] &gt; 5]\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\n\n\n0\nNew York\n35620\nNE\n-74.101056\n40.768770\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\nMiddle Atlantic\n\n\n1\nLos Angeles\n31080\nW\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\nPacific\n\n\n2\nChicago\n16980\nNC\n-87.958820\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\nEast North Central\n\n\n3\nDallas\n19100\nS\n-96.970508\n32.849480\n7.543340\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\nWest South Central\n\n\n4\nHouston\n26420\nS\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\nWest South Central\n\n\n5\nWashington\n47900\nS\n-77.513075\n38.812484\n6.332069\n363.732689\n42.4\n111252\n67.4\n1601\n28.8\nSouth Atlantic\n\n\n6\nPhiladelphia\n37980\nNE\n-75.302635\n39.905213\n6.215222\n506.068130\n42.6\n79070\n71.1\n1083\n30.0\nMiddle Atlantic\n\n\n7\nMiami\n33100\nS\n-80.506307\n26.155369\n6.105897\n430.103162\n43.9\n62870\n60.8\n1230\n36.8\nSouth Atlantic\n\n\n8\nAtlanta\n12060\nS\n-84.399567\n33.691787\n6.026734\n263.275821\n41.9\n75267\n67.3\n1181\n30.3\nSouth Atlantic\n\n\n\n\n\n\n\nBecause population is given in millions of people, we filtered based on pop being greater than 5. The output dataset has only 9 rows, compared to the 916 in the original data. Other comparisons can be done with &lt;, &gt;= and &lt;=. There is also a special method called .between() that is often useful. For example, below are all of the regions that have between 1 and 2 million people living in them.\n\ncbsa[cbsa['pop'].between(1, 2)]\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\n\n\n35\nSan Jose\n41940\nW\n-121.372676\n36.908496\n1.995351\n286.417670\n41.4\n138370\n58.5\n2227\n27.8\nPacific\n\n\n36\nNashville\n34980\nS\n-86.645966\n36.118950\n1.960999\n131.347255\n40.6\n72537\n68.7\n1083\n29.1\nEast South Central\n\n\n37\nVirginia Beach\n47260\nS\n-76.542499\n36.785028\n1.791198\n183.098156\n40.0\n71612\n64.5\n998\n30.7\nSouth Atlantic\n\n\n38\nProvidence\n39300\nNE\n-71.400290\n41.718250\n1.668019\n381.965971\n43.0\n74422\n66.5\n830\n29.0\nNew England\n\n\n39\nJacksonville\n27260\nS\n-81.791535\n30.236294\n1.581680\n178.083158\n41.6\n66664\n67.3\n950\n30.7\nSouth Atlantic\n\n\n40\nMilwaukee\n33340\nNC\n-88.172713\n43.176122\n1.571784\n407.097337\n41.7\n67448\n64.2\n809\n28.6\nEast North Central\n\n\n41\nOklahoma City\n36420\nS\n-97.505031\n35.428987\n1.412874\n97.737719\n40.3\n63351\n66.4\n748\n28.0\nWest South Central\n\n\n42\nRaleigh\n39580\nS\n-78.461010\n35.756718\n1.391801\n250.282432\n41.8\n83581\n69.6\n1104\n28.1\nSouth Atlantic\n\n\n43\nMemphis\n32820\nS\n-89.869496\n35.023319\n1.335291\n109.877481\n42.1\n56926\n62.2\n826\n31.2\nEast South Central\n\n\n44\nRichmond\n40060\nS\n-77.446702\n37.412085\n1.303212\n112.159801\n42.2\n74592\n68.7\n1035\n29.9\nSouth Atlantic\n\n\n45\nLouisville/Jefferson County\n31140\nS\n-85.681818\n38.303038\n1.279554\n150.206269\n42.1\n64533\n70.2\n766\n27.2\nEast South Central\n\n\n46\nNew Orleans\n35380\nS\n-89.955883\n29.924412\n1.269037\n101.673175\n42.1\n57656\n65.7\n887\n33.5\nWest South Central\n\n\n47\nSalt Lake City\n41620\nW\n-113.011235\n40.471039\n1.244671\n59.447488\n38.9\n82506\n72.9\n1038\n28.4\nMountain\n\n\n48\nHartford\n25540\nNE\n-72.577300\n41.734217\n1.213324\n302.037628\n43.7\n82359\n70.7\n982\n30.5\nNew England\n\n\n49\nBuffalo\n15380\nNE\n-78.736788\n42.909972\n1.162523\n283.478105\n42.5\n62282\n70.7\n714\n29.5\nMiddle Atlantic\n\n\n50\nBirmingham\n13820\nS\n-86.729411\n33.402220\n1.109895\n93.840306\n41.9\n62873\n72.5\n871\n29.4\nEast South Central\n\n\n51\nRochester\n40380\nNE\n-77.507198\n42.965737\n1.088373\n125.643564\n42.9\n65812\n71.3\n806\n30.6\nMiddle Atlantic\n\n\n52\nGrand Rapids\n24340\nNC\n-85.439269\n43.072071\n1.081665\n152.144573\n40.4\n70347\n77.6\n816\n29.0\nEast North Central\n\n\n53\nTucson\n46060\nW\n-111.789247\n32.099121\n1.035063\n43.455454\n40.4\n59215\n66.3\n766\n30.6\nMountain\n\n\n55\nTulsa\n46140\nS\n-96.164839\n36.249229\n1.009982\n60.377114\n41.5\n60866\n67.7\n734\n27.0\nWest South Central\n\n\n56\nFresno\n23420\nW\n-119.653393\n36.758521\n1.003150\n64.453980\n39.8\n61276\n54.5\n846\n32.5\nPacific\n\n\n\n\n\n\n\nSometimes we want to filter based on a character variable in order to select a set of rows based on a specified set of categories. To do this, we can use the .isin() method to select specific categories that we want selected in the output. Below we show an example of selecting only regions in the Northeast (NE) and South (S) quadrants.\n\ncbsa[cbsa['quad'].isin(['NE', 'S'])]\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\n\n\n0\nNew York\n35620\nNE\n-74.101056\n40.768770\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\nMiddle Atlantic\n\n\n3\nDallas\n19100\nS\n-96.970508\n32.849480\n7.543340\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\nWest South Central\n\n\n4\nHouston\n26420\nS\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\nWest South Central\n\n\n5\nWashington\n47900\nS\n-77.513075\n38.812484\n6.332069\n363.732689\n42.4\n111252\n67.4\n1601\n28.8\nSouth Atlantic\n\n\n6\nPhiladelphia\n37980\nNE\n-75.302635\n39.905213\n6.215222\n506.068130\n42.6\n79070\n71.1\n1083\n30.0\nMiddle Atlantic\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n927\nSweetwater\n45020\nS\n-100.405986\n32.303445\n0.014727\n6.220819\n41.2\n44700\n62.6\n577\n24.8\nWest South Central\n\n\n928\nPecos\n37780\nS\n-103.669712\n31.430030\n0.014667\n1.705080\n39.1\n53448\n74.7\n818\n27.4\nWest South Central\n\n\n929\nZapata\n49820\nS\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\nWest South Central\n\n\n932\nVernon\n46900\nS\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\nWest South Central\n\n\n933\nLamesa\n29500\nS\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\nWest South Central\n\n\n\n\n469 rows × 13 columns\n\n\n\nAs mentioned in the introduction, we can chain together multiple operations to produce more complex logic. For example, consider finding all of the regions with more than 1 million people that are in the Northeast quadrant. We can do this by applying multiple boolean conditions, as in the example below. We can combine multiple conditions using the & (and) and | (or) operators, making sure to use parentheses around each condition.\n\ncbsa[(cbsa['pop'] &gt; 1) & (cbsa['quad'].isin(['NE']))]\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\n\n\n0\nNew York\n35620\nNE\n-74.101056\n40.768770\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\nMiddle Atlantic\n\n\n6\nPhiladelphia\n37980\nNE\n-75.302635\n39.905213\n6.215222\n506.068130\n42.6\n79070\n71.1\n1083\n30.0\nMiddle Atlantic\n\n\n9\nBoston\n14460\nNE\n-71.099912\n42.555194\n4.912030\n517.827702\n42.2\n99039\n66.4\n1390\n29.5\nNew England\n\n\n26\nPittsburgh\n38300\nNE\n-79.830762\n40.437868\n2.366544\n171.198592\n43.3\n65894\n74.9\n743\n27.1\nMiddle Atlantic\n\n\n38\nProvidence\n39300\nNE\n-71.400290\n41.718250\n1.668019\n381.965971\n43.0\n74422\n66.5\n830\n29.0\nNew England\n\n\n48\nHartford\n25540\nNE\n-72.577300\n41.734217\n1.213324\n302.037628\n43.7\n82359\n70.7\n982\n30.5\nNew England\n\n\n49\nBuffalo\n15380\nNE\n-78.736788\n42.909972\n1.162523\n283.478105\n42.5\n62282\n70.7\n714\n29.5\nMiddle Atlantic\n\n\n51\nRochester\n40380\nNE\n-77.507198\n42.965737\n1.088373\n125.643564\n42.9\n65812\n71.3\n806\n30.6\nMiddle Atlantic\n\n\n\n\n\n\n\nThe results now include only 8 regions. We can use all of these different subsets to create new visualizations, models, and summaries. Let’s now see how we can use data subsets to create new and interesting types of data visualizations.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#data-and-layers",
    "href": "03_verbs.html#data-and-layers",
    "title": "3  EDA II: Organizing Data",
    "section": "3.3 Data and Layers",
    "text": "3.3 Data and Layers\nWe know that we could use our newly created subsets of data with plotnine to create visualizations such as those in the previous chapter on different parts of a larger dataset. It is also possible to create new types of visualizations that were previously inaccessible. These will let us understand how a subset of the data relates to the larger collection of data. To start, we will create a dataset that consists only of those regions that have more than 2 million people living in them in order to understand the relationships between larger cities. We store the data in an object called cbsa_large.\n\ncbsa_large = cbsa[cbsa['pop'] &gt; 2]\ncbsa_large\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\n\n\n0\nNew York\n35620\nNE\n-74.101056\n40.768770\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\nMiddle Atlantic\n\n\n1\nLos Angeles\n31080\nW\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\nPacific\n\n\n2\nChicago\n16980\nNC\n-87.958820\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\nEast North Central\n\n\n3\nDallas\n19100\nS\n-96.970508\n32.849480\n7.543340\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\nWest South Central\n\n\n4\nHouston\n26420\nS\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\nWest South Central\n\n\n5\nWashington\n47900\nS\n-77.513075\n38.812484\n6.332069\n363.732689\n42.4\n111252\n67.4\n1601\n28.8\nSouth Atlantic\n\n\n6\nPhiladelphia\n37980\nNE\n-75.302635\n39.905213\n6.215222\n506.068130\n42.6\n79070\n71.1\n1083\n30.0\nMiddle Atlantic\n\n\n7\nMiami\n33100\nS\n-80.506307\n26.155369\n6.105897\n430.103162\n43.9\n62870\n60.8\n1230\n36.8\nSouth Atlantic\n\n\n8\nAtlanta\n12060\nS\n-84.399567\n33.691787\n6.026734\n263.275821\n41.9\n75267\n67.3\n1181\n30.3\nSouth Atlantic\n\n\n9\nBoston\n14460\nNE\n-71.099912\n42.555194\n4.912030\n517.827702\n42.2\n99039\n66.4\n1390\n29.5\nNew England\n\n\n10\nPhoenix\n38060\nW\n-112.069027\n33.186173\n4.787811\n126.558955\n40.7\n72211\n66.5\n1065\n29.3\nMountain\n\n\n11\nSan Francisco\n41860\nW\n-122.166182\n37.780765\n4.725584\n712.992772\n41.9\n118547\n58.4\n1940\n28.3\nPacific\n\n\n12\nRiverside\n40140\nW\n-116.127488\n34.549836\n4.580402\n64.511974\n40.2\n73424\n65.2\n1120\n34.0\nPacific\n\n\n13\nDetroit\n19820\nNC\n-83.233992\n42.718660\n4.382832\n421.567420\n42.9\n66878\n73.1\n811\n29.4\nEast North Central\n\n\n14\nSeattle\n42660\nW\n-121.853072\n47.554512\n3.971125\n256.189989\n40.6\n97675\n64.7\n1478\n29.1\nPacific\n\n\n15\nMinneapolis\n33460\nNC\n-93.273933\n45.101345\n3.659156\n188.297448\n41.6\n87397\n75.0\n1035\n28.5\nWest North Central\n\n\n16\nSan Diego\n41740\nW\n-116.734692\n33.034088\n3.296317\n298.781488\n39.8\n88240\n55.7\n1518\n33.4\nPacific\n\n\n17\nTampa\n45300\nS\n-82.405645\n28.153011\n3.146074\n452.790117\n42.8\n61121\n67.3\n1045\n32.1\nSouth Atlantic\n\n\n18\nDenver\n19740\nW\n-104.896367\n39.434353\n2.936665\n135.070927\n40.4\n88512\n68.2\n1340\n30.4\nMountain\n\n\n19\nBaltimore\n12580\nS\n-76.579833\n39.338100\n2.837237\n404.661335\n42.4\n87513\n70.6\n1132\n30.0\nSouth Atlantic\n\n\n20\nSt. Louis\n41180\nNC\n-90.350830\n38.734976\n2.815627\n134.615515\n42.2\n69635\n73.4\n778\n28.1\nWest North Central\n\n\n21\nOrlando\n36740\nS\n-81.361852\n28.433502\n2.632721\n253.043039\n41.0\n65086\n63.5\n1156\n33.0\nSouth Atlantic\n\n\n22\nCharlotte\n16740\nS\n-80.795443\n35.167416\n2.625282\n177.393426\n41.8\n69559\n68.6\n1074\n28.1\nSouth Atlantic\n\n\n23\nSan Antonio\n41700\nS\n-98.601799\n29.428308\n2.529453\n132.337510\n40.0\n65355\n66.7\n936\n30.1\nWest South Central\n\n\n24\nPortland\n38900\nW\n-122.479832\n45.597144\n2.493429\n141.406821\n41.2\n82901\n66.1\n1262\n30.2\nPacific\n\n\n25\nSacramento\n40900\nW\n-120.999752\n38.781108\n2.379368\n173.268846\n41.5\n81264\n62.7\n1156\n32.1\nPacific\n\n\n26\nPittsburgh\n38300\nNE\n-79.830762\n40.437868\n2.366544\n171.198592\n43.3\n65894\n74.9\n743\n27.1\nMiddle Atlantic\n\n\n27\nCincinnati\n17140\nNC\n-84.480196\n39.100073\n2.244329\n187.276368\n41.7\n70308\n71.2\n714\n27.3\nEast North Central\n\n\n28\nAustin\n12420\nS\n-97.654303\n30.261635\n2.234300\n201.447732\n39.6\n85398\n64.0\n1216\n28.8\nWest South Central\n\n\n29\nLas Vegas\n29820\nW\n-115.013625\n36.213907\n2.231147\n106.878725\n41.0\n64210\n57.1\n1004\n31.8\nMountain\n\n\n30\nKansas City\n28140\nNC\n-94.445801\n38.934698\n2.176124\n114.046901\n41.4\n73299\n69.4\n905\n27.4\nWest North Central\n\n\n31\nColumbus\n18140\nNC\n-82.835789\n39.967336\n2.122480\n169.140160\n40.6\n71020\n65.2\n882\n26.9\nEast North Central\n\n\n33\nIndianapolis\n26900\nNC\n-86.206780\n39.746760\n2.089990\n186.089808\n41.2\n67330\n70.0\n832\n28.7\nEast North Central\n\n\n34\nCleveland\n17460\nNC\n-81.685431\n41.375244\n2.084462\n399.998911\n43.4\n61320\n68.9\n721\n28.3\nEast North Central\n\n\n\n\n\n\n\nAs we saw in the previous chapter, we can create a scatter plot of the median rental price for a 1 bedroom apartment and the median household income using the following block of code.\n\n(ggplot(cbsa_large, aes('rent_1br_median', 'hh_income_median')) +\n    geom_point())\n\nOne of the core ideas behind the grammar of graphics is that complex visualizations can be constructed by layering relatively simple elements on top of one another. What if we wanted to put together two layers where one layer uses the cbsa dataset and the other uses cbsa_large? To do this, we can override the default dataset in any geometry layer with the option data =. This will use a different dataset within a particular layer. For example, the code below would produce two sets of points. The first layer (second line of the code) would include all of the regions and the second layer (third line) would only have data from the set of the largest regions.\n\n(ggplot(cbsa, aes('rent_1br_median', 'hh_income_median')) +\n    geom_point() +\n    geom_point(data=cbsa_large))\n\nThe resulting plot, however, would not look any different than it would if we were just to plot all of the cities together. The second layer of points would sit unassumingly on top of the rest of the data, with no way of seeing that the largest cities have two sets of black points plotted on top of one another. To rectify this, we can color each layer a different color in order to distinguish them from one another. We will do this by highlighting the largest cities in navy blue, while making the rest of the points a light grey. This plot can be further built up by showing the names of just the rows of the largest cities (including all of the cities would take up far too much space). These changes are shown in the code below.\n\n(ggplot(cbsa, aes('rent_1br_median', 'hh_income_median')) +\n    geom_point(color='grey') +\n    geom_point(color='navy', data=cbsa_large) +\n    geom_text(aes(label='name'), color='navy', data=cbsa_large, \n              size=8, nudge_y=5000))\n\n\n\n\nPlot of the core-based statistical areas in the United States, showing the median price to rent a one-bedroom apartment and the median household income as reported by the 2021 American community survey. Here, regions with greater than two million residents are highlighted in navy and their short names are added as labels.\n\n\n\n\nThe output of our new plot shows both the largest regions and how they relate to the full set of regions. In general, there is a positive relationship between household income and median apartment rent. This relationship is not a perfect line, though, with some cities appearing relatively more or less affordable, at least for a household with a median income renting a one-bedroom apartment. Also, the largest regions tend to be among the most expensive regions. However, there are smaller regions in the upper right-hand corner of the plot that appear close to the largest cities.\nStepping back from the data itself, we see here already how a relatively small set of commands can be put together in different ways to build a variety of plots. Already, we are making further progress towards building informative and layered graphics with our dataset.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#selecting-columns",
    "href": "03_verbs.html#selecting-columns",
    "title": "3  EDA II: Organizing Data",
    "section": "3.4 Selecting Columns",
    "text": "3.4 Selecting Columns\nNow that we have seen how to select a subset of the rows in a dataset, another step would be to select a subset of the columns. To do this, we will make use of column selection with square brackets or the .loc[] indexer. We pass it the names of the variables we want to keep in the output dataset, in the (possibly new) order that we want the columns to be arranged in. Here, for example, is a new version of the cbsa dataset containing only the name of the region, the population, the longitude and latitude, and the household median income.\n\ncbsa[['name', 'pop', 'lon', 'hh_income_median']]\n\n\n\n\n\n\n\n\nname\npop\nlon\nhh_income_median\n\n\n\n\n0\nNew York\n20.011812\n-74.101056\n86445\n\n\n1\nLos Angeles\n13.202558\n-118.148722\n81652\n\n\n2\nChicago\n9.607711\n-87.958820\n78790\n\n\n3\nDallas\n7.543340\n-96.970508\n76916\n\n\n4\nHouston\n7.048954\n-95.401574\n72551\n\n\n...\n...\n...\n...\n...\n\n\n928\nPecos\n0.014667\n-103.669712\n53448\n\n\n929\nZapata\n0.013945\n-99.168533\n34406\n\n\n931\nCraig\n0.013240\n-108.207523\n58583\n\n\n932\nVernon\n0.012887\n-99.240853\n45262\n\n\n933\nLamesa\n0.012371\n-101.947637\n42778\n\n\n\n\n916 rows × 4 columns\n\n\n\nWe will find that column selection is not as immediately useful as the row filtering when working interactively in Python. For many tasks, having extra variables around does not effect data visualizations or data models. As we have seen already in the grammar of graphics, the plot always shows all of the available rows, but only uses those columns in the data that we have explicitly mapped to aesthetics. Selecting columns, though, can be useful for displaying results. This is particularly the case when writing code that will be printed—the code in this book is a great example. As we saw above, the household median income column was cut off in the original output but is now visible in the selected dataset version. Removing and reordering unneeded columns will be useful throughout this book when we want to show columns that would otherwise be hidden given the limited width of the page. Finally, removing columns can be useful as a first step in the more complex data reorganization tasks shown in the following chapter. We will discuss these strategies more as the need arises in the following chapters.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#arranging-rows",
    "href": "03_verbs.html#arranging-rows",
    "title": "3  EDA II: Organizing Data",
    "section": "3.5 Arranging Rows",
    "text": "3.5 Arranging Rows\nThe indexing and boolean filtering operations determine a subset of rows to keep from the original dataset. The .sort_values() method keeps all of the original data but re-orders its rows. It works by giving it one or more variable names. The method sorts the data by the first variable from smallest to largest (or alphabetically for character variables). In the case of ties, the second variable, if given, is used. More variables can be given to further break additional ties. If any ties are not broken by the given variables, the rows in question will retain the order given in the original input dataset. The following block of code provides an example in which we order the dataset first by division and then by pop. We will use column selection to display the columns of the data that are used in the arranging of the rows.\n\ncbsa.sort_values(['division', 'pop'])[['name', 'division', 'pop']]\n\n\n\n\n\n\n\n\nname\ndivision\npop\n\n\n\n\n874\nConnersville\nEast North Central\n0.023393\n\n\n870\nScottsburg\nEast North Central\n0.024290\n\n\n847\nGreensburg\nEast North Central\n0.026466\n\n\n838\nNorth Vernon\nEast North Central\n0.027619\n\n\n835\nMacomb\nEast North Central\n0.027743\n\n\n...\n...\n...\n...\n\n\n41\nOklahoma City\nWest South Central\n1.412874\n\n\n28\nAustin\nWest South Central\n2.234300\n\n\n23\nSan Antonio\nWest South Central\n2.529453\n\n\n4\nHouston\nWest South Central\n7.048954\n\n\n3\nDallas\nWest South Central\n7.543340\n\n\n\n\n916 rows × 3 columns\n\n\n\nIn the new dataset all of the regions in the “East North Central” division come up first as a result of this division being first alphabetically. Within each group, the items are sorted from the lowest to highest population. Any ordering can be reversed (i.e., from the highest to the lowest value) by setting the ascending parameter to False. Below is an example that repeats the sorting above, but this time in descending order of population.\n\ncbsa.sort_values(['division', 'pop'], ascending=[True, False])[['name', 'division', 'pop']]\n\n\n\n\n\n\n\n\nname\ndivision\npop\n\n\n\n\n2\nChicago\nEast North Central\n9.607711\n\n\n13\nDetroit\nEast North Central\n4.382832\n\n\n27\nCincinnati\nEast North Central\n2.244329\n\n\n31\nColumbus\nEast North Central\n2.122480\n\n\n33\nIndianapolis\nEast North Central\n2.089990\n\n\n...\n...\n...\n...\n\n\n927\nSweetwater\nWest South Central\n0.014727\n\n\n928\nPecos\nWest South Central\n0.014667\n\n\n929\nZapata\nWest South Central\n0.013945\n\n\n932\nVernon\nWest South Central\n0.012887\n\n\n933\nLamesa\nWest South Central\n0.012371\n\n\n\n\n916 rows × 3 columns\n\n\n\nIn the result here, “Chicago” has been placed at the top of the dataset, followed by “Detroit” and “Cincinnati”. These are the three largest regions in the “East North Central” division of the United States. A particularly useful application of this kind of sorting with .sort_values() is to pair it with .head() or .iloc[]. Below, for example, is the code to select the 30 regions in our dataset that have the largest population.\n\ncbsa.sort_values('pop', ascending=False).head(30)\n\nThis is the code that we used to create the smaller version of the cbsa dataset that was used throughout Chapter 2. As we can see, putting these operations together offers a powerful way to explore our data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#group-by-and-aggregation",
    "href": "03_verbs.html#group-by-and-aggregation",
    "title": "3  EDA II: Organizing Data",
    "section": "3.6 Group By and Aggregation",
    "text": "3.6 Group By and Aggregation\nThe two operations, aggregation and .groupby(), are often used in sequence to create interesting new summarized versions of a dataset. We will start with aggregation operations. These functions collapse the rows of a DataFrame using summary functions. To apply aggregation requires specifying exactly how the data should be summarized and in terms of which columns the summary should be done.\nAs with every operation, we start by taking a dataset and applying an aggregation method. We need to specify which aggregation function to use, such as .mean(), .min(), .max(), .sum(), or .count(). An example will help illustrate how this works. The code below computes the mean (in other words, average) population in each of the CBSA regions.\n\ncbsa['pop'].mean()\n\nnp.float64(0.3379417838427948)\n\n\nFrom the output, we see that the average CBSA region has around 0.33 million residents. We are not constrained to computing just one summary at once, however. We can compute multiple summaries using the .agg() method. To do this, we can pass a dictionary specifying which aggregation functions to apply to which columns. For example, here we compute the mean value of the household median income, the sum of the population across all regions, the maximum median age, and a total count of the number of rows.\n\ncbsa.agg({\n    'hh_income_median': 'mean',\n    'pop': 'sum', \n    'age_median': 'max',\n    'name': 'count'\n}).rename({'hh_income_median': 'avg_hh_income_median',\n           'pop': 'sum_pop',\n           'age_median': 'max_age_median', \n           'name': 'count'})\n\navg_hh_income_median    58466.081878\nsum_pop                   309.554674\nmax_age_median             48.800000\ncount                     916.000000\ndtype: float64\n\n\nSummarizing the dataset to a single set of values can be useful for understanding the general trends in a dataset or highlighting outliers. However, the real power of aggregation comes when we pair it with the .groupby() method. This will allow us to produce summaries within one or more grouping variables in our dataset.\nThe .groupby() method is a special kind of data operation. The method takes one or more names of variables in the dataset and creates a special grouped object that allows subsequent operations to be applied separately for each group. It makes no direct, visible changes to the dataset and is used only to change the way that other data operations treat the dataset. For example, when we use the .groupby() method, subsequent uses of aggregation functions will produce a summary that describes the properties of variables within the variable used for grouping. The variable name(s) placed inside of the .groupby() method indicate which variable(s) should be used for the groups. For example, we recompute the summary from the previous code block, but this time group the data by the division.\n\n(cbsa.groupby('division')\n    .agg({'hh_income_median': 'mean',\n          'pop': 'sum',\n          'age_median': 'max',\n          'name': 'count'})\n    .rename(columns={'hh_income_median': 'avg_hh_income_median',\n                     'pop': 'sum_pop', \n                     'age_median': 'max_age_median',\n                     'name': 'count'}))\n\n\n\n\n\n\n\n\navg_hh_income_median\nsum_pop\nmax_age_median\ncount\n\n\ndivision\n\n\n\n\n\n\n\n\nEast North Central\n58998.287500\n43.308362\n46.6\n160\n\n\nEast South Central\n49271.568421\n16.731891\n46.4\n95\n\n\nMiddle Atlantic\n62316.318182\n42.117002\n46.9\n66\n\n\nMountain\n63649.376344\n23.364852\n47.4\n93\n\n\nNew England\n73872.653846\n14.400133\n48.8\n26\n\n\nPacific\n68737.351351\n50.721772\n47.1\n74\n\n\nSouth Atlantic\n54863.085526\n61.832725\n48.7\n152\n\n\nWest North Central\n60065.508333\n19.362469\n46.4\n120\n\n\nWest South Central\n52675.646154\n37.715468\n47.5\n130\n\n\n\n\n\n\n\nWe see that the output dataset contains a row for each value of the grouping variable (division) as well as the newly created summary variables. The summarized variable names are the same as the non-grouped version after renaming. However, the output dataset now contains nine rows, one for each division. The output here is now a full dataset that we could try to visualize, in order to understand variations in these economic metrics across larger areas of the United States.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#geometries-for-summaries",
    "href": "03_verbs.html#geometries-for-summaries",
    "title": "3  EDA II: Organizing Data",
    "section": "3.7 Geometries for Summaries",
    "text": "3.7 Geometries for Summaries\nWe can use summarized datasets to produce new data visualizations. For example, consider summarizing the household median income and average age for each division of the country. We can take this data and construct a scatter plot that shows the average median income and average median age for each division, along with informative labels. Code to produce the visualization is given below.\n\nsummary_data = (cbsa.groupby('division')\n    .agg({'hh_income_median': 'mean',\n          'pop': 'mean',\n          'age_median': 'mean',\n          'name': 'count'})\n    .rename(columns={'hh_income_median': 'avg_hh_income_median',\n                     'pop': 'avg_pop',\n                     'age_median': 'avg_age_median', \n                     'name': 'n'})\n    .reset_index())\n\n(ggplot(summary_data, aes('avg_hh_income_median', 'avg_age_median')) +\n    geom_point(aes(size='n'), color='grey') +\n    geom_text(aes(label='division'), nudge_y=0.3, size=8) +\n    scale_size_area())\n\n\n\n\nPlot showing the average household median income and average median age based on the geographic division using data from the core-based statistical areas in the as reported by the 2021 American community survey.\n\n\n\n\nScatter plots are often useful for displaying summarized information. The geom_col geometry introduced in the previous chapter is also useful, particularly when combined with counting operations. If we want to create a bar plot that shows the distribution of the total number of rows associated with each category, we can do this by combining the group by and aggregation functions with a plot that uses geom_col. Below is an example that counts the total number of CBSA regions in each division.\n\ndivision_counts = (cbsa.groupby('division')\n    .size()\n    .reset_index(name='n'))\n\n(ggplot(division_counts, aes(x='n', y='division')) +\n    geom_col(color='black', fill='white'))\n\n\n\n\nBar plot showing the number of core-based statistical areas found in each division of the United States.\n\n\n\n\nThe white fill color and black border are often a good-looking starting point. Also, as before, making the bars horizontal will make it easier to read the category names when there are a larger number of categories. As mentioned in the previous chapter, it would be nice to arrange the categories in the plot in the order of their counts. We will see how to make this possible in the final two sections.\n\n\n\n\n\n\nFigure 3.1: Visual descriptions of the six data verbs intrduced in this chapter.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#assign",
    "href": "03_verbs.html#assign",
    "title": "3  EDA II: Organizing Data",
    "section": "3.8 Assign",
    "text": "3.8 Assign\nThe final core pandas operation that we will look at is the .assign() method. It is used to create a new variable in our dataset based on other variables that are already present. Similar to aggregation functions, this method works by specifying the name of the variable we want to create followed by the code that describes how to construct the variable in terms of the rest of the data.\nFor an example of using .assign(), consider converting the population of each region from the millions given in the dataset into a new variable that is in terms of hundreds of thousands. This involves multiplying the population variable by 10. In the code below, we use the assign method to create a new variable named pop_100k that is defined as 10 times the pop variable. We will use column selection to highlight the variables that are involved in the calculation.\n\ncbsa.assign(pop_100k=cbsa['pop'] * 10)[['name', 'pop', 'pop_100k']]\n\n\n\n\n\n\n\n\nname\npop\npop_100k\n\n\n\n\n0\nNew York\n20.011812\n200.11812\n\n\n1\nLos Angeles\n13.202558\n132.02558\n\n\n2\nChicago\n9.607711\n96.07711\n\n\n3\nDallas\n7.543340\n75.43340\n\n\n4\nHouston\n7.048954\n70.48954\n\n\n...\n...\n...\n...\n\n\n928\nPecos\n0.014667\n0.14667\n\n\n929\nZapata\n0.013945\n0.13945\n\n\n931\nCraig\n0.013240\n0.13240\n\n\n932\nVernon\n0.012887\n0.12887\n\n\n933\nLamesa\n0.012371\n0.12371\n\n\n\n\n916 rows × 3 columns\n\n\n\nNotice that there is a new variable named pop_100k. This new variable was added as the last column in the dataset, but we have rearranged the ordering using column selection. We can also modify an existing column in the dataset by using the name of an existing variable, or by direct assignment with the bracket notation. In this case the position of the variable within the DataFrame does not change.\nThe assign method itself has a relatively straightforward syntax. The main challenge is knowing how to apply and chain together the various transformations that are useful within an analysis. Let’s see some common types of operations that will be useful in subsequent applications.\nMany of the uses for creating new variables involve assigning one value when a set of conditions is true and another if the conditions are false. For example, consider creating a new character variable called region_size based on the population of each region. We might classify a region as being “large” if it has more than 7 million people and “small” otherwise. In order to create a variable like this, we need the function np.where(). The np.where() function has three parts: a TRUE/FALSE condition, the value to use when the condition is true, and the value to use when it is false. Below is an example showing the construction of the new variable region_size, where we have selected the relevant variables.\n\n(cbsa.assign(region_size=np.where(cbsa['pop'] &gt; 7, 'large', 'small'))\n    [['name', 'pop', 'region_size']])\n\n\n\n\n\n\n\n\nname\npop\nregion_size\n\n\n\n\n0\nNew York\n20.011812\nlarge\n\n\n1\nLos Angeles\n13.202558\nlarge\n\n\n2\nChicago\n9.607711\nlarge\n\n\n3\nDallas\n7.543340\nlarge\n\n\n4\nHouston\n7.048954\nlarge\n\n\n...\n...\n...\n...\n\n\n928\nPecos\n0.014667\nsmall\n\n\n929\nZapata\n0.013945\nsmall\n\n\n931\nCraig\n0.013240\nsmall\n\n\n932\nVernon\n0.012887\nsmall\n\n\n933\nLamesa\n0.012371\nsmall\n\n\n\n\n916 rows × 3 columns\n\n\n\nLooking at the first several rows of data, we see that there are five regions classified as large, with all of the others classified as small. This classification can be used just like any other character variable. For example, we could group by it and then see summary statistics broken out by small and large regions. Or, we could build a plot where the small and large regions have different colors.\nThe np.where() function can be used to produce any number of categories by using it multiple times. Let’s modify our region size variable to now have five categories: “huge” (over 7 million), “large” (2-7 million), and “medium” (between 1-2 million), “small” (0.3 to 1 million), and the remainder “tiny”. There are several different ways to get to the same result. One of the best and easiest to understand is to start by assigning a default value and then changing the value of the new variable in sequence. For example, below is a code block that creates the new categories and then summarizes the data based on the categories. We have replaced the variable name region_size with rs in order to make sure the code fits easily on the page.\n\nresult = (cbsa.assign(rs='default')\n    .assign(rs=lambda x: np.where(x['pop'] &gt; 7, 'huge', x['rs']))\n    .assign(rs=lambda x: np.where(x['pop'].between(2, 7), 'large', x['rs']))\n    .assign(rs=lambda x: np.where(x['pop'].between(1, 2), 'medium', x['rs']))\n    .assign(rs=lambda x: np.where(x['pop'].between(0.3, 1), 'small', x['rs']))\n    .assign(rs=lambda x: np.where(x['pop'] &lt; 0.3, 'tiny', x['rs']))\n    .groupby('rs')\n    .agg({'pop': ['min', 'max', 'sum'], 'name': 'count'})\n    .round(2))\n\n# Flatten column names for cleaner output\nresult.columns = ['pop_min', 'pop_max', 'pop_sum', 'n']\nresult\n\n\n\n\n\n\n\n\npop_min\npop_max\npop_sum\nn\n\n\nrs\n\n\n\n\n\n\n\n\nhuge\n7.05\n20.01\n57.41\n5\n\n\nlarge\n2.08\n6.33\n100.94\n29\n\n\nmedium\n1.00\n2.00\n28.51\n21\n\n\nsmall\n0.31\n0.97\n60.14\n108\n\n\ntiny\n0.01\n0.30\n62.55\n753\n\n\n\n\n\n\n\nIn each .assign() step we are telling the method that if the condition is false, set rs equal to itself (using lambda functions to reference the current state of the DataFrame). In other words, if the condition does not hold, do not change the value of the variable. Notice that the .between() method we used with filtering becomes very helpful once again.\nWe may wonder why we created a “default” value for the variable rs. It would have been one less line of code to set the default value to “tiny” and remove the final assign function. The reason for the approach above is three-fold. First, it’s easier to understand what the code is doing in its current format because each condition (“huge”, “large”, “medium”, “small”, and “tiny”) is explicitly coded. Secondly, it creates a nice check on our code and data. If we find a row of the output that still has the value “default” we will know that there is a problem somewhere. Finally, the code above will more safely handle the issues with missing values, an issue that we will return to in future chapters.\nAnother reason to create new variables is to handle the formatting of character variables such as quad and division in our cbsa dataset. Pandas has a special data type called a categorical that is specifically designed to handle character variables with a defined set of categories. It is typically not necessary to store data as categorical during most operations, but it can be useful to create a categorical variable just prior to creating a data visualization or model. From the perspective of data analysis, the biggest difference between regular string columns and categorical columns is that a categorical column has a default order of its unique values, called the category order. Creating categorical variables allows us to change the ordering of categories within visualizations and models. As we have seen, these by default are ordered alphabetically for string variables, but will be done in the order specified by the category levels in the case of a categorical variable.\nOne way to produce a categorical variable with a given order is to first sort the data in the desired order, then create the categorical variable using the unique values in that order. Combining this with the .sort_values() method provides a lot of control over how categories become ordered. In the code below, for example, we recreate the previous bar plot, but this time arranges the categories and then forces the categories to be in the order specified in the data.\n\ndivision_counts_ordered = (cbsa.groupby('division')\n    .size()\n    .reset_index(name='n')\n    .sort_values('n'))\n\n# Create categorical with the order from the sorted data\ndivision_counts_ordered['division'] = pd.Categorical(\n    division_counts_ordered['division'], \n    categories=division_counts_ordered['division'].tolist(),\n    ordered=True)\n\n(ggplot(division_counts_ordered, aes(x='n', y='division')) +\n    geom_col(color='black', fill='white'))\n\n\n\n\nPlot showing the number of core-based statistical areas found in each division of the United States. Here, the categories are ordered by their counts.\n\n\n\n\nThe output shows that our plot is now ordered by the number of regions in each division. This can be a much better way of understanding the distribution of a character variable. Notice in the code that we overwrote the variable division rather than creating a new variable. This is a common practice when our modification is changing the data type of a column rather than changing the values themselves, such as in the example above where we converted the division variable from a string into a categorical variable. Other useful functions for manipulating categories in pandas include setting categories manually with pd.Categorical() and various string manipulation methods. We will see these in several of the application chapters when creating plots to better understand our data.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#extensions",
    "href": "03_verbs.html#extensions",
    "title": "3  EDA II: Organizing Data",
    "section": "3.9 Extensions",
    "text": "3.9 Extensions\nThe methods in this chapter provide the primary building blocks for modifying and organizing existing datasets. We will continue to learn new techniques that combine information across datasets (joins) and interchange the relationships between rows and columns (pivots) in Chapter 4. Much of the further power of the techniques shown here come not from new packages, options, and methods, but rather creating complex sequences of the operations we have already seen. Examples of this will be shown throughout the remainder of this book. There are many operations that we have not covered in this chapter that are available in pandas. As mentioned above, we tend to focus on the most commonly used operations in the name of simplicity, though pandas offers many specialized methods for specific use cases. For example, there are various ways to handle missing data, string operations, date/time manipulations, and more. When encountering other code that uses these specialized methods, the pandas documentation is the best source for understanding how they work.\nThe data operations introduced in this chapter are all grounded in the theory of a field called relational algebra [2]. The theory itself is not particularly enlightening for data analysis work. The details are mostly useful for people designing databases and interfaces. However, the consequences of this theoretical underpinning are important to know. Because pandas is based on these ideas, the concepts presented in this chapter have analogs in many other programming languages and most database systems. For example, the R dplyr library has functionality for modifying data tables in R using the same concepts presented that are presented above (often with the same or similar names) [3]. The same can be said for SQL, a standard query language for databases, and its variants [4]. Many of the names—such as filtering, summarizing, and grouping—used in pandas correspond directly to operations in SQL. These links are important because it means that we can translate concepts shown in this chapter to other situations where we might need to use a different programming language.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "03_verbs.html#references",
    "href": "03_verbs.html#references",
    "title": "3  EDA II: Organizing Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] McKinney, W and others (2011 ). Pandas: A foundational python library for data analysis and statistics. Python for high performance and scientific computing. Dallas, TX. 14 1–9\n\n\n[2] Silberschatz, A, Korth, H F and Sudarshan, S (2011 ). Database system concepts. McGraw-Hill Education\n\n\n[3] Wickham, H, Çetinkaya-Rundel, M and Grolemund, G (2023 ). R for Data Science. \" O’Reilly Media, Inc.\"\n\n\n[4] Date, C J (1989 ). A Guide to the SQL Standard. Addison-Wesley Longman Publishing Co., Inc.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>EDA II: Organizing Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html",
    "href": "04_combine.html",
    "title": "4  EDA III: Restructuring Data",
    "section": "",
    "text": "4.1 Introduction\nIn the previous chapter, we learned how to use data operations to modify a dataset within Python. These modifications included taking a subset of a DataFrame, such as filtering to a subset of the rows or selecting a reduced set of columns. We saw how to add new columns and how to rearrange the rows of a dataset by sorting by one or more of the columns. We also investigated ways of grouping and summarizing our data, to create an entirely new set of summary statistics that aggregate the original dataset at different levels of granularity. Along the way, we saw how these modifications can help create more informative data visualizations, particularly when there is too much data to label every row as a point on a single plot. In this chapter, we will continue to see how to modify data using increasingly advanced techniques. To start, we will investigate how to combine information from two different data tables. Then, we proceed to data pivots, a relatively complex but powerful method for modifying data.\nPreviously, we defined the concept of a data operation as a method that takes a dataset and returns a modified copy of the dataset. The operations that we have seen so far work on a single table. In contrast, merge operations take a pair of datasets and produce a new dataset that combines information from both tables. There are several different variants of merge operations; we will look at most of them in this chapter as each will have a different use-case in the later applications.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#joining-by-relation",
    "href": "04_combine.html#joining-by-relation",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.2 Joining by Relation",
    "text": "4.2 Joining by Relation\nTo start, we need another dataset that can be combined with the CBSA data we have been using for our examples. A reminder that the Core-Based Statistical Areas data is census data from the American Communities Survey aggregated to economic centers, rather than political divisions. Recall that each CBSA region can cross over state boundaries. In the code block below, we will read into Python a table that indicates which state(s) every CBSA region overlaps, and the proportion (in terms of population) that each region overlaps into a given state. The table lists a unique identifier for the CBSA, the two-letter state code, and the proportion (a number from 0 to 1) of people living in the CBSA that are residents in the given state.\n\n\n\n\n\n\nFigure 4.1: Visual descriptions table join functions.\n\n\n\n\nlookup = pd.read_csv(\"data/acs_cbsa_to_state.csv\")\nlookup\n\n\n\n\n\n\n\n\ngeoid\nstate\nprop\n\n\n\n\n0\n35620\nNY\n0.648327\n\n\n1\n35620\nNJ\n0.348640\n\n\n2\n35620\nPA\n0.003032\n\n\n3\n31080\nCA\n1.000000\n\n\n4\n16980\nIL\n0.906591\n\n\n...\n...\n...\n...\n\n\n991\n49820\nTX\n1.000000\n\n\n992\n28540\nAK\n1.000000\n\n\n993\n18780\nCO\n1.000000\n\n\n994\n46900\nTX\n1.000000\n\n\n995\n29500\nTX\n1.000000\n\n\n\n\n996 rows × 3 columns\n\n\n\nWe also have another table that gives the state name, abbreviation, and total population of each state in the United States. This data can be read into Python and saved as the dataset state by using the following code.\n\nstate = pd.read_csv(\"data/acs_state.csv\")\nstate\n\n\n\n\n\n\n\n\nstate\nabb\npop\n\n\n\n\n0\nAlabama\nAL\n4.997675\n\n\n1\nAlaska\nAK\n0.735951\n\n\n2\nArizona\nAZ\n7.079203\n\n\n3\nArkansas\nAR\n3.006309\n\n\n4\nCalifornia\nCA\n39.455353\n\n\n5\nColorado\nCO\n5.723176\n\n\n6\nConnecticut\nCT\n3.605330\n\n\n7\nDelaware\nDE\n0.981892\n\n\n8\nFlorida\nFL\n21.339762\n\n\n9\nGeorgia\nGA\n10.625615\n\n\n10\nHawaii\nHI\n1.453498\n\n\n11\nIdaho\nID\n1.811617\n\n\n12\nIllinois\nIL\n12.821813\n\n\n13\nIndiana\nIN\n6.751340\n\n\n14\nIowa\nIA\n3.179090\n\n\n15\nKansas\nKS\n2.932099\n\n\n16\nKentucky\nKY\n4.494141\n\n\n17\nLouisiana\nLA\n4.657305\n\n\n18\nMaine\nME\n1.357046\n\n\n19\nMaryland\nMD\n6.148545\n\n\n20\nMassachusetts\nMA\n6.991852\n\n\n21\nMichigan\nMI\n10.062512\n\n\n22\nMinnesota\nMN\n5.670472\n\n\n23\nMississippi\nMS\n2.967023\n\n\n24\nMissouri\nMO\n6.141534\n\n\n25\nMontana\nMT\n1.077978\n\n\n26\nNebraska\nNE\n1.951480\n\n\n27\nNevada\nNV\n3.059238\n\n\n28\nNew Hampshire\nNH\n1.372175\n\n\n29\nNew Jersey\nNJ\n9.234024\n\n\n30\nNew Mexico\nNM\n2.109366\n\n\n31\nNew York\nNY\n20.114745\n\n\n32\nNorth Carolina\nNC\n10.367022\n\n\n33\nNorth Dakota\nND\n0.773344\n\n\n34\nOhio\nOH\n11.769923\n\n\n35\nOklahoma\nOK\n3.948136\n\n\n36\nOregon\nOR\n4.207177\n\n\n37\nPennsylvania\nPA\n12.970650\n\n\n38\nRhode Island\nRI\n1.091949\n\n\n39\nSouth Carolina\nSC\n5.078903\n\n\n40\nSouth Dakota\nSD\n0.881785\n\n\n41\nTennessee\nTN\n6.859497\n\n\n42\nTexas\nTX\n28.862581\n\n\n43\nUtah\nUT\n3.231370\n\n\n44\nVermont\nVT\n0.641637\n\n\n45\nVirginia\nVA\n8.582479\n\n\n46\nWashington\nWA\n7.617364\n\n\n47\nWest Virginia\nWV\n1.801049\n\n\n48\nWisconsin\nWI\n5.871661\n\n\n49\nWyoming\nWY\n0.576641\n\n\n\n\n\n\n\nLooking at these two datasets, we see that there should be a way to combine the information in these tables with the information in our cbsa DataFrame. In order to understand exactly how to do this, we need some vocabulary about how different columns can define the rows of a table. A primary key consists of one or more columns that uniquely identify a row of data. In our cbsa dataset, the column geoid is a primary key. The column name might appear to be a primary key, but there are some duplicates, with several CBSA regions having the same (short) name. In the lookup data, the primary key consists of the pair geoid and state; without considering both, we would have multiple rows that have the same value for each individual column. The state dataset has two possible primary keys: state and abb. Using either of these would work fine as a way of describing a unique row of the dataset.\nA foreign key is the appearance of a primary key within a different dataset. The foreign key does not need to have the same names on the two tables; it just needs to have matching information. For example, the primary key geoid in our original datasets appears as a foreign key on the lookup dataset. The primary key abb from the state table appears as a foreign key on the lookup table, where it is called state. Notice that we have chosen different examples here—keys that match exactly, keys match with different names in the two tables, and keys that are both foreign keys and part of the primary key— to illustrate some of the possible different kinds of relationships that commonly occur.\nA primary key and the corresponding foreign key in another table form a relation. Typically, a relation maps a single row in one dataset to many rows in another. A table join is a way of combining two tables based on relations. The goal is to line up a foreign key in one table with the primary key in another table. We can then use these relationships to add new variables from one dataset into another dataset. Several examples will make this notion more clear.\nAs an example of performing table joins, we will start with a pared-down version of our cbsa data. A smaller version will make it possible to fit merged tables all within the width of this text.\n\ncbsa_sml = cbsa[['name', 'geoid', 'pop', 'density']]\ncbsa_sml\n\n\n\n\n\n\n\n\nname\ngeoid\npop\ndensity\n\n\n\n\n0\nNew York\n35620\n20.011812\n1051.306467\n\n\n1\nLos Angeles\n31080\n13.202558\n1040.647281\n\n\n2\nChicago\n16980\n9.607711\n508.629406\n\n\n3\nDallas\n19100\n7.543340\n323.181404\n\n\n4\nHouston\n26420\n7.048954\n316.543514\n\n\n...\n...\n...\n...\n...\n\n\n929\nZapata\n49820\n0.013945\n5.081383\n\n\n930\nKetchikan\n28540\n0.013939\n1.046556\n\n\n931\nCraig\n18780\n0.013240\n1.077471\n\n\n932\nVernon\n46900\n0.012887\n5.086411\n\n\n933\nLamesa\n29500\n0.012371\n5.291467\n\n\n\n\n934 rows × 4 columns\n\n\n\nTo start, we will line up the values in the lookup table with the values in the cbsa_sml table. What is the relation here? The cbsa_sml table has the primary key geoid, which appears as a foreign key on the table lookup. If we associate each row of the lookup table with the corresponding row of the cbsa_sml dataset, we will be able to combine the two tables into one. The method to do this with is called pd.merge() with how='left'. The method takes the two datasets as arguments, along with an on parameter that indicates the names of the keys that will be used to match the rows of the dataset together. The code below shows the specific syntax and output for performing the join.\n\nlookup.merge(cbsa_sml, on='geoid', how='left')\n\n\n\n\n\n\n\n\ngeoid\nstate\nprop\nname\npop\ndensity\n\n\n\n\n0\n35620\nNY\n0.648327\nNew York\n20.011812\n1051.306467\n\n\n1\n35620\nNJ\n0.348640\nNew York\n20.011812\n1051.306467\n\n\n2\n35620\nPA\n0.003032\nNew York\n20.011812\n1051.306467\n\n\n3\n31080\nCA\n1.000000\nLos Angeles\n13.202558\n1040.647281\n\n\n4\n16980\nIL\n0.906591\nChicago\n9.607711\n508.629406\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n991\n49820\nTX\n1.000000\nZapata\n0.013945\n5.081383\n\n\n992\n28540\nAK\n1.000000\nKetchikan\n0.013939\n1.046556\n\n\n993\n18780\nCO\n1.000000\nCraig\n0.013240\n1.077471\n\n\n994\n46900\nTX\n1.000000\nVernon\n0.012887\n5.086411\n\n\n995\n29500\nTX\n1.000000\nLamesa\n0.012371\n5.291467\n\n\n\n\n996 rows × 6 columns\n\n\n\nThe new dataset has the same number of rows in the original lookup data along with the original first same four columns. The three columns from cbsa_sml are added to the end, matched up by the variable geoid. We now have every CBSA joined with its corresponding state. We could now proceed to analyze the CBSA data on a state-by-state level.\nNow, let’s consider combining the datasets state and lookup. This would be helpful, for example, if we wanted to know the total population of the state or the full name of the state in conjunction with the CBSA region. The added complexity here is that the primary key now has different names in the two datasets. We can account for this by setting the left_on and right_on parameters to specify the different column names, as shown in the following code chunk.\n\nlookup.merge(state, left_on='state', right_on='abb', how='left')\n\n\n\n\n\n\n\n\ngeoid\nstate_x\nprop\nstate_y\nabb\npop\n\n\n\n\n0\n35620\nNY\n0.648327\nNew York\nNY\n20.114745\n\n\n1\n35620\nNJ\n0.348640\nNew Jersey\nNJ\n9.234024\n\n\n2\n35620\nPA\n0.003032\nPennsylvania\nPA\n12.970650\n\n\n3\n31080\nCA\n1.000000\nCalifornia\nCA\n39.455353\n\n\n4\n16980\nIL\n0.906591\nIllinois\nIL\n12.821813\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n991\n49820\nTX\n1.000000\nTexas\nTX\n28.862581\n\n\n992\n28540\nAK\n1.000000\nAlaska\nAK\n0.735951\n\n\n993\n18780\nCO\n1.000000\nColorado\nCO\n5.723176\n\n\n994\n46900\nTX\n1.000000\nTexas\nTX\n28.862581\n\n\n995\n29500\nTX\n1.000000\nTexas\nTX\n28.862581\n\n\n\n\n996 rows × 6 columns\n\n\n\nAs before, this creates a new dataset with one row for each row in the original lookup dataset. All of the original columns are in the new data, and the added columns are joined on the right. One difficulty here is that the dataset state contains a variable called state that conflicts with the variable of the same name in the lookup dataset. It is not possible in pandas to have two columns with the same name; otherwise, we would not know how to refer to them in our code. To account for this, the suffix “_y” has been added to the name in the second table. Usually, in these cases, the suffix “_x” would be added to the first table, but this is not done when the matching name on the first table is also part of the key. We can override the default suffix by providing a suffixes argument to the merge function, as shown in the following code.\n\nlookup.merge(state, left_on='state', right_on='abb', how='left', \n             suffixes=('', '_name'))\n\n\n\n\n\n\n\n\ngeoid\nstate\nprop\nstate_name\nabb\npop\n\n\n\n\n0\n35620\nNY\n0.648327\nNew York\nNY\n20.114745\n\n\n1\n35620\nNJ\n0.348640\nNew Jersey\nNJ\n9.234024\n\n\n2\n35620\nPA\n0.003032\nPennsylvania\nPA\n12.970650\n\n\n3\n31080\nCA\n1.000000\nCalifornia\nCA\n39.455353\n\n\n4\n16980\nIL\n0.906591\nIllinois\nIL\n12.821813\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n991\n49820\nTX\n1.000000\nTexas\nTX\n28.862581\n\n\n992\n28540\nAK\n1.000000\nAlaska\nAK\n0.735951\n\n\n993\n18780\nCO\n1.000000\nColorado\nCO\n5.723176\n\n\n994\n46900\nTX\n1.000000\nTexas\nTX\n28.862581\n\n\n995\n29500\nTX\n1.000000\nTexas\nTX\n28.862581\n\n\n\n\n996 rows × 6 columns\n\n\n\nNow, the data has a new column called state_name that has the full name of the state. Note that the suffix is only added to columns that have a duplicate name. Non-overlapping names will remain the same in the joined dataset.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#different-types-of-joins",
    "href": "04_combine.html#different-types-of-joins",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.3 Different Types of Joins",
    "text": "4.3 Different Types of Joins\nThe function pd.merge() with how='left' is an example of a left join. There are three additional similar variations of joins available in pandas that have the same syntax as left joins. If the join key is a primary key in one table and there are no missing matches, all of these variations produce exactly the same output. If the join key is a primary key in one table and not all foreign keys in the other table match an entry in the other, the choice of the join type changes what happens with the missing relations. A summary of the four options are:\n\nhow='left' only non-matching rows in the first dataset are kept\nhow='right' only non-matching rows in the second dataset are kept\nhow='outer' non-matching rows are included from either table\nhow='inner' only matching rows from both tables are included\n\nThe terminology of left versus right comes from considering the relative positions of the two joining datasets when written as a function. The left join always keeps things in the left (first) dataset, but ignores non-matching keys on the right. The right joins works in an analogous way, but only keeping the originals in the right dataset. An outer join keeps everything and the inner join only keeps things that have a full match.\nWhenever possible, we suggest making sure that the key used in a join is a primary key for the second dataset; that is, it uniquely defines each row of the second dataset, which frequently contains metadata about the first dataset. This is the rule that we followed above when joining the lookup table to the other two tables. If we follow this rule, we will find that we almost always can get by with just left joins (if we are okay with missing values in the metadata) and inner joins (if we want to remove rows that did not have associated metadata).\nIn addition to the standard joins, there are ways to identify which rows would match or not match between datasets without actually combining them. We can use the indicator=True parameter in pd.merge() to see which rows come from which dataset. For example, we can check which (if any) rows of the lookup table are not in the state table.\n\nresult = lookup.merge(state, left_on='state', right_on='abb', \n                     how='outer', indicator=True)\nprint(\"Column names in result:\", result.columns.tolist())\nmissing_states = result[result['_merge'] == 'left_only'][['state_x', '_merge']]\nmissing_states\n\nColumn names in result: ['geoid', 'state_x', 'prop', 'state_y', 'abb', 'pop', '_merge']\n\n\n\n\n\n\n\n\n\nstate_x\n_merge\n\n\n\n\n122\nDC\nleft_only\n\n\n\n\n\n\n\nIn the result, we see that the District of Columbia is treated as a state code in the lookup table but is not included in the state dataset since it is not technically one of the 50 states. This kind of check is useful for data integrity issues. We can also simulate other filtering operations by using different merge strategies and then filtering the results based on the indicator column. We will employ this trick when working with textual data in Chapter 6.\nThere are no built-in three-table merge operations in pandas. To combine information from more tables, we can combine two tables first and then chain the output into a second merge. A chain of merges can quickly become powerful, if somewhat complex, tools for data analysis. Let’s consider an example with our three tables. Can we determine which states have the smallest proportion of their total population living within one of the CBSA regions? To do this, we will start with the lookup table and merge it with the cbsa table. Then, we can group by the state and add up the amount of the population that each CBSA contributes to each state. Finally, we merge (carefully) to the state dataset to get the total state populations and arrange the output. These steps are shown in the following block of code.\n\nresult = (lookup\n    .merge(cbsa, on='geoid', how='inner')\n    .assign(weighted_pop=lambda x: x['pop'] * x['prop'])\n    .groupby('state')\n    .agg({'weighted_pop': 'sum'})\n    .reset_index()\n    .rename(columns={'weighted_pop': 'pop'})\n    .merge(state, left_on='state', right_on='abb', how='inner', suffixes=('', '_full'))\n    .assign(percentage=lambda x: x['pop'] / x['pop_full'] * 100)\n    .sort_values('percentage', ascending=False)\n    [['state_full', 'percentage']])\n\nresult\n\n\n\n\n\n\n\n\nstate_full\npercentage\n\n\n\n\n30\nNew Jersey\n101.202785\n\n\n7\nDelaware\n100.638565\n\n\n10\nHawaii\n99.996698\n\n\n6\nConnecticut\n99.989340\n\n\n38\nRhode Island\n99.930335\n\n\n18\nMassachusetts\n99.716925\n\n\n4\nCalifornia\n99.295495\n\n\n32\nNevada\n98.962879\n\n\n3\nArizona\n98.688270\n\n\n19\nMaryland\n98.592417\n\n\n8\nFlorida\n97.901509\n\n\n46\nWashington\n97.787522\n\n\n33\nNew York\n97.496855\n\n\n36\nOregon\n97.266643\n\n\n37\nPennsylvania\n97.133737\n\n\n29\nNew Hampshire\n96.830430\n\n\n34\nOhio\n96.205756\n\n\n31\nNew Mexico\n95.824812\n\n\n43\nUtah\n95.556598\n\n\n42\nTexas\n95.275283\n\n\n13\nIllinois\n95.102558\n\n\n5\nColorado\n94.667891\n\n\n26\nNorth Carolina\n94.519273\n\n\n39\nSouth Carolina\n94.494560\n\n\n14\nIndiana\n94.187449\n\n\n17\nLouisiana\n93.816687\n\n\n9\nGeorgia\n92.770803\n\n\n12\nIdaho\n92.273397\n\n\n21\nMichigan\n92.028750\n\n\n1\nAlabama\n91.772661\n\n\n41\nTennessee\n91.269077\n\n\n44\nVirginia\n90.853613\n\n\n22\nMinnesota\n90.798742\n\n\n47\nWisconsin\n87.457256\n\n\n15\nKansas\n86.796665\n\n\n23\nMissouri\n86.776798\n\n\n35\nOklahoma\n86.064981\n\n\n2\nArkansas\n83.683737\n\n\n28\nNebraska\n82.462477\n\n\n48\nWest Virginia\n81.342980\n\n\n24\nMississippi\n81.171164\n\n\n16\nKentucky\n79.732292\n\n\n11\nIowa\n76.036886\n\n\n40\nSouth Dakota\n75.674932\n\n\n49\nWyoming\n75.184547\n\n\n27\nNorth Dakota\n74.709951\n\n\n0\nAlaska\n73.738605\n\n\n45\nVermont\n73.185042\n\n\n20\nMaine\n68.801942\n\n\n25\nMontana\n66.985875\n\n\n\n\n\n\n\nSince the CBSA regions correspond to large metropolitan areas, it is not surprising that states considered relatively rural, such as Montana, Maine, Vermont, and Alaska, have the smallest percentage of the population living inside one of these regions. Sorting the table the other way, with the highest percentages at the top, shows that small, urban states such as New Jersey, Delaware, and Hawaii have some of the largest percentages of the population living in a CBSA.\nThe pandas library provides comprehensive support for different types of joins and merge operations. While we have covered the most common use cases, there are additional parameters and methods available for more complex scenarios. We will see these when working with time series data in Chapter 7.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#pivot-longer",
    "href": "04_combine.html#pivot-longer",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.4 Pivot Longer",
    "text": "4.4 Pivot Longer\nIn this section we introduce another set of methods for manipulating datasets. Table pivots, which are related but not identical to the spreadsheet concept of a pivot table, are a way of rearranging the values in a table without adding or losing any additional information. This is achieved by either making the table longer (more rows, fewer columns) or wider (more columns, fewer rows).\nWhat sort of situations would require going between two different formats with a different number of rows? As an example, consider a hypothetical project where we measure the number of people living in 100 different cities, one per year for 20 years. There are two equally valid but different ways to store this data. We could have 100 rows, one for each city, with variables pop_year1, pop_year2, and so on all the way through pop_year20. Alternatively, we could have 2000 rows with just three columns: an id for the city, a variable for the year, and a variable for population. Notice that both of these options capture the same information, but each privileges a particular kind of analysis. It will often depend on our data and object of study whether a wider of longer table format is amenable to exploratory data analysis. In general, we have found that it is easier to start with a longer table format and then make it wider as needed.\n\n\n\n\n\n\nFigure 4.2: Visual description of table pivots.\n\n\n\nIn the wider table format, it is straightforward to compute the amount that each city grew over the twenty years using a single assign operation. In the longer table format, it would be straightforward to filter by a specific city and draw a line plot showing the growth of the city over a twenty year period. Both drawing a plot with the wider table or computing the growth with the longer table are possible, but require a surprising amount of work and code.\nIn this and the following section, we will introduce two new methods for alternating between wider and longer formats for a dataset. These are principles that will be fundamental to several of the application chapters, particularly with text and temporal datasets in Chapters 6-7. We will use the food_prices dataset as example. As will be shown in our motivation example, pivoting is a particularly useful operation to apply when analyzing data collected over time. As a reminder, the food_prices dataset is organized with year as the observation and each food type as a column, as shown below.\n\nfood_prices\n\n\n\n\n\n\n\n\nyear\ntea\nsugar\npeanuts\ncoffee\ncocoa\nwheat\nrye\nrice\ncorn\nbarley\npork\nbeef\nlamb\n\n\n\n\n0\n1870\n128.845647\n151.406650\n203.422160\n88.067280\n78.817566\n88.102178\n103.393214\n83.478261\n121.373971\n103.241107\n107.813162\n73.540373\n63.257185\n\n\n1\n1871\n131.730769\n167.156863\n221.774193\n108.728711\n66.686484\n118.157238\n105.251996\n84.543919\n88.393285\n130.236487\n68.466144\n86.755952\n73.411017\n\n\n2\n1872\n134.455128\n161.764706\n188.508064\n140.092762\n71.591359\n121.523799\n102.095808\n92.905405\n69.229178\n124.662162\n58.644715\n85.119048\n78.072034\n\n\n3\n1873\n136.337312\n154.104124\n178.826474\n172.869998\n65.752662\n116.056854\n106.028976\n91.034483\n67.136989\n166.034483\n62.871238\n93.563218\n84.444769\n\n\n4\n1874\n146.088815\n153.246661\n230.709444\n187.244900\n69.924808\n113.202047\n125.977031\n99.637681\n127.551727\n174.275362\n87.613692\n89.531573\n77.480963\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n141\n2011\n33.092175\n34.758403\n49.430540\n104.633987\n30.950856\n32.303050\n56.050786\n26.970989\n56.191498\n76.784074\n18.957462\n195.682011\n199.901823\n\n\n142\n2012\n32.169401\n28.216065\n50.480016\n70.519447\n24.339000\n32.329419\n54.278655\n27.791165\n60.997731\n87.240944\n17.266451\n196.471582\n179.915106\n\n\n143\n2013\n31.300000\n22.830000\n31.508720\n51.984920\n24.449543\n30.635729\n55.776368\n24.481270\n55.410171\n72.322420\n17.782412\n190.347418\n164.486798\n\n\n144\n2014\n29.530000\n21.790000\n29.420000\n74.140000\n30.470000\n26.370000\n53.558470\n19.970000\n36.750000\n50.220000\n20.960000\n229.492089\n184.540000\n\n\n145\n2015\n29.174600\n17.087190\n28.121785\n58.715669\n30.967121\n22.009427\n51.521938\n17.670020\n32.904916\n68.492986\n13.747636\n203.667366\n149.682263\n\n\n\n\n146 rows × 14 columns\n\n\n\nThis format makes it straightforward to compute the correlation between the prices of different kinds of food items. A longer format for the dataset would, instead, have one row for each combination of year and food type.\nIn order to make this table longer, we will apply the pd.melt() function. This function requires knowing which current variables in the dataset should remain as identifier variables and which should be turned into values in the output dataset. In the code below, we indicate that the year value should remain as an identifier variable in the output dataset.\n\nfood_prices.melt(id_vars=['year'])\n\n\n\n\n\n\n\n\nyear\nvariable\nvalue\n\n\n\n\n0\n1870\ntea\n128.845647\n\n\n1\n1871\ntea\n131.730769\n\n\n2\n1872\ntea\n134.455128\n\n\n3\n1873\ntea\n136.337312\n\n\n4\n1874\ntea\n146.088815\n\n\n...\n...\n...\n...\n\n\n1893\n2011\nlamb\n199.901823\n\n\n1894\n2012\nlamb\n179.915106\n\n\n1895\n2013\nlamb\n164.486798\n\n\n1896\n2014\nlamb\n184.540000\n\n\n1897\n2015\nlamb\n149.682263\n\n\n\n\n1898 rows × 3 columns\n\n\n\nAlready this looks close to what a long form of the food prices dataset should look like. One improvement that we can make is to set better column names, which can be done by setting the var_name and value_name parameters in the function call. An example is given in the following code.\n\nfood_prices.melt(id_vars=['year'], var_name='food', value_name='price')\n\n\n\n\n\n\n\n\nyear\nfood\nprice\n\n\n\n\n0\n1870\ntea\n128.845647\n\n\n1\n1871\ntea\n131.730769\n\n\n2\n1872\ntea\n134.455128\n\n\n3\n1873\ntea\n136.337312\n\n\n4\n1874\ntea\n146.088815\n\n\n...\n...\n...\n...\n\n\n1893\n2011\nlamb\n199.901823\n\n\n1894\n2012\nlamb\n179.915106\n\n\n1895\n2013\nlamb\n164.486798\n\n\n1896\n2014\nlamb\n184.540000\n\n\n1897\n2015\nlamb\n149.682263\n\n\n\n\n1898 rows × 3 columns\n\n\n\nThe longer form of the dataset makes it much easier to do some kinds of analysis. For example, we can draw a line chart of all of the food prices with a single graphics layer.\n\nfood_long = food_prices.melt(id_vars=['year'], var_name='food', value_name='price')\n\n(ggplot(food_long, aes('year', 'price', color='food')) +\n    geom_line() +\n    scale_color_cmap_d())\n\n\n\n\nPlot showing the relative cost of thirteen different food items over time, with the value 100 corresponding to the price in 1900.\n\n\n\n\nDrawing this plot with the original dataset would require manually including a layer for each food type, selecting their colors, and building a manual legend. The alternative using the longer table is the preferred approach.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#pivot-wider",
    "href": "04_combine.html#pivot-wider",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.5 Pivot Wider",
    "text": "4.5 Pivot Wider\nJust as we sometimes have a dataset in a wide format that we need to convert to a long one, it is sometimes the case that we have a long dataset that needs to be made wider. To illustrate making a table wider, let’s create a new dataset consisting of the long format of the food prices dataset from just the years 1950 and 1975.\n\nfood_prices_long = (food_prices\n    .melt(id_vars=['year'], var_name='food', value_name='price')\n    .query('year in [1950, 1975]'))\n\nAs described in our motivating example, it makes sense for some analyses to make each time point a column in a wider dataset. To do this, we use the pd.pivot() or pd.pivot_table() function. We need to indicate which variable contains the values that will become new columns (columns parameter), which variable to use as the index (index parameter), and the variable from which to take the values for the new columns (values parameter). Here, the column names will come from the year column (we want a new column for 1950 and another one for 1975) and the values will be filled in with prices.\n\nfood_prices_long.pivot(index='food', columns='year', values='price')\n\n\n\n\n\n\n\nyear\n1950\n1975\n\n\nfood\n\n\n\n\n\n\nbarley\n98.837209\n91.315409\n\n\nbeef\n62.416235\n268.415155\n\n\ncocoa\n64.589223\n54.066178\n\n\ncoffee\n209.554560\n105.491333\n\n\ncorn\n134.582748\n112.703291\n\n\nlamb\n33.789828\n186.110848\n\n\npeanuts\n118.859179\n95.590279\n\n\npork\n52.338904\n88.811119\n\n\nrice\n62.698668\n74.174218\n\n\nrye\n88.389666\n71.218633\n\n\nsugar\n59.484424\n114.289677\n\n\ntea\n68.941159\n54.068946\n\n\nwheat\n106.400588\n74.799223\n\n\n\n\n\n\n\nOne issue with the default output is that the column names now start with a number, which can be awkward to work with. It is better to add a prefix to the names to make them more descriptive. This can be done by renaming the columns after pivoting.\n\npivoted = food_prices_long.pivot(index='food', columns='year', values='price')\npivoted.columns = [f'year_{col}' for col in pivoted.columns]\npivoted = pivoted.reset_index()\npivoted\n\n\n\n\n\n\n\n\nfood\nyear_1950\nyear_1975\n\n\n\n\n0\nbarley\n98.837209\n91.315409\n\n\n1\nbeef\n62.416235\n268.415155\n\n\n2\ncocoa\n64.589223\n54.066178\n\n\n3\ncoffee\n209.554560\n105.491333\n\n\n4\ncorn\n134.582748\n112.703291\n\n\n5\nlamb\n33.789828\n186.110848\n\n\n6\npeanuts\n118.859179\n95.590279\n\n\n7\npork\n52.338904\n88.811119\n\n\n8\nrice\n62.698668\n74.174218\n\n\n9\nrye\n88.389666\n71.218633\n\n\n10\nsugar\n59.484424\n114.289677\n\n\n11\ntea\n68.941159\n54.068946\n\n\n12\nwheat\n106.400588\n74.799223\n\n\n\n\n\n\n\nThis new form of the dataset makes it straightforward to plot the price of each food type in 1975 as a function of its price in 1950, by putting the 1950 price on the x-axis and the 1975 price on the y-axis. The code to do this is shown below.\n\npivoted = food_prices_long.pivot(index='food', columns='year', values='price')\npivoted.columns = [f'year_{col}' for col in pivoted.columns]\npivoted = pivoted.reset_index()\n\n(ggplot(pivoted, aes('year_1950', 'year_1975')) +\n    geom_point() +\n    geom_text(aes(label='food'), nudge_y=5, size=8))\n\n\n\n\nPlot showing the relative cost of thirteen different food items in 1950 and 1975. Prices are on a scale where all products cost 100 in 1900.\n\n\n\n\nWe can now begin to delve into which products got much more expensive, much less expensive, and stayed about the same between 1950 and 1975.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#patterns-for-table-pivots",
    "href": "04_combine.html#patterns-for-table-pivots",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.6 Patterns for Table Pivots",
    "text": "4.6 Patterns for Table Pivots\nThe syntax for making tables wider or longer is, on the surface, not much more complex than other table operations that we have covered in this text. The biggest challenges with table pivots are identifying when they will simplify an analysis and not over-using them. The best way to avoid these issues is to store data in the longest format that makes sense for the data. For example, in the motivating example about city growth, it is better if possible to store the data with 2000 rows and 3 columns.\nStoring data in a longer format has a number of benefits. Reducing the number of columns makes it easier to document the (smaller set of) variables with a well-written data dictionary, a topic we will see next in Chapter 5. Also, pivoting wider also often requires less code and results in fewer bugs. Several of these are illustrated in the chapter’s exercises.\nPerhaps the biggest benefit of storing data in a longer format is to avoid the potentially complex chain of operations required to make the plot at the end of the previous section. The original dataset is stored with years as rows and items as columns. Producing the plot requires thinking of years and columns and items as rows; this needed us to first pivot longer and then pivot wider. Keeping data in a longer format avoids the need for double pivots, while also making the different kinds of analysis (item and year, year by item, item by year) all reasonably accessible.\nLet’s put together all of the elements of pivots together to produce a plot of wheat prices that highlights the differences in prices following World War I and following World War II. We will add a complete set of titles and captions. We selected the second color by picking the complementary color of the maroon used for the first time period.\n\n# Create filtered datasets for different time periods  \nww1_period = food_prices[(food_prices['year'] &gt;= 1919) & (food_prices['year'] &lt;= 1939)]\nww2_period = food_prices[(food_prices['year'] &gt;= 1945) & (food_prices['year'] &lt;= 2015)]\n\n(ggplot(food_prices, aes('year', 'wheat')) +\n    geom_line(color='grey85') +\n    geom_line(color='maroon', data=ww1_period) +\n    geom_line(color='#30b080', data=ww2_period) +\n    labs(\n        title='Wheat Price Index, 1850 to 2015',\n        subtitle='Commodity prices are given as a price index relative to real prices in 1900',\n        caption='Jacks, D.S. (2019), \"A Typology of Real Commodity Prices in the Long Run.\" Cliometrica 13(2), 202-220.',\n        x='Year',\n        y='Price Index of Wheat (1900 = 100)'\n    ))\n\nTo finish the plot off, and make it look particularly professional, it would be nice to add annotations explaining the main points that we want our audience to take-away from the plot. It is possible to do this directly in Python with plotnine annotations, but these can be somewhat awkward and time consuming to work with. Often, a better solution is to open the figure in another program such as Google Slides, Microsoft PowerPoint, or image editing software.\n\n\n\n\n\n\nFigure 4.3: Plot showing the relative cost of wheat between 1850 and 2015, with manual annotations.\n\n\n\nNotice that the plot both shows all of the data for the viewer, but guides them to the specific points of interest. Annotations explaining peaks and trends can be shown in smaller font sizes and in dark grey, because they are secondary to the main points we want to make about the overall trends in the two post-war periods.\nOverall, the process of combining, joining, and pivoting is a powerful way to connect and analyze information across datasets. The approaches also speak to why we must be very careful when creating datasets, from variable names to the relationship between columns and rows to the shape of our dataset. How datasets are collected, built, and organized will shape which data we can put in conversation and explore.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#extensions",
    "href": "04_combine.html#extensions",
    "title": "4  EDA III: Restructuring Data",
    "section": "4.7 Extensions",
    "text": "4.7 Extensions\nWe have seen in this chapter the core methods for combining two datasets using merges and for rearranging the rows and columns of a dataset with pivots. In Chapter 8 we will see how to use range-based merges that combine datasets based on inequalities between numeric keys in place of the exact matches as shown above. Each of the merge functions and pivot functions that we have shown have a number of additional options that can extend and refine the results. The Python for Data Analysis book by Wes McKinney is a good reference for these additional details [1]. The package documentation for pandas provides even more in-depth descriptions of each of the options.\nTo better understand relational joins, it is useful to understand the concepts of database normalization. In database theory, there are a variety of different terms to describe how the relations in different tables are connected via their keys. For a good and concise introduction to the five most common normal forms, we recommend the “simple guide” from William Kent [2]. Textbooks in relational database design can provide further examples and motivations [3]. In the next chapter, we will discuss best practices to avoid requiring overly complex chains of pivots through good dataset design.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "04_combine.html#references",
    "href": "04_combine.html#references",
    "title": "4  EDA III: Restructuring Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] McKinney, W (2012 ). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O’Reilly Media, Inc.\"\n\n\n[2] Kent, W (1983 ). A simple guide to five normal forms in relational database theory. Communications of the ACM. ACM New York, NY, USA. 26 120–5\n\n\n[3] Date, C J (2019 ). Database Design and Relational Theory: Normal Forms and All That Jazz. Apress",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA III: Restructuring Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html",
    "href": "05_collect.html",
    "title": "5  Collecting Data",
    "section": "",
    "text": "5.1 Introduction\nIt is a common saying within data science that the majority of our time is spent collecting and cleaning our data. If we can collect data in a tidy format from the start, it will allow us to proceed directly to the exploration stage once the data have been collected.\nThere are a number of excellent articles that give an extensive overview of how to collect and organize data. Hadley Wickham’s “Tidy Data”, one of the most cited papers across all of data science, offers an extensive theoretical framework for describing a process for collecting datasets [1]. Karl Broman and Kara Woo’s “Data Organization in Spreadsheets” offers a balance between practical advice and an extended discussion of general principles for collecting datasets [2]. Catherine D’Iganzio and Lauren Klein’s Data Feminism lays out important considerations about bias, inequality and power when collecting and organizing data [3].\nThis short chapter provides a summarized set of advice for organizing and storing data within a spreadsheet program. Rather than an extensive discussion of various pros and cons, it primarily focuses on the explicit approaches that we recommend. For readers interested in a broader coverage, we suggest reading the sources cited above. Because we are not using any fancy spreadsheet functions here, any program that we would like to use should be fine. The screenshots come from Microsoft Excel, but the same approach will work in Google Sheets, LibreOffice, or another spreadsheet program.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#rectangular-data",
    "href": "05_collect.html#rectangular-data",
    "title": "5  Collecting Data",
    "section": "5.2 Rectangular Data",
    "text": "5.2 Rectangular Data\nIn Chapter 1 the concept of a rectangular dataset, with observations in rows and variables in columns, was introduced. This is the same format that we will use to collect our data. The first thing we will need to do, then, is determine what things we are observing and what properties we would like to collect about each thing. If we are observing different kinds of things, each of which has a different set of associated properties, we may need to store each set in a different table.\nTo match the format of rectangular data that we have been working with in Python, we need to structure our dataset with a single row of column names, followed by a row of data for each observation. For example, Fig. 5.1 shows a screenshot from Excel of a nonsense dataset with three variables.\n\n\n\n\n\n\nFigure 5.1: Screenshot from Excel of a nonsense dataset with three variables.\n\n\n\nNotice that we need to always start in the first cell, A1, and fill in a consistent number of rows and columns. We do not have multiple tables scattered around the spreadsheet. We do not have multiple header columns. It is just the data itself, stored in an contained rectangle in the upper-left hand corner of our spreadsheet. It is okay to include a small amount of formatting of the cells to help with the data-entry process (we like to the make the first row bold), but do not try to record measurable properties of our data with formatting such as fonts and colors.\nZooming out, a key idea is that each table is an observational unit. For example, we may be interested in food prices or Hollywood films at the box office. Each row has a specific observation associated with it, such as a kind of food or a film. The columns are variables that give us relevant information about the observation in each row. For example, a variable might be the cost at the grocery store for food item or, when talking about movies, variables might include the director, time duration, and film studio. When our data is organized in this way, we can explore our data harnessing the full analytical power of Python.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#naming-variables",
    "href": "05_collect.html#naming-variables",
    "title": "5  Collecting Data",
    "section": "5.3 Naming Variables",
    "text": "5.3 Naming Variables\nIt is important to choose good variable names. As we have seen, the variable names in a dataset are used to describe the graphics in the grammar of graphics and for manipulating data with pandas operations. If our names are too complex or difficult to remember, it will be more difficult to create data visualizations. When variables contain spaces or other special characters, it can become nearly impossible to work within Python without first cleaning up the variable names after loading the data.\nWe find the best approach to variable names is to only use lower-case letters, numbers, and underscores. The underscores can be used in place of spaces, but we avoid making variable names more complex than needed. Also, we make sure to start the name of a variable with a lower-case letter (starting with a number is invalid in Python and many other programming languages). Note that we recommend using lowercase even for variable names that should be capitalized, such as acronyms and proper nouns. If we selectively capitalize, we will always need to remember where this was done. In our scheme, it is one less thing to remember.\nThroughout the rest of this chapter, we will show examples of a small dataset collecting information about a set of cookbooks. Fig. 5.2 shows the table before filling in any information, with just the column names.\n\n\n\n\n\n\nFigure 5.2: Example of a data table before filling in any information other than the column names.\n\n\n\nFor the specific values within a dataset, spaces, capital letters, and other special characters are fine. Just be consistent. Do not use “female” in one observations, “Female” in another, and “F” in a third. Just pick a format and stick to it. Where applicable, try to use standards-based labels, such as ISO, and schemas, such as Dublin Core, which will also help with connecting and restructuring data. This will help if we later want to merge our dataset with other sources.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#what-goes-in-a-cell",
    "href": "05_collect.html#what-goes-in-a-cell",
    "title": "5  Collecting Data",
    "section": "5.4 What Goes in a Cell",
    "text": "5.4 What Goes in a Cell\nWithin each cell, there should only be one piece of information. In particular, this means that cells should not contain units or current symbols. If we have data collected in different units, create a new column and put the units there. Though, when possible, it is best to store everything on the same scale. If there is something to note about a particular value, do not put a star or other mark with it: create a new column. This also means that, as mentioned above, we should not try to store two things in one cell by using formatting to indicate a secondary piece of information. Again, if we have two things to indicate, create a new column. An example of our cookbook dataset with these principles applied is shown in Fig. 5.3.\n\n\n\n\n\n\nFigure 5.3: Example of a small cookbook dataset stored in a tidy format in Excel.\n\n\n\nIf we need to include explanatory notes for some of the data, which is often a great idea, we avoid abandoning the rectangular data format. Instead, we include an extra column of notes. For example, we explain that one of our books is out of print in the notes column shown in Fig. 5.4. In our example table, the number of pages and weight of one book is missing because it is out of print. In order to indicate this, the corresponding cell is blank. Blank values are the only cross-software way to indicate missing values in a consistent way.\n\n\n\n\n\n\nFigure 5.4: Example of a small cookbook dataset stored in a tidy format in Excel. Here we add explanatory notes in a new column.\n\n\n\nIn order to store data in a rectangular format with one thing in each cell, it is important to make a new table for each observational unit. So, where the table in Fig. 5.4 focuses on information about each cookbook, we make a new table shown in Fig. 5.5 to record information about the authors. It is possible that one author wrote multiple cookbooks and it is also possible that a cookbook was written by multiple people. Keeping each sheet corresponding to a single observational unit with each observation helps avoid duplicating information and creating data inconsistencies.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#dates",
    "href": "05_collect.html#dates",
    "title": "5  Collecting Data",
    "section": "5.5 Dates",
    "text": "5.5 Dates\nDates are important to a significant amount of humanities data. Date variables are also a well-known source of error in the collecting and recording of data. Our recommendation for the most flexible and least error-prone method is to simply record each component of a date as its own column. This means one column for year, one for month, and one for day. It will be much easier to work with if we keep months as numbers rather than names. If we have time information as well, this can be recorded by putting the hours, minutes, and seconds as their own columns. One benefit of this method is that it will be easy to record historical data in cases where we may not be sure of the month or day for every row of the dataset. For example, Fig. 5.5 shows a dataset showing properties of the cookbook authors from our dataset.\n\n\n\n\n\n\nFigure 5.5: A small dataset showing properties of cookbook authors using a tidy format in Excel.\n\n\n\nThere is a standard format recommended by the International Organization for Standardization (ISO 8601) for representing dates and times in a consistent way: YYYY-MM-DD. This is often a great format for storing data, and one that is used in several example datasets for this book, but (1) can lead to errors when opening and re-saving in a spreadsheet program and (2) cannot easily store dates with unknown information. We suggest using the separate column approach while collecting our initial dataset. Later, if we re-save a modified dataset from within Python, the ISO 8601 format is a good option.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#output-format",
    "href": "05_collect.html#output-format",
    "title": "5  Collecting Data",
    "section": "5.6 Output Format",
    "text": "5.6 Output Format\nMost sources on collecting data suggest storing our results in a plain text format. This is a stripped down representation of the data that contains no formatting information and is application agnostic. Excel, GoogleSheets, LibreOffice, and any other spreadsheet program, should be able to save a dataset in a plain text format. The most commonly used format for tabular data in data science is called a comma separated value (CSV) file. Here, columns are split by commas and each row is on its own line. The example below shows what a the CSV file of our cookbook authors dataset looks like.\nvariable,long_name,units,description\nbook_title,Book Title,,\"Book name, given in English\"\nauthor,Author's name,,Authors given and family name\npages,Number of pages,,Pages in currently available edition\nweight,Book weight,grams,Weight of currently available edition\nnationality,Author's Nationality,,\"Using ISO codes\"\nbirth_year,Author's birth year,,As numeric variable\nbirth_month,Author's birth month,,As numeric variable\nbirth_day,Author's birth day,,As numeric variable\nNearly all of the dataset provided with this book are stored as CSV files, which can be loaded using pd.read_csv() from the pandas package. One thing to be careful of, particularly when using Excel, is that if one’s computer is configured for a language that uses a comma as a decimal separator, the default CSV output may actually use a semicolon (;) in place of a comma. To read these files in Python, just use pd.read_csv() with the sep=';' parameter. For tabulated datasets with different separators, the flexible pd.read_csv() can be used with an appropriate selection for the sep parameter. If we want to save a dataset in a tabular format, perhaps after doing some cleaning or joining with other sources within Python using .assign() and .merge() methods, we can also use the pandas .to_csv() method. It takes the filepath as an argument, saving the results as a local file for future analyses or to post and share with others.\nUnlike some other sources, we are less strict about the need to only export data as a plain text file. Plain text is the best way for sharing and storing a dataset once an analysis is finished, but if we are going to continue adding and changing the dataset, it may actually be preferable to store the data in an .xlsx file (it avoids errors that are introduced when converting back and forth between excel and plain text formats). Data can be loaded directly from an excel file with pandas using pd.read_excel(). Below is the syntax for using pandas to read in a dataset, with either the first sheet or a named sheet.\n\ndata = pd.read_excel(\"authors.xlsx\")                      \ndata = pd.read_excel(\"authors.xlsx\", sheet_name=\"sheetname\")\n\nOnce finished with the data collection and cleaning processing, then it is a good idea to store the data in a plain text format for sharing and long-term preservation.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#data-dictionary",
    "href": "05_collect.html#data-dictionary",
    "title": "5  Collecting Data",
    "section": "5.7 Data Dictionary",
    "text": "5.7 Data Dictionary\nSo far, our discussion has focused on the specifics of storing the data itself. It is also important to document exactly what information is being stored in the variables. To do this, we can construct a data dictionary. This should explain, for each variable, basic information such as the variable’s name, measurement units, and expected character values. Any decisions that needed to be made should also be documented. A data dictionary can be a simple text file, or can be stored itself as a structured dataset. Fig. 5.6 shows an example of a data dictionary for our authors dataset. We included a long name for each variable, which will be useful when creating improved graphics labels when preparing data visualizations for publication.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#summary",
    "href": "05_collect.html#summary",
    "title": "5  Collecting Data",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nThere is a lot of information passed along within this chapter. For future reference, here are the key formatting guidelines for storing datasets in spreadsheets:\n\nrecord a single row of column names, starting in cell A1, followed by rows of observations\nonly use lowercase letters, numbers, and underscores in column names; always start the name with a letter, and keep the variable names relatively short\nwhen recording character variables, keep the codes consistent\nonly one thing in each cell; no units, currency symbols, or notes\nkeep a separate notes column if needed\nblank cells within the rectangle are used if and only if the data is missing\nsave dates by storing year, month, day, hour, etc. as their own columns\nsave results as an xlsx file while in the middle of data collection; save as a CSV file for long-term storage and sharing\ncreate a data dictionary to record important information about the dataset\n\nAs mentioned in the introduction, this is an opinionated list and some other options are equally valid. The most important thing, however, is consistency. If we have a valid reason to avoid some of the advice here, that’s fine. For example, when making a dataset, we sometimes like to use “none” when data is unknown. We want to be able to distinguish between data that we don’t have versus data that we haven’t looked for yet. The approach has also been helpful when working collaboratively on a new data set. Just make sure to document how the data are organized and any interpretive decisions that were made in the data collection process.\nOne final note that not all data that we work with will be tabular data, such as a corpus of text in books. We will see examples in Chapter 12. However, the metadata about the books such as author and date published will be necessary to conduct certain kinds of analysis. Tabular data is everywhere, and key to exploratory data analysis.\n\n\n\n\n\n\nFigure 5.6: Example of a data dictionary from the cookbook authors dataset.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#extensions",
    "href": "05_collect.html#extensions",
    "title": "5  Collecting Data",
    "section": "5.9 Extensions",
    "text": "5.9 Extensions\nWe have provided a consolidation of the information from the research papers mentioned in the introduction, with a focus on the issues that are of particular concern for humanities data collection. The first place to go for more information would be those same papers: Hadley Wickham’s “Tidy Data” [1]; Karl Broman and Kara Woo’s “Data Organization in Spreadsheets” [2]; and Catherine D’Iganzio and Lauren Klein’s Data Feminism [3]. Another source for thinking about data dictionaries, data documentation, and data publishing is the “Datasheets for Datasets” paper [4]. While the paper focuses on predictive modeling, it provides several good examples of the kinds of documentation that are needed when publishing large datasets and has a good critical lens on how choices during the data collection phase have fundamental impacts on the ultimate data analyses that can and will be performed.\nThe kinds of data that need to be collection and how to collect them ultimately depend on one’s underlying research questions. The process of going from a research question to a quantitative framework is outside the scope of our text, but is a very important aspect to carefully consider when working with data to address research questions in the humanities. There are many good guides to research design, though the majority focus on either scientific, hypothesis-driven quantitative designs or purely qualitative data collection. We can recommend a few sources that sit at the intersection of these that may be of interest. Content analysis is a common social technique used across a variety of fields. It often mixes qualitative questions with quantitative data, and therefore is a good place to find research design advice for humanities data analysis [5] [6] [7]. Similarly, corpus linguistics sits at the boundary of the humanities and social sciences and offers many resources for best practices [8]. Corpus linguistics often works with both textual and sound data, lending it to develop specific techniques for working with the kinds of rich, multimodal datasets that will be consider in the following chapters. Finally, sociology is another field that mixes humanistic questions with quantitative data, providing an additional source of references for good research design [9]. While each of these references have domain-specific elements, many of the general principles can be extended to other domains.\nFinally, notice that we have not started the book with data collection even though collecting data is most often the first step in data analysis. We saved this until the last core chapter because understanding how to collect and store data often goes hand-in-hand with understanding how to visualize, organize, and restructure it. A deeper understanding of how to collect data, particularly data that has complex components such as text, networks, and images, will arise as we work through the remaining chapters.",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "05_collect.html#references",
    "href": "05_collect.html#references",
    "title": "5  Collecting Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Hadley, W (2014 ). Tidy data. Journal of Statistical Software. 59 1–23\n\n\n[2] Broman, K W and Woo, K H (2018 ). Data organization in spreadsheets. The American Statistician. Taylor & Francis. 72 2–10\n\n\n[3] D’ignazio, C and Klein, L F (2020 ). Data Feminism. MIT press\n\n\n[4] Gebru, T, Morgenstern, J, Vecchione, B, Vaughan, J W, Wallach, H, Iii, H D and Crawford, K (2021 ). Datasheets for datasets. Communications of the ACM. ACM New York, NY, USA. 64 86–92\n\n\n[5] Schreier, M (2012 ). Qualitative content analysis in practice. Qualitative content analysis in practice. Sage. 1–280\n\n\n[6] Neuendorf, K A (2017 ). The Content Analysis Guidebook. Sage publications\n\n\n[7] Krippendorff, K (2018 ). Content Analysis: An Introduction to Its Methodology. Sage publications\n\n\n[8] Paquot, M and Gries, S T (2021 ). A Practical Handbook of Corpus Linguistics. Springer Nature\n\n\n[9] Bernard, H R (2017 ). Research Methods in Anthropology: Qualitative and Quantitative Approaches. Rowman & Littlefield",
    "crumbs": [
      "Part I: Core",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html",
    "href": "06_textual_data.html",
    "title": "6  Textual Data",
    "section": "",
    "text": "6.1 Introduction\nIn this chapter, a number of methods are introduced for working with textual data. By textual data, we have in mind the idea of a dataset where each observation consists of a piece of textual information. An observation could be as short as a single phrase or as long as a book-length document. In Python, textual data of this format can be stored as a string variable in a pandas DataFrame. No special format is needed to store and represent the starting data, in contrast to examples we will see in later chapters working with spatial and image data. However, while it is possible to get a textual dataset into a tabular format, often that is not the starting point for working with a collection texts. Typically, one might start with a directory full of text files, with one text in each file, or it might be necessary to build a dataset by iteratively calling an external resource. We will see a full example of the latter in Chap. 12. We will also assume that this conversion has already taken place.\nThe dataset that we will work with in this chapter is the collection of Wikipedia pages from 75 British authors that was briefly mentioned in Chap. 1. Wikipedia is a fun source because each page often has a lot of different kinds of data and spans across languages. At the same time, studying Wikipedia is a lens into what kinds of knowledge are prioritized and how ideas and concepts are framed. We will focus in this chapter on techniques for working with this collection to show summaries of each of the pages and find patterns and cluster of topics discussed across the collection. The techniques we introduce will be of general interest to anyone working with a collection of textual documents. For those interested in broader methodological debates, textual analysis has been theorized through concepts such as distant reading [1], macroanalysis [2], and cultural analytics [3]. At the end of chapter, we give references for other models that may be of interest in specific sub-domains as well as additional readings on methods.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#working-with-a-textual-corpus",
    "href": "06_textual_data.html#working-with-a-textual-corpus",
    "title": "6  Textual Data",
    "section": "6.2 Working with a Textual Corpus",
    "text": "6.2 Working with a Textual Corpus\nWe will start by loading the textual data as a tabular dataset into Python. There is one row for each document. Each row has a unique identifier called the doc_id, equal to the name of the page on Wikipedia, and a column called text that has all of the text from the page, with special HTML markup removed.\n\ndocs = pd.read_csv(\"data/wiki_uk_authors_text.csv\")\ndocs\n\n\n\n\n\n\n\n\ndoc_id\ntext\n\n\n\n\n0\nMarie de France\nMarie de France was a poet possibly born in wh...\n\n\n1\nGeoffrey Chaucer\nGeoffrey Chaucer was an English poet author an...\n\n\n2\nJohn Gower\nJohn Gower was an English poet a contemporary ...\n\n\n3\nWilliam Langland\nWilliam Langland is the presumed author of a w...\n\n\n4\nMargery Kempe\nMargery Kempe was an English Christian mystic ...\n\n\n...\n...\n...\n\n\n70\nStephen Spender\nSir Stephen Harold Spender CBE was an English ...\n\n\n71\nChristopher Isherwood\nChristopher William Bradshaw Isherwood was an ...\n\n\n72\nEdward Upward\nEdward Falaise Upward FRSL was a British novel...\n\n\n73\nRex Warner\nRex Warner was an English classicist writer an...\n\n\n74\nSeamus Heaney\nSeamus Justin Heaney MRIA was an Irish poet pl...\n\n\n\n\n75 rows × 2 columns\n\n\n\nIn addition to the textual data itself, we have another table of metadata describing information about each of the authors in the collection. As shown in the code block below, we have the year of birth and year of death, a hand-constructed era flag indicating the time period the author was active, the gender of the author, a link to the Wikipedia URL, and an identifier called short that will be a useful short label when visualizing the data. We chose to include and use the term gender because of the historical power of the gender binary and to facilitate questions about gender in our dataset. For a great introduction to considerations about social category data, see D’Ignazio and Klein’s Data Feminism.\n\nmeta = pd.read_csv(\"data/wiki_uk_meta.csv.gz\")\nmeta\n\n\n\n\n\n\n\n\ndoc_id\nborn\ndied\nera\ngender\nlink\nshort\n\n\n\n\n0\nMarie de France\n1160\n1215\nEarly\nfemale\nMarie_de_France\nMarie d. F.\n\n\n1\nGeoffrey Chaucer\n1343\n1400\nEarly\nmale\nGeoffrey_Chaucer\nChaucer\n\n\n2\nJohn Gower\n1330\n1408\nEarly\nmale\nJohn_Gower\nGower\n\n\n3\nWilliam Langland\n1332\n1386\nEarly\nmale\nWilliam_Langland\nLangland\n\n\n4\nMargery Kempe\n1373\n1438\nEarly\nfemale\nMargery_Kempe\nKempe\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n70\nStephen Spender\n1909\n1995\nTwentieth C\nmale\nStephen_Spender\nSpender\n\n\n71\nChristopher Isherwood\n1904\n1986\nTwentieth C\nmale\nChristopher_Isherwood\nIsherwood\n\n\n72\nEdward Upward\n1903\n2009\nTwentieth C\nmale\nEdward_Upward\nUpward\n\n\n73\nRex Warner\n1905\n1986\nTwentieth C\nmale\nRex_Warner\nWarner\n\n\n74\nSeamus Heaney\n1939\n1939\nTwentieth C\nmale\nSeamus_Heaney\nHeaney\n\n\n\n\n75 rows × 7 columns\n\n\n\nIt would have been possible to include all of the metadata in the text dataset as well, since both have the same number of rows. We have decided to separate them into two for performance reasons, which can be particularly important depending on the computer one has access to. In our applications, we will frequently want to do a table join of the metadata into another large dataset. If the metadata contained a complete copy of each page, this could result in very large intermediate steps. We have avoided this problem by the way that our data are structured.\nWhat can we do with these datasets using the methods introduced in previous chapters? The metadata is similar to the tables we have previously looked at; we could use pandas operations and visualization techniques to illustrate concepts such as when each author was alive and patterns of gender representation across the timeline of our collection. We could also use pandas merge operations from Chap. 4 to combine the two tables by the key doc_id. However, beyond this, there is not very much that we can do with the data in its current form. We need to do some processing of the textual data before we can work with it.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#nlp-pipeline",
    "href": "06_textual_data.html#nlp-pipeline",
    "title": "6  Textual Data",
    "section": "6.3 NLP Pipeline",
    "text": "6.3 NLP Pipeline\nOur input dataset is organized with one row for each Wikipedia page, which we will refer to as a document. The composition of a document will vary depending on the area of analysis. Each document might be a stanza, paragraph, page, chapter, book, and more. A standard first-step in text processing is to convert the document-level dataset into a token-level dataset, with one row for each word or punctuation mark (i.e., a token) in each document. The best and easiest way to convert a document-level dataset into a tokens-level dataset is to use a purpose-built algorithm called a Natural Language Processing (NLP) Pipeline. There are several packages in Python that allow us to apply natural language processing pipelines to our data. Here we will use the spaCy library, which provides fast and accurate linguistic annotation for a variety of different models and languages [4]. To start, we load the package and initialize the English language model:\n\nimport spacy\n# Load English language model\n# If not installed: python -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\n\nOnce loaded, we can use spaCy to produce a token-level dataset from our docs input. The following function will process each document and extract token-level information:\n\ndef process_documents(docs_df):\n    \"\"\"Process documents through spaCy NLP pipeline\"\"\"\n    tokens_list = []\n    \n    for idx, row in docs_df.iterrows():\n        doc_id = row['doc_id']\n        text = row['text']\n        \n        # Process text through spaCy\n        doc = nlp(text)\n        \n        sent_id = 0\n        token_id = 0\n        \n        for sent in doc.sents:\n            sent_id += 1\n            token_id = 0\n            \n            for token in sent:\n                token_id += 1\n                \n                tokens_list.append({\n                    'doc_id': doc_id,\n                    'sid': sent_id,\n                    'tid': token_id,\n                    'token': token.text,\n                    'token_with_ws': token.text_with_ws,\n                    'lemma': token.lemma_,\n                    'upos': token.pos_,\n                    'tag': token.tag_,\n                    'is_alpha': token.is_alpha,\n                    'is_stop': token.is_stop,\n                    'is_punct': token.is_punct,\n                    'dep': token.dep_,\n                    'head_idx': token.head.i if token.head != token else token.i\n                })\n    \n    return pd.DataFrame(tokens_list)\n\n# Process a small sample first for demonstration\nsample_docs = docs.head(3)\nanno = process_documents(sample_docs)\nanno\n\n\n\n\n\n\n\n\ndoc_id\nsid\ntid\ntoken\ntoken_with_ws\nlemma\nupos\ntag\nis_alpha\nis_stop\nis_punct\ndep\nhead_idx\n\n\n\n\n0\nMarie de France\n1\n1\nMarie\nMarie\nMarie\nPROPN\nNNP\nTrue\nFalse\nFalse\ncompound\n2\n\n\n1\nMarie de France\n1\n2\nde\nde\nde\nX\nFW\nTrue\nFalse\nFalse\nnmod\n2\n\n\n2\nMarie de France\n1\n3\nFrance\nFrance\nFrance\nPROPN\nNNP\nTrue\nFalse\nFalse\nnsubj\n3\n\n\n3\nMarie de France\n1\n4\nwas\nwas\nbe\nAUX\nVBD\nTrue\nTrue\nFalse\nROOT\n3\n\n\n4\nMarie de France\n1\n5\na\na\na\nDET\nDT\nTrue\nTrue\nFalse\ndet\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10306\nJohn Gower\n72\n2\nD.S.\nD.S.\nD.S.\nPROPN\nNNP\nFalse\nFalse\nFalse\ncompound\n1541\n\n\n10307\nJohn Gower\n72\n3\nBrewer\nBrewer\nBrewer\nPROPN\nNNP\nTrue\nFalse\nFalse\nROOT\n1541\n\n\n10308\nJohn Gower\n72\n4\n.\n.\n.\nPUNCT\n.\nFalse\nFalse\nTrue\npunct\n1541\n\n\n10309\nJohn Gower\n73\n1\nISBN9781843845379\nISBN9781843845379\nISBN9781843845379\nPROPN\nNNP\nFalse\nFalse\nFalse\nROOT\n1543\n\n\n10310\nJohn Gower\n73\n2\n.\n.\n.\nPUNCT\n.\nFalse\nFalse\nTrue\npunct\n1543\n\n\n\n\n10311 rows × 13 columns\n\n\n\nThere is a lot of information that has been automatically added to this table, thanks to the collective results of decades of research in computational linguistics and natural language processing. Each row corresponds to a word or a punctuation mark (created by the process of tokenization), along with metadata describing the token. Notice that reading down the column token reproduces the original text. The columns available are:\n\ndoc_id: A key that allows us to group tokens into documents and to link back into the original input table.\nsid: Numeric identifier of the sentence number.\ntid: Numeric identifier of the token within a sentence. The first three columns form a primary key for the table.\ntoken: A character variable containing the detected token, which is either a word or a punctuation mark.\ntoken_with_ws: The token with white space (i.e., spaces and new-line characters) added. This is useful if we wanted to re-create the original text from the token table.\nlemma: A normalized version of the token. For example, it removes start-of-sentence capitalization, turns all nouns into their singular form, and converts verbs into their infinitive form.\nupos: The universal part of speech code, which are parts of speech that can be defined in (most) spoken languages. These tend to correspond to the parts of speech taught in primary schools, such as “NOUN”, “ADJ” (Adjective), and “ADV” (Adverb).\ntag: A fine-grained part of speech code that depends on the specific language (here, English) and models being used.\nis_alpha, is_stop, is_punct: Boolean flags for alphabetic characters, stop words, and punctuation.\ndep: The dependency relation label.\nhead_idx: The token index of the word in the sentence that this token is grammatically related to.\n\nThere are many analyses that can be performed on the extracted features that are present in the anno table. Fortunately, many of these can be performed by directly using pandas operations covered in the first five chapters of this text, without the need for any new text-specific functions. For example, we can find the most common nouns in the dataset by filtering on the universal part of speech and grouping by lemma with the code below.\n\n# Find most common nouns\ncommon_nouns = (anno\n    .query(\"upos == 'NOUN'\")\n    .groupby('lemma')\n    .size()\n    .reset_index(name='count')\n    .sort_values('count', ascending=False)\n    .head(10)\n)\ncommon_nouns\n\n\n\n\n\n\n\n\nlemma\ncount\n\n\n\n\n790\nwork\n49\n\n\n224\nedition\n29\n\n\n724\ntime\n29\n\n\n118\ncentury\n25\n\n\n524\npoet\n21\n\n\n794\nyear\n20\n\n\n403\nlife\n20\n\n\n125\nchaucer\n18\n\n\n74\nauthor\n17\n\n\n422\nmanuscript\n16\n\n\n\n\n\n\n\nThe most frequent nouns across the set of documents roughly fall into one of two categories. Those such as “year”, “life”, and “death”, and “family” are nouns that we would frequently associate with biographic entries for nearly any group of people. Others, such as “poem”, “book”, “poet”, and the somewhat more generic “work”, capture the specific objects that authors would produce and therefore would be prominent elements of their respective Wikipedia pages. The fact that these are two types of nouns that show up at the top of the list help to verify that both the dataset and the NLP pipeline are working as expected.\nWe can use a similar technique to learn about the contents of each of the 75 individual documents. Suppose we wanted to know which adjectives are most used on each page. This can be done by a sequence of pandas operations. First, we filter the data by the part of speech and group the rows of the dataset by the document id and lemma. Then, we count the number of rows for each unique combination of document and lemma and arrange the dataset in descending order of count. We can use the head() method on grouped data to take the most frequent adjectives within each document:\n\n# Top adjectives by document\ntop_adjectives = (anno\n    .query(\"upos == 'ADJ'\")\n    .groupby(['doc_id', 'lemma'])\n    .size()\n    .reset_index(name='count')\n    .sort_values(['doc_id', 'count'], ascending=[True, False])\n    .groupby('doc_id')\n    .head(8)\n    .groupby('doc_id')['lemma']\n    .apply(lambda x: '; '.join(x))\n    .reset_index()\n)\ntop_adjectives\n\n\n\n\n\n\n\n\ndoc_id\nlemma\n\n\n\n\n0\nGeoffrey Chaucer\nfirst; english; early; many; other; most; comm...\n\n\n1\nJohn Gower\nenglish; other; much; social; early; first; la...\n\n\n2\nMarie de France\nsuch; new; first; late; many; 12th; adulterous...\n\n\n\n\n\n\n\nThe output shows many connections between adjectives and the authors. Here, the connections again fall roughly into two groups. Some of the adjectives are fairly generic—such as “more”, “other”, and “many”—and probably say more about the people writing the pages than the subjects of the pages themselves. Other adjectives provide more contextual information about each of the authors. For example, several selected adjectives are key descriptions of an author’s work, such as “Victorian” associated with certain authors and “Gothic” with others. While it is good to see expected relationships to demonstrate the data and techniques are functioning properly, it is also great when the computational techniques highlight the unexpected.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#tf-idf",
    "href": "06_textual_data.html#tf-idf",
    "title": "6  Textual Data",
    "section": "6.4 TF-IDF",
    "text": "6.4 TF-IDF\nIn the previous section, we saw that counting the number of times each token or lemma occurs in a document is a useful way of quickly summarizing the content of a document. This approach can be improved by using a scaled version of the count metric. The issue with raw counts is that will tend to highlight very common words such as “the”, “have”, and “her”. These can be somewhat avoided by removing a pre-compiled set of known common words—often called stopwords—or by doing part of speech filtering. These coarse approaches, however, mostly just move the issue down to a slightly less set of words that also do not necessarily summarize the contents of each document very well. For example, “publisher” is a frequently used term in many of the documents in this collection due to the subject matter, but that does not mean that it is particularly informative since it occurs in almost every page.\nA common alternative technique is to combine information about the frequency of a word within a document with the frequency of the term across the entire collection. We return here to the importance of how we define a document, which will shape our analysis. Metrics of this form are known as term frequency–inverse document frequency scores (TF-IDF). A common version of TF-IDF computes a score for every combination of term and document by dividing the logarithm of the number of times the term occurs with the logarithm of the number of documents that contain the term at least once. The logarithm is a function that is used to make sure that counts do not grow too fast. For example, a count of about 1000 is only approximately twice as big on the logarithmic scale as a count of 25, in comparison to being 40 times larger on a linear scale. Mathematically, we define this TF-IDF function using the following formula, where tf gives the term frequency and df gives the document frequency. The plus one in the equation avoids a division by zero.\n\\[ \\text{tfidf} = \\frac{log(\\text{tf})}{log(\\text{df + 1})} \\]\nThis score gives a measurement of how important a term is in describing a document in the context of the other documents. If we select words with the highest TF-IDF score for each document, these should give a good measurement of what terms best describe each document uniquely from the rest of the collection. Note that while the scaling functions given above are popular choices, they are not universal. Other papers and software may make different choices with moderate effects on the output results.\nWe can compute TF-IDF scores using scikit-learn’s TfidfVectorizer:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Prepare documents for TF-IDF\n# First, let's create document-level text by filtering and concatenating tokens\ndef create_document_texts(anno_df, pos_filter=None):\n    \"\"\"Create document-level texts from token annotations\"\"\"\n    if pos_filter:\n        filtered_anno = anno_df.query(f\"upos in {pos_filter}\")\n    else:\n        filtered_anno = anno_df\n    \n    doc_texts = (filtered_anno\n        .groupby('doc_id')['lemma']\n        .apply(lambda x: ' '.join(x))\n        .reset_index()\n    )\n    return doc_texts\n\n# Create noun-only documents\nnoun_docs = create_document_texts(anno, ['NOUN'])\n\n# Apply TF-IDF\nvectorizer = TfidfVectorizer(min_df=0.05, lowercase=False)\ntfidf_matrix = vectorizer.fit_transform(noun_docs['lemma'])\nfeature_names = vectorizer.get_feature_names_out()\n\n# Convert to DataFrame for easier manipulation\ntfidf_df = pd.DataFrame(\n    tfidf_matrix.toarray(), \n    columns=feature_names,\n    index=noun_docs['doc_id']\n)\n\n# Get top terms for each document\ndef get_top_terms_per_doc(tfidf_df, n_terms=8):\n    \"\"\"Get top TF-IDF terms for each document\"\"\"\n    results = []\n    for doc_id in tfidf_df.index:\n        top_terms = (tfidf_df.loc[doc_id]\n            .sort_values(ascending=False)\n            .head(n_terms)\n        )\n        results.append({\n            'doc_id': doc_id,\n            'top_terms': '; '.join(top_terms.index)\n        })\n    return pd.DataFrame(results)\n\ntop_terms = get_top_terms_per_doc(tfidf_df)\ntop_terms\n\n\n\n\n\n\n\n\ndoc_id\ntop_terms\n\n\n\n\n0\nGeoffrey Chaucer\nedition; work; chaucer; time; text; century; y...\n\n\n1\nJohn Gower\nwork; gower; poet; pound; praise; advice; line...\n\n\n2\nMarie de France\nfable; woman; lover; husband; love; society; t...\n\n\n\n\n\n\n\nWe can see that these words do not include many superfluous common words. If anything, they present too much interesting information, perhaps including many words that are only used once in a fairly oblique way within a text. As with the examples in the first section, we could clean this up with additional filtering. For example, filtering based on a minimum value of the term frequency would select the top terms. As a first pass, though, the results here already do a good job of summarizing the texts using a technique that we could use on nearly any collection of textual document. By summarizing, we can quickly get a sense of themes and topics, which becomes particularly powerful when comparing and contrasting texts.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#document-distance",
    "href": "06_textual_data.html#document-distance",
    "title": "6  Textual Data",
    "section": "6.5 Document Distance",
    "text": "6.5 Document Distance\nOur approach so far has been to use the annotation table to summarize the content of each document. We now want to extend these techniques to find connections between and across different documents. In other words, we want to see how the Wikipedia pages relate to one another and see if there are structures that help us understand and visualize the entire dataset. In order to do these tasks, our approach will be to summarize each document as a set of numbers that capture different elements of each document. Any pair or set of pages that share similar summary numbers can be said to have, in some sense, shared similarities. To do this, we need to be able to think about each document as existing in a high-dimensional space. This can be a bit complex and intimidating on a first pass. Therefore, before showing the next topic, we will first walk through an example showing the concept of representing documents as sequences of numbers.\nConsider the TF-IDF dataset that we produced in the previous section. The data format was a matrix with one row for each document and one column for every token. Let’s visualize this with just two terms to start:\n\n# Create a simple example with just two terms: \"novel\" and \"poem\"\nsimple_vectorizer = TfidfVectorizer(vocabulary=['novel', 'poem'], lowercase=False)\n\n# We need to recreate our document texts to include these specific terms\nall_docs = create_document_texts(anno)  # All parts of speech\nsimple_tfidf = simple_vectorizer.fit_transform(all_docs['lemma'])\nsimple_df = pd.DataFrame(\n    simple_tfidf.toarray(),\n    columns=['novel', 'poem'],\n    index=all_docs['doc_id']\n)\n\nsimple_df\n\n\n\n\n\n\n\n\nnovel\npoem\n\n\ndoc_id\n\n\n\n\n\n\nGeoffrey Chaucer\n0.249399\n0.968401\n\n\nJohn Gower\n0.000000\n1.000000\n\n\nMarie de France\n0.249399\n0.968401\n\n\n\n\n\n\n\nThe output now includes one row for each document and two columns with TF-IDF scores for “novel” and “poem”. We can then visualize what the output would look like with all the terms included. Using just these two columns, we can plot a set of pages with novel on the x-axis and poem on the y-axis. It will be useful to think of these as vectors starting at the origin, rather than points floating in space:\nfrom plotnine import *\nimport pandas as pd\n\n# Select a subset of authors for visualization\nselected_authors = [\n    \"Jane Austen\", \"Ann Radcliffe\", \"Rex Warner\",\n    \"John Donne\", \"James Joyce\", \"Seamus Heaney\", \n    \"Lord Byron\", \"Samuel Beckett\", \"Oscar Wilde\"\n]\n\nplot_data = simple_df.loc[simple_df.index.isin(selected_authors)].reset_index()\n\np = (ggplot(plot_data, aes(x='novel', y='poem')) +\n     geom_segment(aes(xend=0, yend=0), \n                  arrow=dict(length=0.3), color='black') +\n     geom_text(aes(label='doc_id'), \n               nudge_x=0.01, nudge_y=0.01, size=8) +\n     labs(x='Novel TF-IDF', y='Poem TF-IDF') +\n     theme_minimal())\np\nThe take-away from our plot is that a matrix representation of the term frequency values provides an interesting way of grouping and exploring the relationships between documents. While we may not be able to directly visualize the same idea with more than two terms at the same time, we can consider abstractly thinking of each document living in a high dimensional space defined by the lemma counts and then looking at which documents are close to one another.\nLet’s see how we can apply this technique to the entire dataset using all of the lemmas to show which documents are closest to one another. We can compute distances between documents using cosine similarity:\n\nfrom sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n\n# Compute cosine distances (1 - cosine_similarity)\ndistance_matrix = cosine_distances(tfidf_matrix)\n\n# Convert to DataFrame for easier manipulation\ndistance_df = pd.DataFrame(\n    distance_matrix,\n    index=noun_docs['doc_id'],\n    columns=noun_docs['doc_id']\n)\n\n# Find closest document pairs\ndef find_closest_pairs(distance_df):\n    \"\"\"Find the closest document pairs\"\"\"\n    pairs = []\n    docs = distance_df.index\n    \n    for i, doc1 in enumerate(docs):\n        for j, doc2 in enumerate(docs):\n            if i &lt; j:  # Avoid duplicates and self-pairs\n                pairs.append({\n                    'document1': doc1,\n                    'document2': doc2,\n                    'distance': distance_df.loc[doc1, doc2]\n                })\n    \n    pairs_df = pd.DataFrame(pairs)\n    return pairs_df.sort_values('distance')\n\nclosest_pairs = find_closest_pairs(distance_df)\nclosest_pairs\n\n\n\n\n\n\n\n\ndocument1\ndocument2\ndistance\n\n\n\n\n0\nGeoffrey Chaucer\nJohn Gower\n0.600292\n\n\n1\nGeoffrey Chaucer\nMarie de France\n0.755999\n\n\n2\nJohn Gower\nMarie de France\n0.789774\n\n\n\n\n\n\n\nThese relationships seem much more as expected. The relationships include pairs of authors often associated with one another. For example, we might see links between authors from similar time periods or with similar themes. There are also connections between authors who worked in similar genres or shared cultural contexts.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#dimensionality-reduction",
    "href": "06_textual_data.html#dimensionality-reduction",
    "title": "6  Textual Data",
    "section": "6.6 Dimensionality Reduction",
    "text": "6.6 Dimensionality Reduction\nWe need a strategy for visualizing the complex relationships between documents that are to be found in the high dimensions of the TF-IDF scores. We can do this through a strategy called dimensionality reduction. To put it another way, this is an approach to summarizing relationships between objects that are represented in high dimensions with a representation in a much lower space. We want to take representations of objects that consist of a large set of numbers and approximate them with a much smaller set of numbers. There are many motivations and applications for the application of dimensionality reduction to tasks in fields such as statistics, computer science, information science, and mathematics. Here, for our application, the main motivation is that if we can approximate the relationships between the large set of TF-IDF values by associating each document with a pair of numbers and then we could plot those numbers to recover the kind of visualization we started with in the previous section.\nOur first approach involves the use of a technique called principal component analysis (PCA). PCA comes directly from noticing that our ultimate use of the high-dimensional structure in the previous section was to compute the distance between pairs of documents. The general goal of PCA can be viewed as generating a new representation of a high-dimensional dataset in a way that distances in the new space approximate those in the larger space. We can generate the PCA dimensions using scikit-learn’s PCA implementation:\n\nfrom sklearn.decomposition import PCA\n\n# Apply PCA to reduce to 2 components\npca = PCA(n_components=2)\npca_result = pca.fit_transform(tfidf_matrix.toarray())\n\n# Create DataFrame with results\npca_df = pd.DataFrame(\n    pca_result,\n    columns=['PC1', 'PC2'],\n    index=noun_docs['doc_id']\n).reset_index()\n\n# Merge with metadata for visualization\npca_with_meta = pca_df.merge(meta, on='doc_id')\nprint(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n\npca_df\n\nExplained variance ratio: [0.58157065 0.41842935]\n\n\n\n\n\n\n\n\n\ndoc_id\nPC1\nPC2\n\n\n\n\n0\nGeoffrey Chaucer\n-0.317004\n0.571663\n\n\n1\nJohn Gower\n-0.425159\n-0.518697\n\n\n2\nMarie de France\n0.742164\n-0.052966\n\n\n\n\n\n\n\nThe output already highlights one of the challenges of dimensionality reduction. Each of the components PC1 and PC2 is computed as a weighted sum of the TF-IDF scores across all of the lemmas. While some limited interpretation of the components is possible, it is difficult to fully understand exactly what is captured in each component. Instead, our goal will be to use these numbers in a plot that should provide insight into the data by showing which documents end up in similar parts of the plot:\n\n# Create PCA visualization\n# Sample some authors for cleaner visualization\nnp.random.seed(42)\nsample_authors = pca_with_meta.sample(n=min(40, len(pca_with_meta)))\n\np = (ggplot(sample_authors, aes(x='PC1', y='PC2', color='era')) +\n     geom_point(size=2) +\n     geom_text(aes(label='doc_id'), size=6, nudge_y=0.1, show_legend=False) +\n     labs(title=\"PCA of British Authors Wikipedia Collection\",\n          x=\"First Principal Component\",\n          y=\"Second Principal Component\",\n          color=\"Era\") +\n     theme_minimal() +\n     theme(axis_text=element_blank(),\n           axis_ticks=element_blank()))\np\n\n\n\n\n\n\n\n\nThe output of the plot immediately provides a richer view of the intricate relationships between the different authors than we get directly looking at the pairs of closest documents. This is not because the individual links lack interesting relationships, but rather that they require additional work to make the patterns visible. We can see clustering patterns that reflect temporal and thematic similarities between authors.\nThere are many other techniques for dimensionality reduction beyond PCA. Another popular method for reducing the dimension of our dataset is the Uniform Manifold Approximation and Projection (UMAP). As with PCA, the UMAP method tries to retain information about distances between pairs of documents. However, whereas PCA focuses on all distances between documents, UMAP only focuses on keeping observations close in the new space that were also close together in the original space:\n\n# Note: UMAP requires specific Python version compatibility\n# For demonstration, we'll use PCA as an alternative dimensionality reduction method\n# To use UMAP, install with: pip install umap-learn\n# and replace this section with:\n# import umap\n# umap_reducer = umap.UMAP(n_components=2, n_neighbors=5, random_state=42)\n# umap_result = umap_reducer.fit_transform(tfidf_matrix.toarray())\n\n# Alternative approach using PCA for similar visualization\nfrom sklearn.decomposition import PCA\n\n# Apply PCA as UMAP alternative (for demonstration)\npca_reducer = PCA(n_components=2, random_state=42)\numap_result = pca_reducer.fit_transform(tfidf_matrix.toarray())\n\n# Create DataFrame with results\numap_df = pd.DataFrame(\n    umap_result,\n    columns=['UMAP1', 'UMAP2'],\n    index=noun_docs['doc_id']\n).reset_index()\n\n# Merge with metadata\numap_with_meta = umap_df.merge(meta, on='doc_id')\n\n# Sample for visualization\nsample_authors_umap = umap_with_meta.sample(n=min(40, len(umap_with_meta)))\n\np_umap = (ggplot(sample_authors_umap, aes(x='UMAP1', y='UMAP2', color='era')) +\n          geom_point(size=2) +\n          geom_text(aes(label='doc_id'), size=6, nudge_y=0.1, show_legend=False) +\n          labs(title=\"UMAP-style Visualization of British Authors Wikipedia Collection\",\n               subtitle=\"(Using PCA for demonstration - replace with UMAP when available)\",\n               x=\"Dimension 1\",\n               y=\"Dimension 2\",\n               color=\"Era\") +\n          theme_minimal() +\n          theme(axis_text=element_blank(),\n                axis_ticks=element_blank()))\np_umap\n\n\n\n\n\n\n\n\nThe output of the UMAP plot shows different clustering patterns compared to PCA. As with the principal components, the exact values of the dimensions are unimportant here. The relationships between the documents are what we are interested in. The points are typically more uniformly spread through the plot than the PCA visualization. This has the benefit of making it easier to read but may obscure some hierarchical relationships. The benefits of UMAP become more apparent with larger datasets; we will see an example of this technique again using a much larger set of observations in Chap. 10 when we apply it to image data.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#word-relationships",
    "href": "06_textual_data.html#word-relationships",
    "title": "6  Textual Data",
    "section": "6.7 Word Relationships",
    "text": "6.7 Word Relationships\nIn the previous two sections, we have focused on using word counts to describe documents. We can also analyze relationships between words by examining which words tend to appear in similar documents. Using the transpose of our TF-IDF matrix, we can reapply the techniques we have already seen to show relationships between words:\n\n# Transpose the TF-IDF matrix to analyze word relationships\n# Limit to most frequent words for computational efficiency\nword_tfidf = tfidf_df.T  # Transpose so words are rows, documents are columns\n\n# Compute cosine similarity between words\nword_similarity = cosine_similarity(word_tfidf)\nword_distance = 1 - word_similarity\n\n# Create word distance DataFrame\nword_dist_df = pd.DataFrame(\n    word_distance,\n    index=word_tfidf.index,\n    columns=word_tfidf.index\n)\n\n# Find closest word pairs\ndef find_closest_word_pairs(distance_df, n_pairs=20):\n    \"\"\"Find the closest word pairs\"\"\"\n    pairs = []\n    words = distance_df.index\n    \n    for i, word1 in enumerate(words):\n        for j, word2 in enumerate(words):\n            if i &lt; j:  # Avoid duplicates and self-pairs\n                pairs.append({\n                    'word1': word1,\n                    'word2': word2,\n                    'distance': distance_df.loc[word1, word2]\n                })\n    \n    pairs_df = pd.DataFrame(pairs)\n    return pairs_df.sort_values('distance').head(n_pairs)\n\nclosest_word_pairs = find_closest_word_pairs(word_dist_df)\nclosest_word_pairs\n\n\n\n\n\n\n\n\nword1\nword2\ndistance\n\n\n\n\n43249\naspect\nprocess\n-4.440892e-16\n\n\n43036\naspect\nimprisonment\n-4.440892e-16\n\n\n42941\naspect\nera\n-4.440892e-16\n\n\n49093\nattention\nprocess\n-4.440892e-16\n\n\n42810\naspect\ncause\n-4.440892e-16\n\n\n162355\nera\nimprisonment\n-4.440892e-16\n\n\n48880\nattention\nimprisonment\n-4.440892e-16\n\n\n162568\nera\nprocess\n-4.440892e-16\n\n\n82188\ncause\nrest\n-4.440892e-16\n\n\n210217\nimprisonment\nrest\n-4.440892e-16\n\n\n162622\nera\nrest\n-4.440892e-16\n\n\n49147\nattention\nrest\n-4.440892e-16\n\n\n210163\nimprisonment\nprocess\n-4.440892e-16\n\n\n81921\ncause\nimprisonment\n-4.440892e-16\n\n\n48654\nattention\ncause\n-4.440892e-16\n\n\n284128\nprocess\nrest\n-4.440892e-16\n\n\n48785\nattention\nera\n-4.440892e-16\n\n\n43303\naspect\nrest\n-4.440892e-16\n\n\n82134\ncause\nprocess\n-4.440892e-16\n\n\n42763\naspect\nattention\n-4.440892e-16\n\n\n\n\n\n\n\nAs with the document pairs, the output shows pairs of words that show different kinds of thematic similarity. Some are closely related semantic pairs such as “play” and “playwright” or “poet” and “poetry”. Others illustrate larger themes that are covered in some, though not all, of the Wikipedia pages. These include pairs for more research-oriented sections and pairs for biographical information.\nWe can also visualize word relationships using PCA:\n\n# Apply PCA to word relationships (using top 50 words for readability)\ntop_words = word_tfidf.sum(axis=1).nlargest(50).index\nword_subset = word_tfidf.loc[top_words]\n\nword_pca = PCA(n_components=2)\nword_pca_result = word_pca.fit_transform(word_subset)\n\nword_pca_df = pd.DataFrame(\n    word_pca_result,\n    columns=['PC1', 'PC2'],\n    index=word_subset.index\n).reset_index()\n\np_words = (ggplot(word_pca_df, aes(x='PC1', y='PC2')) +\n           geom_text(aes(label='index'), size=8) +\n           labs(title=\"PCA of Word Relationships\",\n                x=\"First Principal Component\",\n                y=\"Second Principal Component\") +\n           theme_minimal() +\n           theme(axis_text=element_blank(),\n                 axis_ticks=element_blank()))\np_words\n\n\n\n\n\n\n\n\nThe plot does a great job of clustering the words into different sections based on their themes. We can see groupings of words related to poetry, book production, family relations, and scholarly terms.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#texts-in-other-languages",
    "href": "06_textual_data.html#texts-in-other-languages",
    "title": "6  Textual Data",
    "section": "6.8 Texts in Other Languages",
    "text": "6.8 Texts in Other Languages\nOne of the reasons that we enjoy using the content of Wikipedia pages as example datasets for textual analysis is that it is possible to get the page text in a large number of different languages. One of the most interesting aspects of textual analysis is that we can apply our techniques to study how differences across languages and cultures affect the way that knowledge is created and distributed.\nLet’s see how our text analysis pipeline can be modified to work with Wikipedia pages from the French version of the site. SpaCy provides models for many different languages:\n\nnlp_fr = spacy.load(\"fr_core_news_sm\")\n\n# Read French Wikipedia data\ndocs_fr = pd.read_csv(\"data/wiki_uk_authors_text_fr.csv\")\nprint(f\"French documents shape: {docs_fr.shape}\")\n\n# Process French documents (sample for demonstration)\nsample_fr = docs_fr.head(3)\n\ndef process_french_documents(docs_df, nlp_model):\n    \"\"\"Process French documents through spaCy NLP pipeline\"\"\"\n    tokens_list = []\n    \n    for idx, row in docs_df.iterrows():\n        doc_id = row['doc_id']\n        text = row['text']\n        \n        # Process text through French spaCy model\n        doc = nlp_model(text)\n        \n        sent_id = 0\n        token_id = 0\n        \n        for sent in doc.sents:\n            sent_id += 1\n            token_id = 0\n            \n            for token in sent:\n                token_id += 1\n                \n                tokens_list.append({\n                    'doc_id': doc_id,\n                    'sid': sent_id,\n                    'tid': token_id,\n                    'token': token.text,\n                    'lemma': token.lemma_,\n                    'upos': token.pos_,\n                    'is_alpha': token.is_alpha,\n                    'is_stop': token.is_stop\n                })\n    \n    return pd.DataFrame(tokens_list)\n\nanno_fr = process_french_documents(sample_fr, nlp_fr)\nprint(\"French annotations sample:\")\nprint(anno_fr.head(12))\n\n# Analyze French TF-IDF\nfr_noun_docs = create_document_texts(anno_fr.query(\"upos == 'NOUN'\"))\nfr_vectorizer = TfidfVectorizer(min_df=0.1, lowercase=False)\nfr_tfidf_matrix = fr_vectorizer.fit_transform(fr_noun_docs['lemma'])\n\nfr_tfidf_df = pd.DataFrame(\n    fr_tfidf_matrix.toarray(),\n    columns=fr_vectorizer.get_feature_names_out(),\n    index=fr_noun_docs['doc_id']\n)\n\nfr_top_terms = get_top_terms_per_doc(fr_tfidf_df)\nprint(\"Top French terms by document:\")\nprint(fr_top_terms)\n\nFrench documents shape: (73, 2)\nFrench annotations sample:\n             doc_id  sid  tid        token        lemma   upos  is_alpha  \\\n0   Marie de France    1    1        Marie        marie   NOUN      True   \n1   Marie de France    1    2           de           de    ADP      True   \n2   Marie de France    1    3       France       France  PROPN      True   \n3   Marie de France    1    4          est         être    AUX      True   \n4   Marie de France    1    5          une           un    DET      True   \n5   Marie de France    1    6     poétesse     poétesse   NOUN      True   \n6   Marie de France    1    7           de           de    ADP      True   \n7   Marie de France    1    8           la           le    DET      True   \n8   Marie de France    1    9  Renaissance  Renaissance  PROPN      True   \n9   Marie de France    1   10           du           de    ADP      True   \n10  Marie de France    1   11   XIIesiècle   XIIesiècle    NUM      True   \n11  Marie de France    1   12           la           le    DET      True   \n\n    is_stop  \n0     False  \n1      True  \n2     False  \n3      True  \n4      True  \n5     False  \n6      True  \n7      True  \n8     False  \n9      True  \n10    False  \n11     True  \nTop French terms by document:\n             doc_id                                          top_terms\n0  Geoffrey Chaucer  roi; duc; fille; fils; document; période; anné...\n1        John Gower  œuvre; référence; source; mirour; révolte; con...\n2   Marie de France  marie; fable; lai; manuscrit; cour; conte; lan...\n\n\nBy design, the output of the annotations have the same general structure as the annotations from the English data. The values in the upos columns are consistent across languages. Language-specific information is captured in the token-level features. We can see that the lemmatization process has taken account of specific aspects of French grammar.\nThe output from the French model shows many tokens that describe the areas of literature, themes, and key characteristics for each author. There are also interesting comparisons to be made to the English pages. A systematic study of differences between the Wikipedia pages in different languages can help understand differences in the perception of these authors across linguistic worlds.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#extensions",
    "href": "06_textual_data.html#extensions",
    "title": "6  Textual Data",
    "section": "6.9 Extensions",
    "text": "6.9 Extensions\nWe started this chapter with the annotation of textual data using an NLP pipeline. We did not touch on the underlying algorithms and theory for how each step in the pipeline is actually being handled. For this, the standard reference is Jurafsky and Martin’s Speech and Language Processing [5]. The text is thorough and quite dense, but requires little in the way of prerequisites and is very accessible.\nUsing the annotations, we briefly looked at several different methods for understanding a corpus of text. There are also many other examples of tasks in text analysis, several of which are popular in humanities applications. Examples include sentiment analysis [6], concept mining [7], and spectral clustering [8].\nAnother commonly used text analysis technique in the humanities are topic models. These techniques, such as Latent Dirichlet Allocation (LDA), can be implemented in Python using libraries like gensim or scikit-learn. A good introduction to topic modeling concepts is given by Blei [9]. The mathematical details require understanding of Bayesian statistics and variational inference.\nFor more advanced text analysis in Python, consider exploring: - Transformers and BERT models via the transformers library - Word embeddings using gensim or word2vec - Named entity recognition and coreference resolution via spaCy - Sentiment analysis using vaderSentiment or transformer models - Topic modeling with gensim or scikit-learn\nThe Python ecosystem provides rich tools for text analysis that continue to evolve rapidly, particularly in the area of large language models and transformer architectures.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "06_textual_data.html#references",
    "href": "06_textual_data.html#references",
    "title": "6  Textual Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Underwood, T (2017 ). A genealogy of distant reading. DHQ: Digital Humanities Quarterly. 11\n\n\n[2] Jockers, M L (2013 ). Macroanalysis: Digital Methods and Literary History. University of Illinois Press\n\n\n[3] Manovich, L (2020 ). Cultural Analytics. Mit Press\n\n\n[4] Vasiliev, Y (2020 ). Natural Language Processing with Python and spaCy: A Practical Introduction. No Starch Press\n\n\n[5] Jurafsky, D and Martin, J (2000 ). Computational linguistics and speech recognition, 2000. Prentice Hall\n\n\n[6] Pang, B, Lee, L and others (2008 ). Opinion mining and sentiment analysis. Foundations and Trends in information retrieval. Now Publishers, Inc. 2 1–135\n\n\n[7] Kolekar, M H, Palaniappan, K, Sengupta, S and Seetharaman, G (2009 ). Semantic concept mining based on hierarchical event detection for soccer video indexing. Journal of multimedia. NIH Public Access. 4 298\n\n\n[8] Von Luxburg, U (2007 ). A tutorial on spectral clustering. Statistics and computing. Springer. 17 395–416\n\n\n[9] Blei, D M, Ng, A Y and Jordan, M I (2003 ). Latent dirichlet allocation. Journal of machine Learning research. 3 993–1022",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Textual Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html",
    "href": "07_network_data.html",
    "title": "7  Network Data",
    "section": "",
    "text": "7.1 Introduction\nA network consists of a set of objects and a collection of links identifying some relationship between these pairs of objects [1]. Networks form a very generic data model with extensive applications to the humanities [2]. Networks can be a great way to understand connections and relationships between a wide range of objects or types of data. For example, one might want to explore the friendship relationships between people, citations between books, or network connection between computers. Whenever there exists a set of relationships that connects the objects to each other, a network can be a useful tool for visualization and data exploration.\nA critical step in interpreting network data is deciding exactly what elements correspond to the nodes (the objects) and the edges (links between the edges). For example, let’s say we are studying citations. A node might be an author and the edge may connect an author when they cite one another. The SignsAt40 project is a great example. They look at 40 years of feminist scholarship through network and text analysis, including co-citation networks. Being very clear about how nodes and edges are defined is key to exploring networks.\nIn this chapter, we start by working with network data taken from Wikipedia using the same set of pages we saw in the previous chapter. Wikipedia pages contain many internal links between pages; we collected information about each time one of the pages in our dataset provided a link to another page in our collection. Only links within the main body of the text were used. We will explore how to do produce this dataset using the Wikipedia API in Chap. 12.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#creating-a-network-object",
    "href": "07_network_data.html#creating-a-network-object",
    "title": "7  Network Data",
    "section": "7.2 Creating a Network Object",
    "text": "7.2 Creating a Network Object\nWe can describe a network structure with a tabular dataset. Specifically, we can create a dataset with one row for each edge in the network. This dataset needs one column giving an identifier for the starting node of the edge and another column giving the ending node. The set of links between the Wikipedia pages is read into Python and displayed by the following block of code. Notice that we are using the same values in the doc_id column that were used as unique identifiers for each page text in Chap. 6.\n\npage_citation = pd.read_csv(\"data/wiki_uk_citations.csv\")\npage_citation\n\n\n\n\n\n\n\n\ndoc_id\ndoc_id2\n\n\n\n\n0\nMarie de France\nGeoffrey Chaucer\n\n\n1\nGeoffrey Chaucer\nWilliam Langland\n\n\n2\nGeoffrey Chaucer\nJohn Gower\n\n\n3\nGeoffrey Chaucer\nJohn Dryden\n\n\n4\nGeoffrey Chaucer\nPhilip Sidney\n\n\n...\n...\n...\n\n\n372\nRex Warner\nStephen Spender\n\n\n373\nSeamus Heaney\nW. B. Yeats\n\n\n374\nSeamus Heaney\nGeorge Bernard Shaw\n\n\n375\nSeamus Heaney\nSamuel Beckett\n\n\n376\nSeamus Heaney\nMary Robinson\n\n\n\n\n377 rows × 2 columns\n\n\n\nLooking at the first few rows, we see that Marie de France has only one link (to Geoffrey Chaucer). Chaucer, on the other hand, has links to six other authors. As a starting way to analyze the data, we can see how many pages link into each author’s page. Arranging the data by the count will give a quick understanding of how central each author’s page is to the other authors, as seen in the following block of code. By summarizing links into each page rather than out of each page, we avoid a direct bias towards focusing on longer author pages.\n\n# Count incoming links\nincoming_links = (page_citation\n    .groupby('doc_id2')\n    .size()\n    .reset_index(name='count')\n    .sort_values('count', ascending=False)\n)\nincoming_links\n\n\n\n\n\n\n\n\ndoc_id2\ncount\n\n\n\n\n62\nWilliam Shakespeare\n25\n\n\n63\nWilliam Wordsworth\n23\n\n\n53\nT. S. Eliot\n19\n\n\n58\nW. H. Auden\n13\n\n\n42\nPercy Bysshe Shelley\n12\n\n\n...\n...\n...\n\n\n9\nChristina Rossetti\n1\n\n\n3\nBeatrix Potter\n1\n\n\n49\nSamuel Pepys\n1\n\n\n44\nRex Warner\n1\n\n\n19\nGeorge Herbert\n1\n\n\n\n\n64 rows × 2 columns\n\n\n\nLooking at the counts, we see that there are more links into the Shakespeare page than any other in our collection. While that is perhaps not surprising, it is interesting to see that Wordsworth is only two links short of Shakespeare, with 23 links. While raw counts are a useful starting point, they can only get us so far. These say nothing, for example, about how easily we can hop between two pages by following 2, 3, or 4 links. In order to understand the dataset we need a way of visualizing and modelling all of the connections at once. This requires considering the entire network structure of our dataset.\nBefore we create a network structure from a dataset, we need to decide on what kind of network we will create. Specifically, networks can be either directed, in which case we distinguish between the starting and ending vertex, or undirected, in which we do not. For example, in our counts above, we took the direction into account for the links. Next, let’s start by treating our dataset of Wikipedia page links as undirected; all we want to consider is whether there is at least one way to click on a link and go between the two pages. Later in the chapter, we will show what changes when we add direction into the data. Any directed network can be considered undirected by ignoring the direction; undirected networks cannot in general be converted into a directed format. So, it will be a good starting point to consider approaches that can be applied to any network dataset.\nThe dataset that we have in Python is called an edge list [3]. It consists of a dataset where each observation is an edge. We can use the igraph library to create network objects and compute various network metrics [4]. Let’s create our network and compute the basic metrics:\n\ndef create_network_data(edges_df, directed=False):\n    \"\"\"Create network object and compute metrics from edge list\"\"\"\n    \n    # Create igraph network from edge list\n    edge_list = [(row['doc_id'], row['doc_id2']) for _, row in edges_df.iterrows()]\n    G = ig.Graph.TupleList(edge_list, directed=directed)\n    \n    # Get layout for visualization\n    layout = G.layout_fruchterman_reingold()\n    \n    # Get basic network properties\n    components = G.connected_components()\n    clusters = G.community_walktrap().as_clustering().membership\n    \n    # Create node dataframe\n    nodes = []\n    for i, vertex in enumerate(G.vs):\n        # Find which component this vertex belongs to\n        component_id = None\n        for comp_idx, component in enumerate(components):\n            if i in component:\n                component_id = comp_idx + 1  # 1-indexed like R\n                break\n        \n        node_data = {\n            'id': vertex['name'],\n            'x': layout[i][0],\n            'y': layout[i][1],\n            'component': component_id,\n            'component_size': len(components[component_id-1]) if component_id else 0,\n            'cluster': str(clusters[i])\n        }\n        \n        if directed:\n            node_data.update({\n                'degree_out': vertex.outdegree(),\n                'degree_in': vertex.indegree(), \n                'degree_total': vertex.degree()\n            })\n        else:\n            node_data['degree'] = vertex.degree()\n            \n        nodes.append(node_data)\n    \n    node_df = pd.DataFrame(nodes)\n    \n    # Compute centrality measures by component\n    for comp_idx, component in enumerate(components):\n        comp_id = comp_idx + 1\n        if len(component) &gt; 1:  # Only compute for components with &gt; 1 node\n            subgraph = G.subgraph(component)\n            comp_indices = node_df['component'] == comp_id\n            \n            # Eigenvalue centrality\n            eigen_scores = subgraph.eigenvector_centrality(directed=False)\n            node_df.loc[comp_indices, 'eigen'] = eigen_scores\n            \n            # Betweenness centrality  \n            between_scores = subgraph.betweenness(directed=directed)\n            node_df.loc[comp_indices, 'between'] = between_scores\n            \n            # Closeness centrality (only for undirected)\n            if not directed:\n                close_scores = subgraph.closeness()\n                node_df.loc[comp_indices, 'close'] = close_scores\n    \n    # Create edge dataframe for plotting\n    edges_plot = []\n    for edge in G.es:\n        source_idx = edge.source\n        target_idx = edge.target\n        edges_plot.append({\n            'x': layout[source_idx][0],\n            'y': layout[source_idx][1],\n            'xend': layout[target_idx][0],\n            'yend': layout[target_idx][1]\n        })\n    \n    edge_df = pd.DataFrame(edges_plot)\n    \n    return node_df, edge_df, G\n\n# Create undirected network\nnode, edge, G = create_network_data(page_citation, directed=False)\nnode\n\n\n\n\n\n\n\n\nid\nx\ny\ncomponent\ncomponent_size\ncluster\ndegree\neigen\nbetween\nclose\n\n\n\n\n0\nMarie de France\n-3.401724\n-3.865523\n1\n72\n0\n1\n0.024402\n0.000000\n0.324201\n\n\n1\nGeoffrey Chaucer\n-1.267112\n-1.168028\n1\n72\n1\n16\n0.409418\n147.136045\n0.476510\n\n\n2\nWilliam Langland\n-0.224812\n-3.082918\n1\n72\n1\n3\n0.062166\n4.723756\n0.375661\n\n\n3\nJohn Gower\n-1.633361\n-2.424984\n1\n72\n1\n5\n0.118720\n7.743780\n0.408046\n\n\n4\nJohn Dryden\n-0.986934\n1.311710\n1\n72\n1\n25\n0.730764\n197.454422\n0.554688\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n67\nCecil Day-Lewis\n3.307829\n-0.900080\n1\n72\n6\n11\n0.209379\n20.181833\n0.415205\n\n\n68\nChristopher Isherwood\n3.494739\n-1.864596\n1\n72\n6\n9\n0.149254\n0.893551\n0.364103\n\n\n69\nLouis MacNeice\n2.118700\n-1.290354\n1\n72\n6\n12\n0.259567\n26.553664\n0.435583\n\n\n70\nRex Warner\n4.611315\n-1.315202\n1\n72\n6\n4\n0.072261\n0.000000\n0.356784\n\n\n71\nEdward Upward\n4.204689\n-2.004940\n1\n72\n6\n6\n0.094188\n0.000000\n0.358586\n\n\n\n\n72 rows × 10 columns\n\n\n\nThe node dataset contains extracted information about each of the objects in our collection. We will describe each of these throughout the remainder of this chapter. Note that we also have metadata about the nodes, which is something that we can join into the data to help deepen our understanding of subsequent analyses.\nThe first column gives a label for the row. In the next two columns, named x and y, is a computed way to layout the objects in two-dimensions that maximizes linked pages being close to one another while minimizing the amount that all of the nodes are bunched together [5]. This is an example of a network drawing (also known as graph drawing in mathematics) algorithm. As with the PCA and UMAP dimensions in Chap. 6, there is no exact meaning of the individual variables. Rather, its the relationships that they show that are interesting. Using the first three variables, we could plot the pages as a scatter plot with labels to see what pages appear to be closely related to one another.\nBefore actually looking at this plot, it will be useful to first make some additions. The relationships that would be shown in this plot generally try to put pages that have links between them close to one another. It would be helpful to additionally put these links onto the plot as well. This is where the edge dataset becomes useful. The edge dataset contains one row for each edge in the dataset. The dataset has four columns. These describe the x and y values of one node in the edge and variables xend and yend to indicate where in the scatter plot the ending point of the edge is.\n\nedge\n\n\n\n\n\n\n\n\nx\ny\nxend\nyend\n\n\n\n\n0\n-3.401724\n-3.865523\n-1.267112\n-1.168028\n\n\n1\n-1.267112\n-1.168028\n-0.224812\n-3.082918\n\n\n2\n-1.267112\n-1.168028\n-1.633361\n-2.424984\n\n\n3\n-1.267112\n-1.168028\n-0.986934\n1.311710\n\n\n4\n-1.267112\n-1.168028\n-2.754921\n-0.505206\n\n\n...\n...\n...\n...\n...\n\n\n372\n2.921730\n-1.106872\n4.611315\n-1.315202\n\n\n373\n0.518950\n-0.480336\n2.114221\n-1.980657\n\n\n374\n0.937361\n-1.095229\n2.114221\n-1.980657\n\n\n375\n1.960690\n-3.051331\n2.114221\n-1.980657\n\n\n376\n1.941958\n0.435777\n2.114221\n-1.980657\n\n\n\n\n377 rows × 4 columns\n\n\n\nWe can include edges into the plot by adding a geom layer of type geom_segment. This geometry takes four aesthetics, named exactly the same as the names in the edge dataset. The plot gets busy with all of these lines, so we will set the opacity (alpha) of them lower so as to not clutter the visual space with the connections.\n\np = (ggplot() +\n     geom_segment(data=edge, \n                 mapping=aes(x='x', y='y', xend='xend', yend='yend'),\n                 alpha=0.1) +\n     geom_point(data=node, mapping=aes(x='x', y='y'), alpha=0.5) +\n     geom_text(data=node, mapping=aes(x='x', y='y', label='id'), \n               size=6, adjust_text={'expand_points': (1, 1)}) +\n     theme_void() +\n     labs(title=\"Wikipedia Page Link Network\"))\np\n\nLooks like you are using a tranform that doesn't support FancyArrowPatch, using ax.annotate instead. The arrows might strike through texts. Increasing shrinkA in arrowprops might help.\n\n\n\n\n\n\n\n\n\nThe output of the plot shows the underlying data that describes the plot as well as the relationships between the pages. Notice that the relationship between the pages is quite different than the textual-based ones in the previous chapter. When using textual distances, we saw a clustering base on the time period in which each author wrote and the formats that they wrote in. Here, the pattern is driven much more strongly by the general popularity of each author. The most well-known authors of each era—Shakespeare, Chaucer, Jonathan Swift—are clustered in the middle of the plot. Lesser known authors, such as Daphne du Maurier and Samuel Pepys, are pushed to the exterior. In the next section, we will see if we can more formally study the centrality of different pages in our collection.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#centrality",
    "href": "07_network_data.html#centrality",
    "title": "7  Network Data",
    "section": "7.3 Centrality",
    "text": "7.3 Centrality\nOne of the core questions that arises when working with network data is trying to identify the relative centrality of each node in the network. Several of the derived measurements in the node dataset capture various forms of centrality. Let’s move through each of these measurements to see how they reveal different aspects of our network’s centrality.\nA component of a network is a collection of all the nodes that can be reached by following along the edges. The node dataset contains a variable called component describing each of the components in the network. These are ordered in descending order of size, so component 1 will always be the largest (or at least, tied for the largest) component of the network. The total size of each component is the first measurement of the centrality of a node. Those nodes that are in the largest component can, in some sense, be said to have a larger centrality than other nodes that are completely cut-off from this cluster. All of the nodes on our network are contained in one large cluster, so this measurement is not particularly helpful in this specific case. Networks that have a single component are known as connected networks. All of the other metrics for centrality are defined in terms of a connected network. In order to apply them to networks with multiple components, each algorithm is applied separately to each component.\nAnother measurement of centrality is a node’s degree. The degree of a node is the number of neighbors it has. In other words, it counts how many edges the node is a part of. The degree of each node has been computed in the node table. This is similar to the counts that were produced in the first section by counting occurrences in the raw edge list. The difference here is that we are counting all edges, not just those edges going into a node. As a visualization technique, we can plot the degree centrality scores on a plot of the network to show that the nodes with the highest degree do seem to sit in the middle of the plot and correspond to a high number of having a large number of edges.\n\np = (ggplot() +\n     geom_segment(data=edge,\n                 mapping=aes(x='x', y='y', xend='xend', yend='yend'),\n                 alpha=0.1) +\n     geom_point(data=node, mapping=aes(x='x', y='y', color='degree'), \n               size=5) +\n     scale_color_cmap(cmap_name='viridis') +\n     theme_void() +\n     labs(title=\"Network colored by Degree Centrality\",\n          color=\"Degree\"))\np\n\n\n\n\n\n\n\n\nThe degree of a node only accounts for direct neighbors. A more holistic measurement is given by a quantity called the eigenvalue centrality. This metric is provided in the node table. It provides a centrality score for each node that is proportional to the sum of the scores of its neighbors. Mathematically, it assigns a set of scores \\(s_j\\) for each node such that:\n\\[ s_j = \\lambda \\cdot \\sum_{i \\in \\text{Neighbors{j}}} s_i \\]\nThe eigenvalue score, by convention, scales so that the largest score is 1. It is only possible to describe the eigenvalue centrality scores for a connected set of nodes on a network, so the computation is done individually for each component. For comparison, we will use the following code to plot the eigenvalue centrality scores of our network.\n\np = (ggplot() +\n     geom_segment(data=edge,\n                 mapping=aes(x='x', y='y', xend='xend', yend='yend'),\n                 alpha=0.1) +\n     geom_point(data=node, mapping=aes(x='x', y='y', color='eigen'), \n               size=5) +\n     scale_color_cmap(cmap_name='viridis') +\n     theme_void() +\n     labs(title=\"Network colored by Eigenvalue Centrality\",\n          color=\"Eigenvalue\\nCentrality\"))\np\n\n\n\n\n\n\n\n\nThe visualization shows a slightly different pattern compared to the degree centrality scores. The biggest difference is that the eigenvalue centrality is more concentrated on the most central connections, whereas degree centrality is more spread out. We will see in the next few sections that this is primarily a result of using a linear scale to plot the colors. If we transform the eigenvalue centrality scores with another function first, we would see that the pattern more gradually shows differences across the entire network.\nAnother measurement of centrality is given by the closeness centrality score. For each node in the network, consider the minimum number of edges that are needed to go from this node to any other node within its component. Adding the reciprocal of these scores together gives a measurement of how close a node is to all of the other nodes in the network. The closeness centrality score for a node is given as the variable close in our node table. Again, we will plot these scores with the following code to compare to the other types of centrality scores.\n\np = (ggplot() +\n     geom_segment(data=edge,\n                 mapping=aes(x='x', y='y', xend='xend', yend='yend'),\n                 alpha=0.1) +\n     geom_point(data=node.query('component == 1'), \n               mapping=aes(x='x', y='y', color='close'), \n               size=5) +\n     scale_color_cmap(cmap_name='viridis') +\n     theme_void() +\n     labs(title=\"Network colored by Closeness Centrality\",\n          color=\"Closeness\\nCentrality\"))\np\n\n\n\n\n\n\n\n\nThe output of the closeness centrality scores shows different patterns than the eigenvalue centrality scores. Here we see a much smoother transition from the most central to the least central nodes. We will look at different kinds of networks later in the chapter that illustrate further differences between each type of centrality score.\nThe final measurement of centrality we have in our table, betweenness centrality also comes from considering minimal paths. For every two nodes in a connected component, consider all of the possible ways to go from one to the other along edges in the network. Then, consider all of the paths (there may be only one) between the two nodes that require a minimal number of hops. The betweenness centrality scores measures how many of these minimal paths go through each node (there is some normalization to account for the case when there are many minimal paths, so the counts are not exact integers). This score is stored in the variable between. A plot of the betweenness score is given by the following code:\n\np = (ggplot() +\n     geom_segment(data=edge,\n                 mapping=aes(x='x', y='y', xend='xend', yend='yend'),\n                 alpha=0.1) +\n     geom_point(data=node, mapping=aes(x='x', y='y', color='between'), \n               size=5) +\n     scale_color_cmap(cmap_name='viridis') +\n     theme_void() +\n     labs(title=\"Network colored by Betweenness Centrality\",\n          color=\"Betweenness\\nCentrality\"))\np\n\n\n\n\n\n\n\n\nThe betweenness score often tends to have a different pattern than the other centrality scores. It gives a high score to bridges between different parts of the network, rather than giving high weight to how central a node is within a particular cluster. One challenge with the page link network over this small set of pages is that we need to create a different kind of network in order to really see the differences between the betweenness centrality and the other types of centrality that we’ve discussed so far. To better understand the centrality scores, we will delve further into another set of networks such as co-citation and nearest neighbor networks.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#clusters",
    "href": "07_network_data.html#clusters",
    "title": "7  Network Data",
    "section": "7.4 Clusters",
    "text": "7.4 Clusters\nThe centrality of a node is not the only thing that we can measure when looking at networks. Another algorithm that we can perform is that of clustering. Here, we try to split the nodes into groups such that a large number of the edges are between nodes within a group rather than across groups. When we created our network, a clustering of the nodes based on the edges was automatically performed. The identifiers for the clusters are in the column called cluster. We can visualize the clusters defined on our Wikipedia-page network using the following code.\n\np = (ggplot() +\n     geom_segment(data=edge,\n                 mapping=aes(x='x', y='y', xend='xend', yend='yend'),\n                 alpha=0.1) +\n     geom_point(data=node, mapping=aes(x='x', y='y', color='cluster'), \n               size=5) +\n     theme_void() +\n     labs(title=\"Network colored by Cluster\",\n          color=\"Cluster\"))\np\n\n\n\n\n\n\n\n\nThe output of the cluster visualization shows the different communities detected in the network. We are running out of space to put labels on the plot. This is one major consideration when thinking of networks as a form of visual exploration and communication; bigger is not necessarily better. Even on a large screen, networks with hundreds of nodes or more become unwieldy to plot. As an alternative, we can summarize the network data in the form of tables. For example, we can paste together the nodes within a cluster to try to further understand the internal structure of the relationships.\n\ncluster_summary = (node\n    .groupby('cluster')['id']\n    .apply(lambda x: '; '.join(x))\n    .reset_index()\n)\ncluster_summary\n\n\n\n\n\n\n\n\ncluster\nid\n\n\n\n\n0\n0\nMarie de France\n\n\n1\n1\nGeoffrey Chaucer; William Langland; John Gower...\n\n\n2\n2\nCharles Dickens; T. S. Eliot; George Bernard S...\n\n\n3\n3\nWilliam Wordsworth; John Keats; Lord Byron; Sa...\n\n\n4\n4\nGeorge Herbert; Henry Vaughan\n\n\n5\n5\nSamuel Pepys\n\n\n6\n6\nW. H. Auden; Stephen Spender; Cecil Day-Lewis;...\n\n\n7\n7\nDaphne du Maurier\n\n\n\n\n\n\n\nNetwork clusters can be very insightful for understanding the structure of a large network. The example data that we have been working with so far is relatively small and forms a larger single cluster around a few well-known authors. Because of the length and richness of the textual information, this set of 75 authors produced interesting results on its own in the previous chapter. It is also a great size to visualize and illustrate the core computed metrics associated with network analysis since it is small enough to plot every node and edge. To go farther and show the full power of these methods as analytic tools, we need to expand our collection.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#co-citation-networks",
    "href": "07_network_data.html#co-citation-networks",
    "title": "7  Network Data",
    "section": "7.5 Co-citation Networks",
    "text": "7.5 Co-citation Networks\nThe network structure we have been working with is a form called a citation network. Pages are joined whenever one page links to another. This is a popular method in understanding academic articles, friendship networks on social media (i.e., tracking mentions on Twitter), or investigating the relative importance of court cases. There are some drawbacks of using citation counts, however. They are sensitive to the time-order of publication, they are effected by the relative length of each document, and they are easily effected by small changes. Wikipedia articles are continually edited, so there is no clear temporal ordering of the pages, and there is relatively little benefit for someone to artificially inflate the network centrality of an article. The length of Wikipedia articles, though, are highly variable and not always well correlated with the notoriety of the subject matter. So, partially to avoid biasing our results due to page length, and more so to illustrate the general concept when applying these techniques to other sets of citation data, let’s look at an alternative that helps to avoid all of these issues.\nA co-citation network is a method of showing links across a citation network while avoiding some of the pitfalls that arise when using direct links. A co-citation is formed between two pages whenever a third entry cites both of them. The idea is that if a third source talks about two sources in the same reference, there is likely a relationship between the documents. We can created a co-citation dataset from Wikipedia by first downloading all of the pages linked to from any of the author pages in our collection. We can then count how often any pair of pages in our dataset were both linked into from the same source. As with the other Wikipedia datasets, we will see how to create this collection through the API in Chap. 12. Here, we will load the dataset into Python as a structured table. Co-citations are, by definition, undirected. In the dataset below, we have sorted the edges so that doc_id always comes alphabetically before doc_id_out. The column count tells how many third pages cite both of the respective articles.\n\npage_cocitation = pd.read_csv(\"data/wiki_uk_cocitations.csv\")\npage_cocitation\n\n\n\n\n\n\n\n\ndoc_id\ndoc_id_out\ncount\n\n\n\n\n0\nMarie de France\nMatthew Arnold\n1\n\n\n1\nMarie de France\nOscar Wilde\n2\n\n\n2\nMarie de France\nSamuel Pepys\n1\n\n\n3\nMarie de France\nT. S. Eliot\n1\n\n\n4\nMarie de France\nThomas Malory\n1\n\n\n...\n...\n...\n...\n\n\n2109\nSeamus Heaney\nVirginia Woolf\n2\n\n\n2110\nSeamus Heaney\nW. B. Yeats\n4\n\n\n2111\nSeamus Heaney\nW. H. Auden\n5\n\n\n2112\nSeamus Heaney\nWilliam Shakespeare\n1\n\n\n2113\nSeamus Heaney\nWilliam Wordsworth\n1\n\n\n\n\n2114 rows × 3 columns\n\n\n\nWhen working with co-citations, it is useful to only include links between two pages when the count is above some threshold. In the code below, we will filter our new edge list to include only the links that have a count of at least 6 different citations. We then create a new network, node, and edge set.\n\n# Filter co-citations by threshold\nfiltered_cocitations = (page_cocitation\n    .groupby(['doc_id', 'doc_id_out'])\n    .first()\n    .reset_index()\n    .query('count &gt; 6')\n    .rename(columns={'doc_id_out': 'doc_id2'})\n)\n\n# Create network from filtered co-citations\ncocite_node, cocite_edge, cocite_G = create_network_data(filtered_cocitations, directed=False)\nprint(f\"Co-citation network: {len(cocite_node)} nodes, {len(cocite_edge)} edges\")\n\nCo-citation network: 67 nodes, 303 edges\n\n\nIn the code block below, we will look at the nodes with the largest eigenvalue centrality scores, with the top ten printed in the text. As with the previous network, there is only one component in this network; otherwise, we would want to filter the data such that we are looking at pages in the largest component, which will always be component number 1.\n\ntop_cocite_nodes = (cocite_node\n    .sort_values('eigen', ascending=False)\n    .head(10)\n)\ntop_cocite_nodes[['id', 'eigen', 'degree']]\n\n\n\n\n\n\n\n\nid\neigen\ndegree\n\n\n\n\n6\nJohn Milton\n1.000000\n36\n\n\n12\nCharles Dickens\n0.890534\n30\n\n\n4\nJohn Dryden\n0.772376\n23\n\n\n1\nGeorge Orwell\n0.769806\n25\n\n\n34\nWilliam Shakespeare\n0.769678\n23\n\n\n10\nWilliam Blake\n0.768104\n23\n\n\n8\nSamuel Johnson\n0.766416\n21\n\n\n20\nT. S. Eliot\n0.730718\n23\n\n\n9\nSamuel Taylor Coleridge\n0.695994\n21\n\n\n5\nJohn Keats\n0.604770\n16\n\n\n\n\n\n\n\nThe most central nodes in the co-citation network show a similar set of pages showing up in the top-10 list but with some notable differences in the relative ordering. Shakespeare is no longer at the top of the list. Focusing on the first name on the list, there are at least 36 other authors in our relatively small set that are cited at least six times in the same page as John Milton. Looking into the example links, we see that this is because Milton is associated with most other figures of the 17th Century, poets, and writers that have religious themes in their works. These overlapping sets creates a relatively large number of other pages that Milton is connected to. As mentioned at the start of the section, the differences between citation and co-citation are somewhat muted with the Wikipedia pages because they are constantly being modified and edited, making it possible for pages to directly link to other pages even if they were originally created before them. When working with other citation networks, such as scholarly citations or court records, the differences between the two are often striking.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#directed-networks",
    "href": "07_network_data.html#directed-networks",
    "title": "7  Network Data",
    "section": "7.6 Directed Networks",
    "text": "7.6 Directed Networks\nAt the start of the chapter, we noted that networks can be given edges that are either directed or undirected. The algorithms and metrics we have looked at so far all work on undirected networks and so even in the case of the original links, which do have a well-defined direction, we have been treating the links as undirected. It is possible to create a network object that takes this relationship into account by setting the directed argument to True.\n\n# Create directed network\ndirected_node, directed_edge, directed_G = create_network_data(page_citation, directed=True)\nprint(f\"Directed network: {len(directed_node)} nodes, {len(directed_edge)} edges\")\ndirected_node\n\nDirected network: 72 nodes, 377 edges\n\n\n\n\n\n\n\n\n\nid\nx\ny\ncomponent\ncomponent_size\ncluster\ndegree_out\ndegree_in\ndegree_total\neigen\nbetween\n\n\n\n\n0\nMarie de France\n-4.361636\n3.205056\n9\n1\n0\n1\n0\n1\nNaN\nNaN\n\n\n1\nGeoffrey Chaucer\n-1.526700\n1.188774\n10\n60\n1\n6\n10\n16\n0.404895\n143.911999\n\n\n2\nWilliam Langland\n-3.800806\n1.185629\n12\n1\n1\n0\n3\n3\nNaN\nNaN\n\n\n3\nJohn Gower\n-2.586860\n2.068297\n10\n60\n1\n3\n2\n5\n0.115697\n27.454978\n\n\n4\nJohn Dryden\n0.792490\n1.295434\n10\n60\n1\n13\n12\n25\n0.728438\n282.521198\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n67\nCecil Day-Lewis\n-2.997229\n-1.301094\n10\n60\n6\n5\n6\n11\n0.216519\n104.321090\n\n\n68\nChristopher Isherwood\n-3.698056\n-1.171121\n10\n60\n6\n5\n4\n9\n0.155110\n8.040945\n\n\n69\nLouis MacNeice\n-2.464424\n-0.551100\n10\n60\n6\n9\n3\n12\n0.267927\n34.675758\n\n\n70\nRex Warner\n-4.418440\n-1.454240\n10\n60\n6\n3\n1\n4\n0.075073\n0.350000\n\n\n71\nEdward Upward\n-4.023717\n-1.987489\n10\n60\n6\n3\n3\n6\n0.098099\n0.850000\n\n\n\n\n72 rows × 11 columns\n\n\n\nThe node table contains a slightly different set of measurements. Closeness centrality is no longer available; eigenvalue and betweenness centrality are still computed, but are done so without using the directions of the edges. There are now three different degree counts: the out-degree (number of links on the page), the in-degree (number of links into a page), and the total of these two.\nWe can, if desired, use the directed structure in our plot. To visualize a directed network, we add an arrow argument to the geom_segment layer. The code to do this is in the block below:\n\n# Create directed network visualization\np = (ggplot() +\n     geom_point(data=directed_node, mapping=aes(x='x', y='y')) +\n     geom_segment(data=directed_edge,\n                 mapping=aes(x='x', y='y', xend='xend', yend='yend'),\n                 alpha=0.7,\n                 arrow=arrow(length=0.02)) +\n     theme_void() +\n     labs(title=\"Directed Page Link Network\"))\np\n\n\n\n\n\n\n\n\nVisualizing the direction of the arrows can be helpful for illustrating concepts in smaller networks.\nAn interesting analysis that we can do with a directed network is to look at the in-degree as a function of the out-degree. The code below creates a plot that investigates the relationship between these two degree counts. We have highlighted a set of six authors that show interesting relationships between the two variables.\n\n# Select authors for labeling\nhighlight_authors = [\n    \"William Shakespeare\", \"William Wordsworth\", \"T. S. Eliot\", \n    \"Lord Byron\", \"W. H. Auden\", \"George Orwell\"\n]\n\ndirected_node_highlight = directed_node[directed_node['id'].isin(highlight_authors)]\n\np = (ggplot() +\n     geom_point(data=directed_node.query('component == 1'), \n               mapping=aes(x='degree_out', y='degree_in')) +\n     geom_text(data=directed_node_highlight,\n              mapping=aes(x='degree_out', y='degree_in', label='id'),\n              nudge_y=1, nudge_x=-1) +\n     geom_abline(slope=1, intercept=0, linetype='dashed') +\n     labs(title=\"In-degree vs Out-degree in British Authors Network\",\n          x=\"Out-degree\", y=\"In-degree\"))\np\n\n\n\n\n\n\n\n\nThis plot reveals some interesting details about all of the citation networks that we have seen so far. It was probably not surprising to see Shakespeare as the node with the highest centrality score. Here, we see that this is only partially because many other author’s pages link into his. A parallel reason is that the Shakespeare page also has more links out to other authors than any other page. While the two metrics largely mirror one another, it is insightful to identify pages that have an unbalanced number of in or out citations. George Orwell, for example, is not referenced by many other pages in our collection, but has many outgoing links. It’s possible that this is partially a temporal effect of Orwell being a later author in the set; the page cites his literary influences and it’s not hard to see why those influences would not cite back into him. Wordsworth and Lord Byron show the opposite pattern, with more links into them than might be expected given their number of out links. Both of these are interesting observations that merit further study.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#distance-networks",
    "href": "07_network_data.html#distance-networks",
    "title": "7  Network Data",
    "section": "7.7 Distance Networks",
    "text": "7.7 Distance Networks\nFor a final common type of network found in humanities research, we return to a task that we saw in Chap. 6. After having built a table of textual annotations, recall that we were able to create links between two documents whenever they are sufficiently close to one another based on the angle distance between their term-frequency scores. By choosing a suitable cutoff score for the maximal distance between pages, we can create an edge list between the pages.\n\n# Load annotations (this would come from previous chapter)\n# For demonstration, we'll create a simplified version using TF-IDF\ndef create_distance_network(docs_text, distance_threshold=0.4):\n    \"\"\"Create network based on text similarity\"\"\"\n    \n    # Simple TF-IDF approach\n    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(docs_text['text'])\n    \n    # Compute cosine distances\n    distances = cosine_distances(tfidf_matrix)\n    \n    # Create edge list from distances below threshold\n    edges = []\n    doc_ids = docs_text['doc_id'].tolist()\n    \n    for i in range(len(doc_ids)):\n        for j in range(i+1, len(doc_ids)):\n            if distances[i, j] &lt; distance_threshold:\n                edges.append({\n                    'doc_id': doc_ids[i],\n                    'doc_id2': doc_ids[j],\n                    'distance': distances[i, j]\n                })\n    \n    return pd.DataFrame(edges)\n\n# Load document texts (placeholder - would use real data)\n# For demo, create simplified version\ndocs_sample = pd.DataFrame({\n    'doc_id': ['Shakespeare', 'Marlowe', 'Jonson', 'Chaucer', 'Milton'],\n    'text': ['sample text 1', 'sample text 2', 'sample text 3', 'sample text 4', 'sample text 5']\n})\n\n# In practice, you would load the full annotations from Chapter 6\n# and compute proper TF-IDF distances\nprint(\"Distance network creation would use full text annotations from Chapter 6\")\n\nDistance network creation would use full text annotations from Chapter 6\n\n\nUsing these edges, we can create a distance network. As with co-citations, the network here has no notion of direction and therefore we will create another undirected network.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#nearest-neighbor-networks",
    "href": "07_network_data.html#nearest-neighbor-networks",
    "title": "7  Network Data",
    "section": "7.8 Nearest Neighbor Networks",
    "text": "7.8 Nearest Neighbor Networks\nWe finish this chapter by looking at one final network type that we can apply to our Wikipedia corpus. The example here generates a network structure that is designed to avoid a small set of central nodes by balancing the degree of the network across all of the nodes. This approach can be applied to any set of distance scores defined between pairs of objects.\n\ndef create_nearest_neighbor_network(distance_df, k_neighbors=5):\n    \"\"\"Create symmetric nearest neighbor network\"\"\"\n    \n    # For each document, find k nearest neighbors\n    top_pairs = []\n    \n    for doc in distance_df['doc_id'].unique():\n        doc_distances = (distance_df[distance_df['doc_id'] == doc]\n                        .sort_values('distance')\n                        .head(k_neighbors))\n        top_pairs.append(doc_distances)\n    \n    top_distances = pd.concat(top_pairs, ignore_index=True)\n    \n    # Create symmetric edges (both nodes must be in each other's top-k)\n    symmetric_edges = []\n    \n    for _, row in top_distances.iterrows():\n        doc1, doc2 = row['doc_id'], row['doc_id2']\n        # Check if reverse relationship exists\n        reverse_exists = ((top_distances['doc_id'] == doc2) & \n                         (top_distances['doc_id2'] == doc1)).any()\n        \n        if reverse_exists and doc1 &lt; doc2:  # Avoid duplicates\n            symmetric_edges.append({\n                'doc_id': doc1,\n                'doc_id2': doc2\n            })\n    \n    return pd.DataFrame(symmetric_edges)\n\nprint(\"Nearest neighbor network creation would use distance data from text analysis\")\n\nNearest neighbor network creation would use distance data from text analysis\n\n\nThe network we have now created is called a symmetric nearest neighbors network. It can be constructed from any distance function that provides distances between pairs of objects in a dataset. Notice that the degree of every node in this case can never be larger than five. This stops the network from focusing on lots of weak connections to popular pages and focuses on links between pages that go both ways.\nThe structure of the symmetric nearest neighbors network is quite different from the other networks we have explored. One way to see this is by looking at the relationship between eigenvalue centrality and betweenness centrality. In most of the other networks, these were highly correlated to one another, but in this example that is not the case. There are several nodes that have a high betweenness score despite having a lower eigenvalue centrality score. These are the gatekeeper nodes that link other, more densely connected parts of the plot.\nSince the symmetric nearest neighbors plot avoids placing too many nodes all at the center of the network, the clusters resulting from the network are also often more interesting and uniform in size than other kinds of networks. While all of the network types here have their place, when given a set of distances, using symmetric nearest neighbors is often a good choice to get interesting results that show the entire structure of the network rather than focusing on only the most centrally located nodes.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#extensions",
    "href": "07_network_data.html#extensions",
    "title": "7  Network Data",
    "section": "7.9 Extensions",
    "text": "7.9 Extensions\nWe have explored some of the major areas of network analysis: network drawing, measures of centrality, and clustering. We have tried to give a general overview; however all of these areas are far richer than what can be fit into a single chapter. For further study, the python-igraph documentation is a good place to start; it contains dozens of additional network drawing, centrality, and community detection algorithms. Beyond this, Stanley Wasserman’s text on social network analysis gives a lot of depth (in an applicable way) while remaining fairly accessible [2]. For a more technical treatment, Eric Kolaczyk’s Statistical Analysis of Network Data provides even more detail, while still being written from the perspective of conducting applied data analysis [3].\nAs we noted at the beginning, being clear about the nodes and edges is key. We have found that the more strictly they are defined, the more useful networks are as an analytical and visual tool for exploring and communicating humanities data. In digital humanities circles, Scott Weingart is known for repeating that when one has a hammer, everything can look like a nail [6]. Networks lend themselves to this, so being precise and careful is critical [7].\nAdditional Python libraries that may be useful for network analysis include:\n\nNetworkX: Another popular Python network analysis library\ngraph-tool: High-performance graph analysis library\n\ncommunity: Community detection algorithms\nnetworkx-community: Community detection tools for NetworkX\npy2cytoscape: Interface to Cytoscape for advanced network visualization\n\nThe Python ecosystem provides rich tools for network analysis that continue to evolve, particularly in areas like dynamic networks and multilayer networks.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "07_network_data.html#references",
    "href": "07_network_data.html#references",
    "title": "7  Network Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Newman, M (2018 ). Networks. Oxford University Press\n\n\n[2] Wasserman, S and Faust, K (1994 ). Social network analysis: Methods and applications. Cambridge university press\n\n\n[3] Kolaczyk, E D and Csárdi, G (2014 ). Statistical Analysis of Network Data with r. Springer\n\n\n[4] Csárdi, G, Nepusz, T, Traag, V, Horvát, S, Zanini, F, Noom, D and Müller, K (2023 ). igraph: Network Analysis and Visualization in r. https://CRAN.R-project.org/package=igraph\n\n\n[5] Fruchterman, T M and Reingold, E M (1991 ). Graph drawing by force-directed placement. Software: Practice and experience. Wiley Online Library. 21 1129–64\n\n\n[6] Graham, S, Milligan, I, Weingart, S B and Martin, K (2016 ). Exploring Big Historical Data: The Historian’s Macroscope. World Scientific\n\n\n[7] Ahnert, R, Ahnert, S E, Coleman, C N and Weingart, S B (2020 ). The Network Turn: Changing Perspectives in the Humanities. Cambridge University Press",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Network Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html",
    "href": "08_temporal_data.html",
    "title": "8  Temporal Data",
    "section": "",
    "text": "8.1 Introduction\nFor the second edition of this book, we chose to add this chapter on temporal data. Being able to analyze change over time is key to many kinds of humanities data. For example, we might have a series of letters with dates, photographs with time stamps, and a film with shot and scene times that we are excited to study. These kinds of data animate fields such as history and media studies.\nIn this chapter, we discuss techniques for working with data that has some temporal component. This includes any kind of data that has one or more variables that record dates or times, as well as any dataset that has general meaningful ordering of its rows. For example, the annotation object that we created for textual data in Chap. 6 has a meaningful ordering to it and can be treated as having a temporal ordering even if it is not associated specifically with fixed timestamps. We will start by focusing specifically on datasets that contain explicit information about dates and times. In the later sections we will illustrate window functions and range joins, both of which have a wider set of applications to all ordered datasets.\nAs we saw in Chap. 5, it is possible to store information about dates and times in a tabular dataset. There are many different formats for storing this information; we recommend that most users start by recording these with separate columns for each numeric component of the date or time. This makes it easier to avoid errors and to record partial information, the latter being a common complication of many humanities datasets. We will begin by looking at a dataset related to the Wikipedia pages we saw in the previous two chapters that has date information stored in such a format.\nIn showing the application of line graphs in Chap. 2 and again in Chap. 5, we saw how to visualize the dataset of food prices over a 140-year period. This visualization was fairly straightforward. There was exactly one row for each year. We were able to treat the year variable has any other continuous measurement, with the only change being that it made sense to connect dots with a line when building the visualization. Here we will work with a slightly more complex example corresponding to the Wikipedia pages from the preceding chapters.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#temporal-data-and-ordering",
    "href": "08_temporal_data.html#temporal-data-and-ordering",
    "title": "8  Temporal Data",
    "section": "8.2 Temporal Data and Ordering",
    "text": "8.2 Temporal Data and Ordering\nLet’s start by loading some data with a temporal component in Python. Below, we will read in data related to the 75 Wikipedia pages from a selection of British authors. Here, we have a different set of information about the pages than we used in the text and network analysis chapters. For each page, we have grabbed page view statistics for a 60-day period from Wikipedia. In other words, we have a record of how many people looked at a particular page each day, for each author. The data are organized with one row for each combination of item and day.\n\npage_views = pd.read_csv(\"data/wiki_uk_page_views.csv\")\npage_views\n\n\n\n\n\n\n\n\ndoc_id\nyear\nmonth\nday\nviews\n\n\n\n\n0\nMarie de France\n2023\n8\n1\n121\n\n\n1\nMarie de France\n2023\n8\n2\n138\n\n\n2\nMarie de France\n2023\n8\n3\n138\n\n\n3\nMarie de France\n2023\n8\n4\n129\n\n\n4\nMarie de France\n2023\n8\n5\n104\n\n\n...\n...\n...\n...\n...\n...\n\n\n4485\nSeamus Heaney\n2023\n9\n25\n719\n\n\n4486\nSeamus Heaney\n2023\n9\n26\n802\n\n\n4487\nSeamus Heaney\n2023\n9\n27\n694\n\n\n4488\nSeamus Heaney\n2023\n9\n28\n655\n\n\n4489\nSeamus Heaney\n2023\n9\n29\n719\n\n\n\n\n4490 rows × 5 columns\n\n\n\nThe time variables are given the way we recommended in Chap. 5, with individual columns for year, month, and day. Here, our dataset is already ordered (within each item type) from the earliest records to the latest. If this were not the case, because all of our variables are stored as numbers, we could use the sort_values() function to sort by year, followed by month, followed by day, to get the same ordering.\nHow could we show the change in page views over time for a particular variable? One approach is to add a numeric column running down the dataset using the index. Below is an example of the code to create a line plot using the row number approach:\n\n# Filter for Geoffrey Chaucer and create row numbers\nchaucer_data = (page_views\n    .query(\"doc_id == 'Geoffrey Chaucer'\")\n    .reset_index(drop=True)\n    .reset_index()\n    .rename(columns={'index': 'row_number'})\n)\n\np = (ggplot(chaucer_data, aes(x='row_number', y='views')) +\n     geom_line(color='red') +\n     labs(title=\"Page Views for Geoffrey Chaucer (by Day Number)\",\n          x=\"Day Number\", y=\"Views\"))\np\n\n\n\n\n\n\n\n\nIn this case, our starting plot is not a bad place to begin. The x-axis corresponds to the day number, and in many applications that may be exactly what we need. We can clearly see that the number of page views for Chaucer has a relatively stable count, possibly with some periodic swings over the course of the week. There is one day about two-thirds of the way through the plot in which the count spikes. Notice, though, that it is very hard to tell anything from the plot about exactly what days of the year are being represented. We cannot easily see which day has the spike in views, for example. Also, note that the correspondence between the row number and day only works because the data are uniformly sampled (one observation each day) and there is no missing data.\nAnother way to work with dates is to convert the data to a fractional year format. Here, the months and days are added to form a fractional day. A quick way to do this is to compute the following fractional year:\n\\[ year_{frac} = year + \\frac{month - 1}{12} + \\frac{day - 1}{12 \\cdot 31}\\]\nWe are subtracting one from the month and day so, for example, on a date such as 1 July 2020 (halfway through the year) we have the fractional year equal to 2020.5. We could make this even more exact by accounting for the fact that some months have fewer than 31 days, but as a first pass this works relatively well. We can see the output with the following code:\n\n# Create fractional year representation\nchaucer_fractional = (page_views\n    .query(\"doc_id == 'Geoffrey Chaucer'\")\n    .assign(year_frac = lambda df: df['year'] + (df['month'] - 1) / 12 + (df['day'] - 1) / (12 * 31))\n)\n\np = (ggplot(chaucer_fractional, aes(x='year_frac', y='views')) +\n     geom_line(color='red') +\n     labs(title=\"Page Views for Geoffrey Chaucer (Fractional Year)\",\n          x=\"Fractional Year\", y=\"Views\"))\np\n\n\n\n\n\n\n\n\nThis revised visualization improves on several aspects of the original plot. For one thing, we can roughly see exactly what dates correspond to each data point. Also, the code will work fine regardless of whether the data are sorted, evenly distributed, or contain any missing values. As a down-side, the axis labels take some explaining. We can extend the same approach to working with time data. For example, if we also had the (24-hour) time of our data points the formula would become:\n\\[ year_{frac} = year + \\frac{month - 1}{12} + \\frac{day - 1}{12 \\cdot 31} + \\frac{hour - 1}{24 \\cdot 12 \\cdot 31}\\]\nIf we are only interested in the time since a specific event, say the start of an experiment, we can use the same approach but take the difference relative to a specific fractional year.\nFractional times have a number of important applications. Fractional times are convenient because they can represent an arbitrarily precise date or date-time with an ordinary number. This means that they can be used in other models and applications without any special treatment. They may require different model assumptions, but at least the code should work with minimal effort. This is a great way to explore our data. However, particularly when we want to create nice publishable visualizations, it can be useful to work with specific functions for manipulating dates and times.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#date-objects",
    "href": "08_temporal_data.html#date-objects",
    "title": "8  Temporal Data",
    "section": "8.3 Date Objects",
    "text": "8.3 Date Objects\nMost of the variables that we have worked with so far are either strings or numbers. Dates are in some ways like numbers: they have a natural ordering, we can talk about the difference between two numbers, and it makes sense to color and plot them on a continuous scale. However, they do have some unique properties, particularly when we want to extract information such as the day of the week from a date, that require a unique data type. To create a date object in Python, we can use pandas’ to_datetime() function or the datetime module directly.\n\n# Create date objects using pandas\nchaucer_with_dates = (page_views\n    .query(\"doc_id == 'Geoffrey Chaucer'\")\n    .assign(date = lambda df: pd.to_datetime(df[['year', 'month', 'day']]))\n)\n\nchaucer_with_dates\n\n\n\n\n\n\n\n\ndoc_id\nyear\nmonth\nday\nviews\ndate\n\n\n\n\n60\nGeoffrey Chaucer\n2023\n8\n1\n2133\n2023-08-01\n\n\n61\nGeoffrey Chaucer\n2023\n8\n2\n1977\n2023-08-02\n\n\n62\nGeoffrey Chaucer\n2023\n8\n3\n1860\n2023-08-03\n\n\n63\nGeoffrey Chaucer\n2023\n8\n4\n1739\n2023-08-04\n\n\n64\nGeoffrey Chaucer\n2023\n8\n5\n1849\n2023-08-05\n\n\n65\nGeoffrey Chaucer\n2023\n8\n6\n1819\n2023-08-06\n\n\n66\nGeoffrey Chaucer\n2023\n8\n7\n1885\n2023-08-07\n\n\n67\nGeoffrey Chaucer\n2023\n8\n8\n1984\n2023-08-08\n\n\n68\nGeoffrey Chaucer\n2023\n8\n9\n1946\n2023-08-09\n\n\n69\nGeoffrey Chaucer\n2023\n8\n10\n1942\n2023-08-10\n\n\n70\nGeoffrey Chaucer\n2023\n8\n11\n1633\n2023-08-11\n\n\n71\nGeoffrey Chaucer\n2023\n8\n12\n1749\n2023-08-12\n\n\n72\nGeoffrey Chaucer\n2023\n8\n13\n1790\n2023-08-13\n\n\n73\nGeoffrey Chaucer\n2023\n8\n14\n1755\n2023-08-14\n\n\n74\nGeoffrey Chaucer\n2023\n8\n15\n1685\n2023-08-15\n\n\n75\nGeoffrey Chaucer\n2023\n8\n16\n1998\n2023-08-16\n\n\n76\nGeoffrey Chaucer\n2023\n8\n17\n1947\n2023-08-17\n\n\n77\nGeoffrey Chaucer\n2023\n8\n18\n1773\n2023-08-18\n\n\n78\nGeoffrey Chaucer\n2023\n8\n19\n1714\n2023-08-19\n\n\n79\nGeoffrey Chaucer\n2023\n8\n20\n1789\n2023-08-20\n\n\n80\nGeoffrey Chaucer\n2023\n8\n21\n1879\n2023-08-21\n\n\n81\nGeoffrey Chaucer\n2023\n8\n22\n2004\n2023-08-22\n\n\n82\nGeoffrey Chaucer\n2023\n8\n23\n1931\n2023-08-23\n\n\n83\nGeoffrey Chaucer\n2023\n8\n24\n1880\n2023-08-24\n\n\n84\nGeoffrey Chaucer\n2023\n8\n25\n1647\n2023-08-25\n\n\n85\nGeoffrey Chaucer\n2023\n8\n26\n1712\n2023-08-26\n\n\n86\nGeoffrey Chaucer\n2023\n8\n27\n1820\n2023-08-27\n\n\n87\nGeoffrey Chaucer\n2023\n8\n28\n1902\n2023-08-28\n\n\n88\nGeoffrey Chaucer\n2023\n8\n29\n1888\n2023-08-29\n\n\n89\nGeoffrey Chaucer\n2023\n8\n30\n1745\n2023-08-30\n\n\n90\nGeoffrey Chaucer\n2023\n8\n31\n1735\n2023-08-31\n\n\n91\nGeoffrey Chaucer\n2023\n9\n1\n1701\n2023-09-01\n\n\n92\nGeoffrey Chaucer\n2023\n9\n2\n1564\n2023-09-02\n\n\n93\nGeoffrey Chaucer\n2023\n9\n3\n1807\n2023-09-03\n\n\n94\nGeoffrey Chaucer\n2023\n9\n4\n1853\n2023-09-04\n\n\n95\nGeoffrey Chaucer\n2023\n9\n5\n4214\n2023-09-05\n\n\n96\nGeoffrey Chaucer\n2023\n9\n6\n2030\n2023-09-06\n\n\n97\nGeoffrey Chaucer\n2023\n9\n7\n2158\n2023-09-07\n\n\n98\nGeoffrey Chaucer\n2023\n9\n8\n2174\n2023-09-08\n\n\n99\nGeoffrey Chaucer\n2023\n9\n9\n1709\n2023-09-09\n\n\n100\nGeoffrey Chaucer\n2023\n9\n10\n2038\n2023-09-10\n\n\n101\nGeoffrey Chaucer\n2023\n9\n11\n2008\n2023-09-11\n\n\n102\nGeoffrey Chaucer\n2023\n9\n12\n2188\n2023-09-12\n\n\n103\nGeoffrey Chaucer\n2023\n9\n13\n2040\n2023-09-13\n\n\n104\nGeoffrey Chaucer\n2023\n9\n14\n2178\n2023-09-14\n\n\n105\nGeoffrey Chaucer\n2023\n9\n15\n1832\n2023-09-15\n\n\n106\nGeoffrey Chaucer\n2023\n9\n16\n2144\n2023-09-16\n\n\n107\nGeoffrey Chaucer\n2023\n9\n17\n1911\n2023-09-17\n\n\n108\nGeoffrey Chaucer\n2023\n9\n18\n2140\n2023-09-18\n\n\n109\nGeoffrey Chaucer\n2023\n9\n19\n2174\n2023-09-19\n\n\n110\nGeoffrey Chaucer\n2023\n9\n20\n2276\n2023-09-20\n\n\n111\nGeoffrey Chaucer\n2023\n9\n21\n2597\n2023-09-21\n\n\n112\nGeoffrey Chaucer\n2023\n9\n22\n1873\n2023-09-22\n\n\n113\nGeoffrey Chaucer\n2023\n9\n23\n1658\n2023-09-23\n\n\n114\nGeoffrey Chaucer\n2023\n9\n24\n1950\n2023-09-24\n\n\n115\nGeoffrey Chaucer\n2023\n9\n25\n2384\n2023-09-25\n\n\n116\nGeoffrey Chaucer\n2023\n9\n26\n2188\n2023-09-26\n\n\n117\nGeoffrey Chaucer\n2023\n9\n27\n1950\n2023-09-27\n\n\n118\nGeoffrey Chaucer\n2023\n9\n28\n2590\n2023-09-28\n\n\n119\nGeoffrey Chaucer\n2023\n9\n29\n2082\n2023-09-29\n\n\n\n\n\n\n\nNotice that the new column has a special data type: datetime64[ns]. If we build a visualization using a date object, plotnine is able to make helpful built-in choices about how to label the axis. For example, the following code will make a line plot that has nicely labeled values on the x-axis.\n\np = (ggplot(chaucer_with_dates, aes(x='date', y='views')) +\n     geom_line(color='red') +\n     labs(title=\"Page Views for Geoffrey Chaucer\",\n          x=\"Date\", y=\"Views\"))\np\n\n\n\n\n\n\n\n\nThe output shows that the algorithm decided to label the dates appropriately. We can manually change the frequency of the labels using scale_x_datetime() and setting the date_breaks option. For example, the code below will display one label for each week:\n\np = (ggplot(chaucer_with_dates, aes(x='date', y='views')) +\n     geom_line(color='red') +\n     scale_x_datetime(date_breaks='1 week', date_labels='%Y-%m-%d') +\n     theme(axis_text_x=element_text(angle=45, hjust=1)) +\n     labs(title=\"Page Views for Geoffrey Chaucer (Weekly Labels)\",\n          x=\"Date\", y=\"Views\"))\np\n\n\n\n\n\n\n\n\nOnce we have a date object, we can also extract useful information from it. For example, we can extract the weekday of the date using pandas’ datetime accessor. Here, we will compute the weekday and then calculate the average number of page views for each day of the week:\n\n# Extract weekday and compute average views by day of week\nweekday_analysis = (chaucer_with_dates\n    .assign(weekday = lambda df: df['date'].dt.day_name())\n    .groupby('weekday')['views']\n    .mean()\n    .reset_index()\n    .sort_values('views', ascending=False)\n)\n\nweekday_analysis\n\n\n\n\n\n\n\n\nweekday\nviews\n\n\n\n\n5\nTuesday\n2273.111111\n\n\n4\nThursday\n2098.555556\n\n\n6\nWednesday\n1988.111111\n\n\n1\nMonday\n1975.750000\n\n\n3\nSunday\n1865.500000\n\n\n0\nFriday\n1828.222222\n\n\n2\nSaturday\n1762.375000\n\n\n\n\n\n\n\nHere we see the average number of page views by day of the week. We can also use the date objects to filter the dataset. For example, we can filter the dataset to only include those dates after 15 January 2020, about two-thirds of the way through our dataset:\n\n# Filter by date\nfiltered_data = chaucer_with_dates.query(\"date &gt; '2020-01-15'\")\nfiltered_data\n\n\n\n\n\n\n\n\ndoc_id\nyear\nmonth\nday\nviews\ndate\n\n\n\n\n60\nGeoffrey Chaucer\n2023\n8\n1\n2133\n2023-08-01\n\n\n61\nGeoffrey Chaucer\n2023\n8\n2\n1977\n2023-08-02\n\n\n62\nGeoffrey Chaucer\n2023\n8\n3\n1860\n2023-08-03\n\n\n63\nGeoffrey Chaucer\n2023\n8\n4\n1739\n2023-08-04\n\n\n64\nGeoffrey Chaucer\n2023\n8\n5\n1849\n2023-08-05\n\n\n65\nGeoffrey Chaucer\n2023\n8\n6\n1819\n2023-08-06\n\n\n66\nGeoffrey Chaucer\n2023\n8\n7\n1885\n2023-08-07\n\n\n67\nGeoffrey Chaucer\n2023\n8\n8\n1984\n2023-08-08\n\n\n68\nGeoffrey Chaucer\n2023\n8\n9\n1946\n2023-08-09\n\n\n69\nGeoffrey Chaucer\n2023\n8\n10\n1942\n2023-08-10\n\n\n70\nGeoffrey Chaucer\n2023\n8\n11\n1633\n2023-08-11\n\n\n71\nGeoffrey Chaucer\n2023\n8\n12\n1749\n2023-08-12\n\n\n72\nGeoffrey Chaucer\n2023\n8\n13\n1790\n2023-08-13\n\n\n73\nGeoffrey Chaucer\n2023\n8\n14\n1755\n2023-08-14\n\n\n74\nGeoffrey Chaucer\n2023\n8\n15\n1685\n2023-08-15\n\n\n75\nGeoffrey Chaucer\n2023\n8\n16\n1998\n2023-08-16\n\n\n76\nGeoffrey Chaucer\n2023\n8\n17\n1947\n2023-08-17\n\n\n77\nGeoffrey Chaucer\n2023\n8\n18\n1773\n2023-08-18\n\n\n78\nGeoffrey Chaucer\n2023\n8\n19\n1714\n2023-08-19\n\n\n79\nGeoffrey Chaucer\n2023\n8\n20\n1789\n2023-08-20\n\n\n80\nGeoffrey Chaucer\n2023\n8\n21\n1879\n2023-08-21\n\n\n81\nGeoffrey Chaucer\n2023\n8\n22\n2004\n2023-08-22\n\n\n82\nGeoffrey Chaucer\n2023\n8\n23\n1931\n2023-08-23\n\n\n83\nGeoffrey Chaucer\n2023\n8\n24\n1880\n2023-08-24\n\n\n84\nGeoffrey Chaucer\n2023\n8\n25\n1647\n2023-08-25\n\n\n85\nGeoffrey Chaucer\n2023\n8\n26\n1712\n2023-08-26\n\n\n86\nGeoffrey Chaucer\n2023\n8\n27\n1820\n2023-08-27\n\n\n87\nGeoffrey Chaucer\n2023\n8\n28\n1902\n2023-08-28\n\n\n88\nGeoffrey Chaucer\n2023\n8\n29\n1888\n2023-08-29\n\n\n89\nGeoffrey Chaucer\n2023\n8\n30\n1745\n2023-08-30\n\n\n90\nGeoffrey Chaucer\n2023\n8\n31\n1735\n2023-08-31\n\n\n91\nGeoffrey Chaucer\n2023\n9\n1\n1701\n2023-09-01\n\n\n92\nGeoffrey Chaucer\n2023\n9\n2\n1564\n2023-09-02\n\n\n93\nGeoffrey Chaucer\n2023\n9\n3\n1807\n2023-09-03\n\n\n94\nGeoffrey Chaucer\n2023\n9\n4\n1853\n2023-09-04\n\n\n95\nGeoffrey Chaucer\n2023\n9\n5\n4214\n2023-09-05\n\n\n96\nGeoffrey Chaucer\n2023\n9\n6\n2030\n2023-09-06\n\n\n97\nGeoffrey Chaucer\n2023\n9\n7\n2158\n2023-09-07\n\n\n98\nGeoffrey Chaucer\n2023\n9\n8\n2174\n2023-09-08\n\n\n99\nGeoffrey Chaucer\n2023\n9\n9\n1709\n2023-09-09\n\n\n100\nGeoffrey Chaucer\n2023\n9\n10\n2038\n2023-09-10\n\n\n101\nGeoffrey Chaucer\n2023\n9\n11\n2008\n2023-09-11\n\n\n102\nGeoffrey Chaucer\n2023\n9\n12\n2188\n2023-09-12\n\n\n103\nGeoffrey Chaucer\n2023\n9\n13\n2040\n2023-09-13\n\n\n104\nGeoffrey Chaucer\n2023\n9\n14\n2178\n2023-09-14\n\n\n105\nGeoffrey Chaucer\n2023\n9\n15\n1832\n2023-09-15\n\n\n106\nGeoffrey Chaucer\n2023\n9\n16\n2144\n2023-09-16\n\n\n107\nGeoffrey Chaucer\n2023\n9\n17\n1911\n2023-09-17\n\n\n108\nGeoffrey Chaucer\n2023\n9\n18\n2140\n2023-09-18\n\n\n109\nGeoffrey Chaucer\n2023\n9\n19\n2174\n2023-09-19\n\n\n110\nGeoffrey Chaucer\n2023\n9\n20\n2276\n2023-09-20\n\n\n111\nGeoffrey Chaucer\n2023\n9\n21\n2597\n2023-09-21\n\n\n112\nGeoffrey Chaucer\n2023\n9\n22\n1873\n2023-09-22\n\n\n113\nGeoffrey Chaucer\n2023\n9\n23\n1658\n2023-09-23\n\n\n114\nGeoffrey Chaucer\n2023\n9\n24\n1950\n2023-09-24\n\n\n115\nGeoffrey Chaucer\n2023\n9\n25\n2384\n2023-09-25\n\n\n116\nGeoffrey Chaucer\n2023\n9\n26\n2188\n2023-09-26\n\n\n117\nGeoffrey Chaucer\n2023\n9\n27\n1950\n2023-09-27\n\n\n118\nGeoffrey Chaucer\n2023\n9\n28\n2590\n2023-09-28\n\n\n119\nGeoffrey Chaucer\n2023\n9\n29\n2082\n2023-09-29\n\n\n\n\n\n\n\nNote that we can use string representations of dates in filtering operations, and pandas will automatically convert them to the appropriate datetime objects.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#datetime-objects",
    "href": "08_temporal_data.html#datetime-objects",
    "title": "8  Temporal Data",
    "section": "8.4 Datetime Objects",
    "text": "8.4 Datetime Objects\nThe page_views dataset records the date of each observation. Sometimes we have data that describes the time of an event more specifically in terms of hours, minutes, and even possibly seconds. We will use the term datetime to describe an object that stores the precise time that an event occurs. The idea is that to describe the time that something happens we need to specify a date and a time. Later in the chapter, we will see an object that stores time without reference to a particular day. Whereas dates have a natural precision (a single day), we might desire to work with datetime objects of different levels of granularity. In some cases we might have just hours of the day and in others we might have access to records at the level of a millisecond such as data from radio and TV. In Python, internally all datetime objects are stored with nanosecond precision, but we can regard the precision as whatever granularity we have given in our data for all practical purposes.\nDatetime objects largely function the same as date objects. Let’s grab another dataset from Wikipedia that has precise timestamps. Below, we read in a dataset consisting of the last 500 edits made to each of the 75 British author pages in our collection.\n\npage_revisions = pd.read_csv(\"data/wiki_uk_page_revisions.csv\")\n# Convert datetime column to pandas datetime\npage_revisions['datetime'] = pd.to_datetime(page_revisions['datetime'])\n\npage_revisions\n\n\n\n\n\n\n\n\ndoc_id\nuser\ndatetime\nsize\ncomment\n\n\n\n\n0\nMarie de France\nYurikBot\n2006-07-06 23:24:54+00:00\n3061\nrobot Adding: [[it:Maria di Francia]]\n\n\n1\nMarie de France\nYurikBot\n2006-07-24 18:39:08+00:00\n3085\nrobot Adding: [[pt:Maria de França]]\n\n\n2\nMarie de France\nCcarroll\n2006-09-10 13:36:44+00:00\n3085\nNaN\n\n\n3\nMarie de France\n63.231.20.88\n2006-09-18 04:59:53+00:00\n3105\nNaN\n\n\n4\nMarie de France\nExplicitImplicity\n2006-09-23 22:27:20+00:00\n3104\ni believe it is stupid to translate one word, ...\n\n\n...\n...\n...\n...\n...\n...\n\n\n35465\nSeamus Heaney\nInternetArchiveBot\n2023-09-21 04:51:45+00:00\n85491\nRescuing 1 sources and tagging 0 as dead.) #IA...\n\n\n35466\nSeamus Heaney\n81.110.56.233\n2023-09-24 12:10:49+00:00\n85493\nNaN\n\n\n35467\nSeamus Heaney\n149.50.162.177\n2023-09-25 16:06:01+00:00\n85482\n/* Death */\n\n\n35468\nSeamus Heaney\n--WikiUser1234945--\n2023-09-25 16:06:11+00:00\n85493\nReverted edits by [[Special:Contributions/149....\n\n\n35469\nSeamus Heaney\nBD2412\n2023-09-27 02:39:46+00:00\n85493\n/* 1957–1969 */Clean up spacing errors around ...\n\n\n\n\n35470 rows × 5 columns\n\n\n\nNotice that each row has a record in the column datetime that provides a precise datetime object giving the second at which the page was modified. The data were stored using the ISO-8601 format (“YYYY-MM-DD HH:MM:SS”), which pandas can automatically parse.\nOur page_revisions dataset includes several pieces of information about each of the edits. We have a username for the person who made the edit (recall that anyone can edit a Wikipedia page), the size in bytes of the page after the edit was made, and a short comment describing what was done in the change. Looking at the page size over time shows when large additions and deletions were made to each record. The code below yields a temporal visualization:\n\n# Filter for two authors and create plot\nselected_authors = ['Geoffrey Chaucer', 'Emily Brontë']\nrevision_subset = page_revisions[page_revisions['doc_id'].isin(selected_authors)]\n\np = (ggplot(revision_subset, aes(x='datetime', y='size', color='doc_id')) +\n     geom_line() +\n     scale_color_manual(values=['red', 'blue']) +\n     labs(title=\"Wikipedia Page Size Over Time\",\n          x=\"Date\", y=\"Page Size (bytes)\",\n          color=\"Author\"))\np\n\n\n\n\n\n\n\n\nLooking at the plot, we can see that there are a few very large edits (both deletions and additions), likely consisting of large sections added and subtracted from the page. If we want to visualize when these large changes occurred, it would be useful to include a more granular set of labels on the x-axis. We can do this using scale_x_datetime() with custom formatting:\n\np = (ggplot(revision_subset, aes(x='datetime', y='size', color='doc_id')) +\n     geom_line() +\n     scale_color_manual(values=['red', 'blue']) +\n     scale_x_datetime(date_breaks='6 months', date_labels='%b %Y') +\n     theme(axis_text_x=element_text(angle=90, hjust=1)) +\n     labs(title=\"Wikipedia Page Size Over Time (Custom Labels)\",\n          x=\"Date\", y=\"Page Size (bytes)\",\n          color=\"Author\"))\np\n\n\n\n\n\n\n\n\nWe can also filter our dataset by a particular range of dates or times. This is useful to zoom into a specific region of our data to investigate patterns that may be otherwise lost. For example, if we wanted to see all of the page sizes for two authors from 2021 onward:\n\n# Filter by datetime\nrecent_revisions = (page_revisions\n    .query(\"doc_id in @selected_authors\")\n    .query(\"datetime &gt; @pd.Timestamp('2021-01-01', tz='UTC')\")\n)\n\nrecent_revisions\n\n\n\n\n\n\n\n\ndoc_id\nuser\ndatetime\nsize\ncomment\n\n\n\n\n650\nGeoffrey Chaucer\n86.31.15.58\n2021-01-18 09:42:33+00:00\n68291\n/* Origin */\n\n\n651\nGeoffrey Chaucer\n86.31.15.58\n2021-01-18 09:43:16+00:00\n68288\n/* Taco Bell */\n\n\n652\nGeoffrey Chaucer\n88.108.207.22\n2021-01-18 11:15:33+00:00\n68284\n/* Career */\n\n\n653\nGeoffrey Chaucer\nPahunkat\n2021-01-18 11:16:09+00:00\n68288\nRollback edit(s) by [[Special:Contributions/88...\n\n\n654\nGeoffrey Chaucer\n88.108.207.22\n2021-01-18 11:16:40+00:00\n68293\n/* Origin */\n\n\n...\n...\n...\n...\n...\n...\n\n\n22577\nEmily Brontë\nHeyElliott\n2023-09-16 20:05:37+00:00\n41598\n[[WP:LQ]]\n\n\n22578\nEmily Brontë\nQwerfjkl (bot)\n2023-09-21 16:34:46+00:00\n41586\nConverting Gutenberg author ID from name to nu...\n\n\n22579\nEmily Brontë\nFicaia\n2023-09-27 12:47:22+00:00\n41717\nNaN\n\n\n22580\nEmily Brontë\nKeith D\n2023-09-27 20:36:18+00:00\n41688\nRemove namespace | Replaced curly quotes with ...\n\n\n22581\nEmily Brontë\nKeith D\n2023-09-27 20:39:51+00:00\n41693\n/* Adulthood */ Cite fix\n\n\n\n\n506 rows × 5 columns\n\n\n\nNotice that the filter includes data from 2021, even though we use a strictly greater than condition. The reason for this is that '2021-01-01' is interpreted as the exact time corresponding to 1 January 2021 at 00:00. Any record that comes at any other time during the year of 2021 will be included in the filter.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#language-and-time-zones",
    "href": "08_temporal_data.html#language-and-time-zones",
    "title": "8  Temporal Data",
    "section": "8.5 Language and Time Zones",
    "text": "8.5 Language and Time Zones\nSo far, we have primarily worked with numeric summaries of the date and datetime objects. In the previous sections, notice that our example of working with the days of the week and the names of the months all had Python create automatically the names of these objects in English. Depending on our audience, it may be desirable to show plots using the names of weekdays and months in another language. This can be controlled by setting the system locale or using custom formatting. Below, for example, are the days of the weeks provided in different languages using pandas datetime functionality:\n\n# Extract various datetime components\ndatetime_analysis = (page_revisions\n    .head(10)\n    .assign(\n        weekday_en = lambda df: df['datetime'].dt.day_name(),\n        month_en = lambda df: df['datetime'].dt.month_name(),\n        hour = lambda df: df['datetime'].dt.hour,\n        date_only = lambda df: df['datetime'].dt.date\n    )\n    .loc[:, ['datetime', 'weekday_en', 'month_en', 'hour', 'date_only']]\n)\n\ndatetime_analysis\n\n\n\n\n\n\n\n\ndatetime\nweekday_en\nmonth_en\nhour\ndate_only\n\n\n\n\n0\n2006-07-06 23:24:54+00:00\nThursday\nJuly\n23\n2006-07-06\n\n\n1\n2006-07-24 18:39:08+00:00\nMonday\nJuly\n18\n2006-07-24\n\n\n2\n2006-09-10 13:36:44+00:00\nSunday\nSeptember\n13\n2006-09-10\n\n\n3\n2006-09-18 04:59:53+00:00\nMonday\nSeptember\n4\n2006-09-18\n\n\n4\n2006-09-23 22:27:20+00:00\nSaturday\nSeptember\n22\n2006-09-23\n\n\n5\n2006-09-23 22:31:03+00:00\nSaturday\nSeptember\n22\n2006-09-23\n\n\n6\n2006-10-03 18:18:09+00:00\nTuesday\nOctober\n18\n2006-10-03\n\n\n7\n2006-10-03 18:19:32+00:00\nTuesday\nOctober\n18\n2006-10-03\n\n\n8\n2006-10-10 18:48:26+00:00\nTuesday\nOctober\n18\n2006-10-10\n\n\n9\n2006-10-11 05:29:32+00:00\nWednesday\nOctober\n5\n2006-10-11\n\n\n\n\n\n\n\nFor non-English locales, you would typically set the system locale before running the analysis. However, since locale support varies by system, we’ll focus on the English representation here.\nAnother regional issue that arises when working with dates and times are time zones. While seemingly not too difficult a concept, getting time zones to work correctly with complex datasets can be incredibly complicated. A wide range of programming bugs have been attributed to all sorts of edge-cases surrounding the processing of time zones.\nAll times stored in pandas can be timezone-aware or timezone-naive. By default, times are stored as timezone-naive (no timezone information). The data can be localized to a specific timezone using pandas timezone functionality. All of the times recorded in the dataset page_revisions are given in UTC. This is not surprising; most technical sources with a global focus will use this convention.\nWe can convert between time zones using pandas and pytz. The tz_localize() method assigns a timezone to naive datetime data, while tz_convert() converts from one timezone to another. Let’s convert our UTC times to New York time:\n\nimport pytz\n\n# Convert UTC times to New York timezone\nrevision_with_tz = (page_revisions\n    .head(10)\n    .assign(\n        datetime_utc = lambda df: df['datetime'],  # Already UTC timezone-aware\n        datetime_nyc = lambda df: df['datetime'].dt.tz_convert('America/New_York')\n    )\n)\n\n# Extract hour information for comparison\nrevision_with_tz = revision_with_tz.assign(\n    hour_utc = lambda df: df['datetime_utc'].dt.hour,\n    hour_nyc = lambda df: df['datetime_nyc'].dt.hour\n)\n\nrevision_with_tz[['datetime_utc', 'datetime_nyc', 'hour_utc', 'hour_nyc']]\n\n\n\n\n\n\n\n\ndatetime_utc\ndatetime_nyc\nhour_utc\nhour_nyc\n\n\n\n\n0\n2006-07-06 23:24:54+00:00\n2006-07-06 19:24:54-04:00\n23\n19\n\n\n1\n2006-07-24 18:39:08+00:00\n2006-07-24 14:39:08-04:00\n18\n14\n\n\n2\n2006-09-10 13:36:44+00:00\n2006-09-10 09:36:44-04:00\n13\n9\n\n\n3\n2006-09-18 04:59:53+00:00\n2006-09-18 00:59:53-04:00\n4\n0\n\n\n4\n2006-09-23 22:27:20+00:00\n2006-09-23 18:27:20-04:00\n22\n18\n\n\n5\n2006-09-23 22:31:03+00:00\n2006-09-23 18:31:03-04:00\n22\n18\n\n\n6\n2006-10-03 18:18:09+00:00\n2006-10-03 14:18:09-04:00\n18\n14\n\n\n7\n2006-10-03 18:19:32+00:00\n2006-10-03 14:19:32-04:00\n18\n14\n\n\n8\n2006-10-10 18:48:26+00:00\n2006-10-10 14:48:26-04:00\n18\n14\n\n\n9\n2006-10-11 05:29:32+00:00\n2006-10-11 01:29:32-04:00\n5\n1\n\n\n\n\n\n\n\nWe can use the timezone information to display data in a useful way to a local audience. For example, the code below displays the frequency of updates as a function of the hour of the day in New York City:\n\n# Analyze edits by hour in New York timezone\nhourly_edits = (page_revisions\n    .assign(\n        datetime_nyc = lambda df: df['datetime'].dt.tz_convert('America/New_York'),\n        hour_nyc = lambda df: df['datetime'].dt.tz_convert('America/New_York').dt.hour\n    )\n    .groupby('hour_nyc')\n    .size()\n    .reset_index(name='count')\n)\n\np = (ggplot(hourly_edits, aes(x='hour_nyc', y='count')) +\n     geom_col() +\n     labs(title=\"Wikipedia Edits by Hour (New York Time)\",\n          x=\"Hour of Day\", y=\"Number of Edits\"))\np\n\n\n\n\n\n\n\n\nWhile certainly many editors are living in other English-speaking cities (London, Los Angeles, or Mumbai), it is generally easier for people to do the mental math for what times correspond with relative to their own time zone than relative to UTC.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#dates-and-datetimes",
    "href": "08_temporal_data.html#dates-and-datetimes",
    "title": "8  Temporal Data",
    "section": "8.6 Dates and Datetimes",
    "text": "8.6 Dates and Datetimes\nWe have shown above how to create date and datetime objects using pandas to_datetime() function. Also, we have seen how to extract the components from date and datetime objects with the pandas datetime accessor (.dt). There are a variety of other functions that help us create and manipulate these temporal objects. For example, pandas can parse string representations in common formats automatically. While we will not give an entire list of all the available functions in pandas datetime functionality, let’s look at a few of the most useful and representative examples for converting between different ways of representing information about time.\nThe page_revisions dataset has revisions recorded with the precision of a second. This is likely overly granular for many applications; it might be better to have the data in a format that is only at the level of an hour, for example. We can truncate any date or datetime object by using pandas’ .dt.floor() method along with a specific frequency. Setting the frequency to “H” (hour), for example, will remove all of the minutes and seconds from the time:\n\n# Truncate datetime to hours\nrevision_hourly = (page_revisions\n    .head(10)\n    .assign(\n        datetime_hour = lambda df: df['datetime'].dt.floor('H'),\n        datetime_day = lambda df: df['datetime'].dt.floor('D')\n    )\n    .loc[:, ['datetime', 'datetime_hour', 'datetime_day']]\n)\n\nrevision_hourly\n\n\n\n\n\n\n\n\ndatetime\ndatetime_hour\ndatetime_day\n\n\n\n\n0\n2006-07-06 23:24:54+00:00\n2006-07-06 23:00:00+00:00\n2006-07-06 00:00:00+00:00\n\n\n1\n2006-07-24 18:39:08+00:00\n2006-07-24 18:00:00+00:00\n2006-07-24 00:00:00+00:00\n\n\n2\n2006-09-10 13:36:44+00:00\n2006-09-10 13:00:00+00:00\n2006-09-10 00:00:00+00:00\n\n\n3\n2006-09-18 04:59:53+00:00\n2006-09-18 04:00:00+00:00\n2006-09-18 00:00:00+00:00\n\n\n4\n2006-09-23 22:27:20+00:00\n2006-09-23 22:00:00+00:00\n2006-09-23 00:00:00+00:00\n\n\n5\n2006-09-23 22:31:03+00:00\n2006-09-23 22:00:00+00:00\n2006-09-23 00:00:00+00:00\n\n\n6\n2006-10-03 18:18:09+00:00\n2006-10-03 18:00:00+00:00\n2006-10-03 00:00:00+00:00\n\n\n7\n2006-10-03 18:19:32+00:00\n2006-10-03 18:00:00+00:00\n2006-10-03 00:00:00+00:00\n\n\n8\n2006-10-10 18:48:26+00:00\n2006-10-10 18:00:00+00:00\n2006-10-10 00:00:00+00:00\n\n\n9\n2006-10-11 05:29:32+00:00\n2006-10-11 05:00:00+00:00\n2006-10-11 00:00:00+00:00\n\n\n\n\n\n\n\nThe benefit of using the .dt.floor() method is that we could then group, join, or summarize the data in a way that treats each value of datetime the same as long as they occur during the same hour. There is also a .dt.round() method for rounding the datetime object to the nearest desired unit. In the special case in which we want to extract just the date part of a datetime, we can use the .dt.date accessor. The code below illustrates this process, as well as showing how reducing the temporal granularity can be a useful first step before grouping and summarizing:\n\n# Count edits by date\ndaily_edit_counts = (page_revisions\n    .assign(date = lambda df: df['datetime'].dt.date)\n    .groupby('date')\n    .size()\n    .reset_index(name='count')\n    .sort_values('date', ascending=False)\n)\n\ndaily_edit_counts\n\n\n\n\n\n\n\n\ndate\ncount\n\n\n\n\n4830\n2023-09-30\n10\n\n\n4829\n2023-09-29\n11\n\n\n4828\n2023-09-28\n9\n\n\n4827\n2023-09-27\n14\n\n\n4826\n2023-09-26\n12\n\n\n...\n...\n...\n\n\n4\n2003-02-01\n1\n\n\n3\n2003-01-29\n2\n\n\n2\n2003-01-14\n1\n\n\n1\n2003-01-04\n1\n\n\n0\n2002-10-26\n1\n\n\n\n\n4831 rows × 2 columns\n\n\n\nAbove, we effectively remove the time component of the datetime object and treat the variable as having only a date element. Occasionally, we might want to do the opposite. That is, considering only the time component of a datetime object without worrying about the specific date. For example, we might want to summarize the number of edits that are made based on the time of the day. We can do this by extracting the time component:\n\n# Extract time component and aggregate by hour\nhourly_edit_pattern = (page_revisions\n    .assign(\n        hour = lambda df: df['datetime'].dt.floor('H').dt.time,\n        hour_numeric = lambda df: df['datetime'].dt.hour\n    )\n    .groupby('hour_numeric')\n    .size()\n    .reset_index(name='count')\n    .sort_values('hour_numeric')\n)\n\nhourly_edit_pattern\n\n\n\n\n\n\n\n\nhour_numeric\ncount\n\n\n\n\n0\n0\n1316\n\n\n1\n1\n1229\n\n\n2\n2\n1149\n\n\n3\n3\n1010\n\n\n4\n4\n982\n\n\n5\n5\n1001\n\n\n6\n6\n930\n\n\n7\n7\n974\n\n\n8\n8\n1145\n\n\n9\n9\n1406\n\n\n10\n10\n1502\n\n\n11\n11\n1479\n\n\n12\n12\n1439\n\n\n13\n13\n1796\n\n\n14\n14\n1856\n\n\n15\n15\n1938\n\n\n16\n16\n2003\n\n\n17\n17\n1836\n\n\n18\n18\n1983\n\n\n19\n19\n1837\n\n\n20\n20\n1838\n\n\n21\n21\n1797\n\n\n22\n22\n1675\n\n\n23\n23\n1349\n\n\n\n\n\n\n\nThis creates a variable that stores time without any date information, which is useful for analyzing patterns that repeat daily.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#window-functions",
    "href": "08_temporal_data.html#window-functions",
    "title": "8  Temporal Data",
    "section": "8.7 Window Functions",
    "text": "8.7 Window Functions\nAt the start of this chapter, we considered time series to be a sequence of events without too much focus on the specific dates and times. This viewpoint can be a useful construct when we want to look at changes over time. For example, we have the overall size of each Wikipedia page after an edit. A measurement that would be useful is the difference in page size made by an edit. To add a variable to a dataset, we usually use the assign() method or direct assignment, and that will again work here. However, in this case we need to reference values that come before or after a certain value. This requires the use of window functions.\nA window function transforms a variable in a dataset into a new variable with the same length in a way that takes into account the entire ordering of the data. Two examples of window functions that are useful when working with time series data are shift() with positive and negative values, which give access to rows preceding or following a row, respectively. Let’s apply this to our dataset of page revisions to get the previous and next values of the page size variable.\n\n# Apply window functions to get lagged and leading values\nrevision_with_lag = (page_revisions\n    .assign(\n        size_last = lambda df: df['size'].shift(1),  # lag\n        size_next = lambda df: df['size'].shift(-1)  # lead\n    )\n    .loc[:, ['doc_id', 'datetime', 'size', 'size_last', 'size_next']]\n)\n\nrevision_with_lag\n\n\n\n\n\n\n\n\ndoc_id\ndatetime\nsize\nsize_last\nsize_next\n\n\n\n\n0\nMarie de France\n2006-07-06 23:24:54+00:00\n3061\nNaN\n3085.0\n\n\n1\nMarie de France\n2006-07-24 18:39:08+00:00\n3085\n3061.0\n3085.0\n\n\n2\nMarie de France\n2006-09-10 13:36:44+00:00\n3085\n3085.0\n3105.0\n\n\n3\nMarie de France\n2006-09-18 04:59:53+00:00\n3105\n3085.0\n3104.0\n\n\n4\nMarie de France\n2006-09-23 22:27:20+00:00\n3104\n3105.0\n3132.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n35465\nSeamus Heaney\n2023-09-21 04:51:45+00:00\n85491\n85341.0\n85493.0\n\n\n35466\nSeamus Heaney\n2023-09-24 12:10:49+00:00\n85493\n85491.0\n85482.0\n\n\n35467\nSeamus Heaney\n2023-09-25 16:06:01+00:00\n85482\n85493.0\n85493.0\n\n\n35468\nSeamus Heaney\n2023-09-25 16:06:11+00:00\n85493\n85482.0\n85493.0\n\n\n35469\nSeamus Heaney\n2023-09-27 02:39:46+00:00\n85493\n85493.0\nNaN\n\n\n\n\n35470 rows × 5 columns\n\n\n\nNotice that the first value of size_last is missing because there is no last value for the first item in our data. Similarly, the variable size_next will have a missing value at the end of the dataset. As written above, the code incorrectly crosses the time points at the boundary of each page. That is, for the first row of the second page (Geoffrey Chaucer) it thinks that the size of the last page is the size of the final page of the Marie de France record. To fix this, we can group the dataset by item prior to applying the window functions. Window functions respect the grouping of the data:\n\n# Apply window functions within groups\nrevision_grouped_lag = (page_revisions\n    .sort_values(['doc_id', 'datetime'])  # Ensure proper ordering\n    .groupby('doc_id', group_keys=False)\n    .apply(lambda group: group.assign(\n        size_last = group['size'].shift(1),\n        size_next = group['size'].shift(-1)\n    ))\n    .reset_index(drop=True)\n)\n\n# Show the boundary between groups\nboundary_data = revision_grouped_lag.iloc[495:505]\nboundary_data[['doc_id', 'datetime', 'size', 'size_last', 'size_next']]\n\n\n\n\n\n\n\n\ndoc_id\ndatetime\nsize\nsize_last\nsize_next\n\n\n\n\n495\nA. A. Milne\n2023-08-05 20:59:54+00:00\n44019\n43990.0\n44038.0\n\n\n496\nA. A. Milne\n2023-08-18 09:26:21+00:00\n44038\n44019.0\n44044.0\n\n\n497\nA. A. Milne\n2023-08-21 22:05:31+00:00\n44044\n44038.0\n44045.0\n\n\n498\nA. A. Milne\n2023-08-21 22:07:46+00:00\n44045\n44044.0\n44077.0\n\n\n499\nA. A. Milne\n2023-08-31 20:41:54+00:00\n44077\n44045.0\nNaN\n\n\n500\nAlexander Pope\n2016-04-17 04:03:45+00:00\n31345\nNaN\n31531.0\n\n\n501\nAlexander Pope\n2016-05-05 22:17:54+00:00\n31531\n31345.0\n31532.0\n\n\n502\nAlexander Pope\n2016-05-05 22:24:29+00:00\n31532\n31531.0\n31585.0\n\n\n503\nAlexander Pope\n2016-05-07 19:05:49+00:00\n31585\n31532.0\n31614.0\n\n\n504\nAlexander Pope\n2016-05-09 17:22:21+00:00\n31614\n31585.0\n31585.0\n\n\n\n\n\n\n\nNotice that now, correctly, the dataset has a missing size_next for the final Marie de France record and a missing size_last for the first Geoffrey Chaucer record. Now, let’s use this to compute the change in the page sizes for each of the revisions:\n\n# Compute size differences\nrevision_with_diff = (page_revisions\n    .sort_values(['doc_id', 'datetime'])\n    .groupby('doc_id', group_keys=False)\n    .apply(lambda group: group.assign(\n        size_diff = group['size'] - group['size'].shift(1)\n    ))\n    .reset_index(drop=True)\n    .loc[:, ['doc_id', 'datetime', 'size', 'size_diff']]\n)\n\nrevision_with_diff\n\n\n\n\n\n\n\n\ndoc_id\ndatetime\nsize\nsize_diff\n\n\n\n\n0\nA. A. Milne\n2015-09-04 16:30:26+00:00\n30702\nNaN\n\n\n1\nA. A. Milne\n2015-09-22 14:51:08+00:00\n30400\n-302.0\n\n\n2\nA. A. Milne\n2015-11-07 10:23:31+00:00\n30389\n-11.0\n\n\n3\nA. A. Milne\n2015-11-08 00:48:23+00:00\n30397\n8.0\n\n\n4\nA. A. Milne\n2015-11-16 03:12:19+00:00\n30427\n30.0\n\n\n...\n...\n...\n...\n...\n\n\n35465\nWilliam Wordsworth\n2023-09-25 11:19:40+00:00\n41466\n0.0\n\n\n35466\nWilliam Wordsworth\n2023-09-27 15:10:27+00:00\n41464\n-2.0\n\n\n35467\nWilliam Wordsworth\n2023-09-27 15:11:44+00:00\n41466\n2.0\n\n\n35468\nWilliam Wordsworth\n2023-09-27 15:13:48+00:00\n41467\n1.0\n\n\n35469\nWilliam Wordsworth\n2023-09-27 15:18:10+00:00\n41466\n-1.0\n\n\n\n\n35470 rows × 4 columns\n\n\n\nIn the above output, we can see the changes in page sizes. If we wanted to find reversions in the dataset, we could apply the shift() function several times. As an alternative, we can also give a parameter to shift() to indicate that we want to go back (or forward) more than one row. Let’s put this together to indicate which commits seem to be a reversion (the page size exactly matches the page size from two commits prior) as well as the overall size of the reversion:\n\n# Identify reversions\nrevision_with_reversion = (page_revisions\n    .sort_values(['doc_id', 'datetime'])\n    .groupby('doc_id', group_keys=False)\n    .apply(lambda group: group.assign(\n        size_diff = group['size'] - group['size'].shift(1),\n        size_two_back = group['size'].shift(2),\n        is_reversion = group['size'] == group['size'].shift(2)\n    ))\n    .reset_index(drop=True)\n    .query('is_reversion == True')\n    .loc[:, ['doc_id', 'datetime', 'size_diff', 'is_reversion']]\n)\n\nrevision_with_reversion\n\n\n\n\n\n\n\n\ndoc_id\ndatetime\nsize_diff\nis_reversion\n\n\n\n\n7\nA. A. Milne\n2015-12-17 18:13:11+00:00\n-6.0\nTrue\n\n\n10\nA. A. Milne\n2016-01-07 18:55:26+00:00\n30407.0\nTrue\n\n\n24\nA. A. Milne\n2016-04-14 17:38:45+00:00\n36.0\nTrue\n\n\n26\nA. A. Milne\n2016-04-17 15:06:58+00:00\n-77.0\nTrue\n\n\n29\nA. A. Milne\n2016-04-27 20:04:19+00:00\n-1.0\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n35460\nWilliam Wordsworth\n2023-09-18 06:35:43+00:00\n-1.0\nTrue\n\n\n35463\nWilliam Wordsworth\n2023-09-24 14:54:33+00:00\n8.0\nTrue\n\n\n35465\nWilliam Wordsworth\n2023-09-25 11:19:40+00:00\n0.0\nTrue\n\n\n35467\nWilliam Wordsworth\n2023-09-27 15:11:44+00:00\n2.0\nTrue\n\n\n35469\nWilliam Wordsworth\n2023-09-27 15:18:10+00:00\n-1.0\nTrue\n\n\n\n\n4927 rows × 4 columns\n\n\n\nThese reversions can be studied to see the nature of the Wikipedia editing process. For example, how long do these reversions tend to take? Are certain pages more likely to undergo reversions? Do these take place during a certain time of the day? These are all questions that we should now be able to address using this dataset and the tools described above.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#range-joins",
    "href": "08_temporal_data.html#range-joins",
    "title": "8  Temporal Data",
    "section": "8.8 Range Joins",
    "text": "8.8 Range Joins\nWe will finish this chapter by looking at range joins, which allow for combining datasets based on inequalities between keys contained in two different datasets. Range joins are functionality that can greatly simplify some operations that arise when working with temporal data. Recall that all of the join functions that we saw in Chap. 5 work by finding a correspondence where keys from one dataset equal the values of keys in another dataset. In some cases it happens that we want to join two tables on inequalities rather than exact values.\nTake, for example, the metadata table for the 75 authors in our Wikipedia collection. Recall that this dataset contains the years that each author was born and the years each author died. What if we wanted to make a dataset by joining the metadata to itself, matching each author with other authors that would have been alive in overlapping years? We can do this using pandas’ merge() function with some custom logic:\n\n# Load metadata\nmeta = pd.read_csv(\"data/wiki_uk_meta.csv.gz\")\n\n# Create overlap pairs using cross join and filtering\n# This creates all combinations and then filters for overlaps\nmeta_cross = meta.assign(key=1).merge(meta.assign(key=1), on='key', suffixes=('', '2'))\n\noverlap_pairs = (meta_cross\n    .query('born &lt;= born2 and died &gt; born2')  # First author's life overlaps with second's birth\n    .loc[:, ['doc_id', 'doc_id2', 'born', 'died', 'born2', 'died2']]\n    .query('doc_id != doc_id2')  # Remove self-matches\n)\n\noverlap_pairs\n\n\n\n\n\n\n\n\ndoc_id\ndoc_id2\nborn\ndied\nborn2\ndied2\n\n\n\n\n79\nGeoffrey Chaucer\nMargery Kempe\n1343\n1400\n1373\n1438\n\n\n151\nJohn Gower\nGeoffrey Chaucer\n1330\n1408\n1343\n1400\n\n\n153\nJohn Gower\nWilliam Langland\n1330\n1408\n1332\n1386\n\n\n154\nJohn Gower\nMargery Kempe\n1330\n1408\n1373\n1438\n\n\n155\nJohn Gower\nThomas Malory\n1330\n1408\n1405\n1471\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n5541\nRex Warner\nSamuel Beckett\n1905\n1986\n1906\n1989\n\n\n5542\nRex Warner\nW. H. Auden\n1905\n1986\n1907\n1973\n\n\n5543\nRex Warner\nLouis MacNeice\n1905\n1986\n1907\n1963\n\n\n5545\nRex Warner\nStephen Spender\n1905\n1986\n1909\n1995\n\n\n5549\nRex Warner\nSeamus Heaney\n1905\n1986\n1939\n1939\n\n\n\n\n693 rows × 6 columns\n\n\n\nThe resulting dataset would make an interesting type of network, showing temporal overlap of authors in the dataset. In fact, it is such a nice example we could create a visualization by bringing together Chap. 7 on networks with our temporal data:\n\n# Create a simple network visualization of temporal overlaps\n# For demonstration, we'll create a subset and simple plot\noverlap_sample = overlap_pairs.head(50)  # Sample for clarity\n\n# Count overlaps per author\noverlap_counts = (overlap_pairs\n    .groupby('doc_id')\n    .size()\n    .reset_index(name='overlap_count')\n    .merge(meta, on='doc_id')\n    .sort_values('overlap_count', ascending=False)\n)\n\noverlap_counts\n\n\n\n\n\n\n\n\ndoc_id\noverlap_count\nborn\ndied\nera\ngender\nlink\nshort\n\n\n\n\n6\nBram Stoker\n22\n1847\n1912\nVictorian\nmale\nBram_Stoker\nStoker\n\n\n24\nGeorge Bernard Shaw\n21\n1856\n1950\nVictorian\nmale\nGeorge_Bernard_Shaw\nShaw\n\n\n67\nWilliam Blake\n20\n1757\n1827\nRomantic\nmale\nWilliam_Blake\nBlake\n\n\n41\nJoseph Conrad\n19\n1857\n1924\nTwentieth C\nmale\nJoseph_Conrad\nConrad\n\n\n2\nAlfred, Lord Tennyson\n18\n1809\n1892\nVictorian\nmale\nAlfred,_Lord_Tennyson\nTennyson\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1\nAlexander Pope\n2\n1688\n1744\nRestoration\nmale\nAlexander_Pope\nPope\n\n\n68\nWilliam Langland\n2\n1332\n1386\nEarly\nmale\nWilliam_Langland\nLangland\n\n\n60\nStephen Spender\n1\n1909\n1995\nTwentieth C\nmale\nStephen_Spender\nSpender\n\n\n46\nMargery Kempe\n1\n1373\n1438\nEarly\nfemale\nMargery_Kempe\nKempe\n\n\n23\nGeoffrey Chaucer\n1\n1343\n1400\nEarly\nmale\nGeoffrey_Chaucer\nChaucer\n\n\n\n\n71 rows × 8 columns\n\n\n\nThe output shows that certain authors have many temporal overlaps with others in our collection, which creates interesting network structures. Adding this chapter on temporal data allows us to explore time and date data. When combined with other chapters such as networks, we can further layer our analysis. Given the calls to add context when working with humanities data, exploring in multiple ways and combining methods offers a way to add nuance. Of course, drawing on other scholarship, archives, and primary sources will also shape our understandings, and provide further insights. No computational method sits in a vacuum. Yet, adding important avenues such as time and date to understand data through a temporal lens is one more way we can work with data in a way that the humanities call for.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#extensions",
    "href": "08_temporal_data.html#extensions",
    "title": "8  Temporal Data",
    "section": "8.9 Extensions",
    "text": "8.9 Extensions\nA large portion of the date and datetime operations in pandas have been introduced throughout this chapter. Pandas contains many more functions for converting strings in other formats into date and datetime objects, all of which can be found along with examples in the pandas documentation. The shift() window function shown above is a very useful tool for any data that has a fixed order, which can include temporal data but also sources such as text.\nAdditional window functions that operate over an arbitrary number of rows at once are provided by pandas’ .rolling() and .expanding() methods. These are excellent for computing moving averages, rolling statistics, and cumulative calculations over time series data.\nBeyond these programming tools, there is a wide array of specialized techniques for modeling time series data. Some useful Python libraries include:\n\nstatsmodels: Comprehensive statistical modeling including time series analysis\nscikit-learn: Machine learning algorithms that can be applied to time series\nprophet: Facebook’s time series forecasting library\npyflux: Bayesian time series modeling\narch: Econometric modeling for financial time series\n\nFor theoretical background, Shumway and Stoffer offer a relatively accessible introduction to time-series modeling [1]. Fumio Hayashi’s Econometrics provides more theoretical details [2]. The canonical and encyclopedic reference for temporal analysis is James Hamilton’s Time series analysis [3].\nThe Python ecosystem provides rich and growing support for temporal data analysis, with particularly strong capabilities in areas like financial time series, econometrics, and forecasting that continue to evolve rapidly.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "08_temporal_data.html#references",
    "href": "08_temporal_data.html#references",
    "title": "8  Temporal Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Shumway, R H and Stoffer, D S (2017 ). Time series analysis and its applications: With r examples. Springer\n\n\n[2] Hayashi, F (2011 ). Econometrics. Princeton University Press\n\n\n[3] Hamilton, J D (2020 ). Time Series Analysis. Princeton university press",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Temporal Data</span>"
    ]
  },
  {
    "objectID": "09_spatial_data.html",
    "href": "09_spatial_data.html",
    "title": "9  Spatial Data",
    "section": "",
    "text": "9.1 Introduction\nThere is a popular phrase thrown around by those working with spatial data claiming that “80% of data contains a spatial component”, likely dating to a version of this statement made by Carl Franklin and Paula Hane specifically regarding data contained in government databases [1]. While actually quantifying the “amount of data” with a spatial component is likely impossible, the premise that a majority of datasets contain some spatial information is a valid one. Consider a dataset containing a record for every item held in a particular public library. It may contain explicit geospatial data such as the address of the branch where each item is housed, but there is a substantial amount of implicit spatial data such as the location of first publication or the birthplaces of the authors. Other sources such as letters, newspapers, and photographs abound with geographical data. Given the preponderance of geospatial data in general, and its particular importance to work in the humanities, this chapter introduces methods for exploring and visualizing a number of spatial data types within Python [2] [3].",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "09_spatial_data.html#spatial-points",
    "href": "09_spatial_data.html#spatial-points",
    "title": "9  Spatial Data",
    "section": "9.2 Spatial Points",
    "text": "9.2 Spatial Points\nAs a starting point, we will begin by working again with the CBSA dataset that we used in the introductory chapters. A reminder that CBSA stands for community based statistical areas, which are regions defined by the U.S. Office of Management and Budget. They are characterized by social and economic ties rather than political boundaries. While we did not make use of it at the time, this dataset does include spatial information about each region given by the longitude and latitude coordinates at the center of each CBSA region. Let’s read the dataset into Python again, focusing on the spatial information contained in the fourth and fifth columns.\n\ncbsa = pd.read_csv(\"data/acs_cbsa.csv\")\ncbsa\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\n\n\n0\nNew York\n35620\nNE\n-74.101056\n40.768770\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\nMiddle Atlantic\n\n\n1\nLos Angeles\n31080\nW\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\nPacific\n\n\n2\nChicago\n16980\nNC\n-87.958820\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\nEast North Central\n\n\n3\nDallas\n19100\nS\n-96.970508\n32.849480\n7.543340\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\nWest South Central\n\n\n4\nHouston\n26420\nS\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\nWest South Central\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n929\nZapata\n49820\nS\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\nWest South Central\n\n\n930\nKetchikan\n28540\nO\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\nPacific\n\n\n931\nCraig\n18780\nW\n-108.207523\n40.618749\n0.013240\n1.077471\n44.5\n58583\n72.8\n485\n29.0\nMountain\n\n\n932\nVernon\n46900\nS\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\nWest South Central\n\n\n933\nLamesa\n29500\nS\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\nWest South Central\n\n\n\n\n934 rows × 13 columns\n\n\n\nWhat can we do with this spatial information? The longitude and latitude are stored as numbers. There is nothing stopping us from plotting them directly in a visualization. This would result in a plot that shows the spatial extent of the regions and could be roughly identified as having the same shape that we see in familiar maps of the United States. This approach is a nice starting point. If we were dealing with data from a smaller geographic region, as we will see in later sections, this kind of scatter plot can actually be a fine base to a spatial visualization. In the case of the United States, we can do better by turning this into a special kind of object specifically designed to store geospatial information. To do this, we make use of geopandas to create a GeoDataFrame from our regular DataFrame. We convert the longitude and latitude columns into Point geometries and set the coordinate reference system (CRS) to 4326, which indicates that these are “raw” longitude and latitude values.\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Create geometry from longitude and latitude\ngeometry = [Point(xy) for xy in zip(cbsa['lon'], cbsa['lat'])]\n\n# Create GeoDataFrame\ncbsa_geo = gpd.GeoDataFrame(cbsa, geometry=geometry, crs='EPSG:4326')\n\ncbsa_geo\n\n\n\n\n\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\ngeometry\n\n\n\n\n0\nNew York\n35620\nNE\n-74.101056\n40.768770\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\nMiddle Atlantic\nPOINT (-74.10106 40.76877)\n\n\n1\nLos Angeles\n31080\nW\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\nPacific\nPOINT (-118.14872 34.21941)\n\n\n2\nChicago\n16980\nNC\n-87.958820\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\nEast North Central\nPOINT (-87.95882 41.70061)\n\n\n3\nDallas\n19100\nS\n-96.970508\n32.849480\n7.543340\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\nWest South Central\nPOINT (-96.97051 32.84948)\n\n\n4\nHouston\n26420\nS\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\nWest South Central\nPOINT (-95.40157 29.78708)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n929\nZapata\n49820\nS\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\nWest South Central\nPOINT (-99.16853 27.00057)\n\n\n930\nKetchikan\n28540\nO\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\nPacific\nPOINT (-130.92722 55.58248)\n\n\n931\nCraig\n18780\nW\n-108.207523\n40.618749\n0.013240\n1.077471\n44.5\n58583\n72.8\n485\n29.0\nMountain\nPOINT (-108.20752 40.61875)\n\n\n932\nVernon\n46900\nS\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\nWest South Central\nPOINT (-99.24085 34.08061)\n\n\n933\nLamesa\n29500\nS\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\nWest South Central\nPOINT (-101.94764 32.74249)\n\n\n\n\n934 rows × 14 columns\n\n\n\nThe output object cbsa_geo is now a special kind of DataFrame called a GeoDataFrame that includes more specific information about the spatial data attached to each observation in addition to the standard information that we are familiar with in a tabular dataset. Printing the cbsa_geo data shows how this object differs from other DataFrames we have used in this book. There is a special column called geometry that has the spatial information attached to it.\nOne of the benefits of the spatially enhanced version of a dataset is that it allows us to create spatial plots using geopandas’ built-in plotting functionality or using plotnine with spatial awareness. We can create a basic spatial plot using geopandas’ .plot() method:\n\n# Basic spatial plot using geopandas\nfig, ax = plt.subplots(figsize=(12, 8))\ncbsa_geo.plot(ax=ax, markersize=5, alpha=0.7)\nax.set_title(\"Centers of CBSA Regions\")\nplt.show()\n\n\n\n\n\n\n\n\nThe output shows the spatial distribution of CBSA centers across the United States. As we have seen with other plots, such as when working with temporal data in Chap. 8, geopandas automatically handles the coordinate system and creates appropriate axis labels. One somewhat unique thing about spatial plots is that we often don’t need to explicitly set x and y coordinates because they are inferred from the spatial information. However, we can still modify other aesthetic properties such as changing the color and size of the points.\nOne important operation that we can perform on spatial data before plotting it is to change the projection of the points. Projections create a better representation of the curved earth on a flat plot based on the region of the world that we are looking at. To change the projection, we can use the .to_crs() method. The method takes one argument, a CRS identifier giving the projection. Above, we already saw the 4326 code to indicate that our input projection was in degrees longitude and latitude.\nA powerful aspect of spatial analysis is working with coordinate reference systems. Each CRS represents points on the earth in terms of two numbers. This allows us to adjust our projection such as selecting a Mercator or Albers projection. A better projection when working with the entire United States is the CRS code 5069, a Conus Albers centered on the continental United States. We can plot the data using this projection:\n\n# Transform to better projection for US\ncbsa_projected = cbsa_geo.to_crs('EPSG:5069')\n\nfig, ax = plt.subplots(figsize=(12, 8))\ncbsa_projected.plot(ax=ax, markersize=5, alpha=0.7)\nax.set_title(\"CBSA Centers (Albers Projection)\")\nax.set_axis_off()  # Remove axis for cleaner look\nplt.show()\n\n\n\n\n\n\n\n\nThe output shows the spatial points plot using the updated CRS code. Notice that the lines of longitude and latitude are no longer straight when we overlay coordinate grids. They curve, much like they would if we were looking at a round globe and spun it around to center our vision on the United States. In order to find a good CRS code for a spatial dataset, we need to look up the CRS codes in an index. One such free service can be found at epsg.io. Just search the broad political terms in the vicinity of the points to find recommended options. Usually searching for the country of our data will suffice. For countries with larger geographic scope such the United States, Canada, Russia, or China, we may need to narrow down our search to a city or region.\nWe can also add text labels to our spatial plots. Let’s add labels for the most populous CBSA regions. We’ll filter to only show regions with at least one million people and focus on the continental United States:\n\n# Filter for large CBSAs in continental US\nlarge_cbsas = (cbsa_projected\n    .query(\"quad != 'O' and pop &gt;= 1\"))\n\nfig, ax = plt.subplots(figsize=(15, 10))\nlarge_cbsas.plot(ax=ax, markersize=20, alpha=0.7, color='red')\n\n# Add labels with some offset to avoid overlap\nfor idx, row in large_cbsas.iterrows():\n    ax.annotate(row['name'], \n                xy=(row.geometry.x, row.geometry.y),\n                xytext=(5, 5), textcoords='offset points',\n                fontsize=6, ha='left')\n\nax.set_title(\"Major CBSA Centers in Continental United States\")\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nThe output shows the points and labels for the largest CBSA regions, giving us a clear view of major population centers across the continental United States.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "09_spatial_data.html#polygons",
    "href": "09_spatial_data.html#polygons",
    "title": "9  Spatial Data",
    "section": "9.3 Polygons",
    "text": "9.3 Polygons\nPolygons are the way that we represent areas on a map. Spatial data can associate regions (i.e. polygons) with each row rather than a single point. It is less likely that we will create this type of spatial data directly. Instead, we usually read polygon data directly from a file that is already designed to store spatial information. The file type we will use is called GeoJSON. Geopandas can read GeoJSON files (and many other geospatial formats like ESRI shapefiles) using the read_file() function. Below, let’s read in a dataset with polygons describing the shape of each state in the United States.\n\n# Read state polygons from GeoJSON\nstate = gpd.read_file(\"data/geo_state.geojson\")\nstate\n\n\n\n\n\n\n\n\nname\nabb\nfips\ngeometry\n\n\n\n\n0\nMaine\nME\n23\nMULTIPOLYGON (((-70.70382 43.05983, -70.8268 4...\n\n\n1\nNew Hampshire\nNH\n33\nMULTIPOLYGON (((-71.50109 45.01338, -71.40564 ...\n\n\n2\nDelaware\nDE\n10\nMULTIPOLYGON (((-75.7886 39.7222, -75.61724 39...\n\n\n3\nSouth Carolina\nSC\n45\nMULTIPOLYGON (((-83.10861 35.00066, -82.46009 ...\n\n\n4\nNebraska\nNE\n31\nMULTIPOLYGON (((-104.05313 43.00059, -102.7921...\n\n\n5\nWashington\nWA\n53\nMULTIPOLYGON (((-117.03235 48.99919, -117.0411...\n\n\n6\nNew Mexico\nNM\n35\nMULTIPOLYGON (((-109.04522 36.99908, -108.2493...\n\n\n7\nSouth Dakota\nSD\n46\nMULTIPOLYGON (((-104.0577 44.99743, -104.03969...\n\n\n8\nTexas\nTX\n48\nMULTIPOLYGON (((-103.00243 36.5004, -102.25045...\n\n\n9\nCalifornia\nCA\n06\nMULTIPOLYGON (((-124.2116 41.99846, -123.34756...\n\n\n10\nKentucky\nKY\n21\nMULTIPOLYGON (((-89.13292 36.98206, -89.18251 ...\n\n\n11\nOhio\nOH\n39\nMULTIPOLYGON (((-84.80608 41.69609, -83.45383 ...\n\n\n12\nAlabama\nAL\n01\nMULTIPOLYGON (((-88.20006 34.99563, -88.20296 ...\n\n\n13\nGeorgia\nGA\n13\nMULTIPOLYGON (((-85.60517 34.98468, -84.32187 ...\n\n\n14\nWisconsin\nWI\n55\nMULTIPOLYGON (((-92.01529 46.70647, -91.82003 ...\n\n\n15\nArkansas\nAR\n05\nMULTIPOLYGON (((-94.61792 36.49941, -93.12597 ...\n\n\n16\nOregon\nOR\n41\nMULTIPOLYGON (((-123.54766 46.25911, -123.4308...\n\n\n17\nPennsylvania\nPA\n42\nMULTIPOLYGON (((-80.51942 41.97752, -80.18808 ...\n\n\n18\nMississippi\nMS\n28\nMULTIPOLYGON (((-91.16607 33.00411, -91.08759 ...\n\n\n19\nColorado\nCO\n08\nMULTIPOLYGON (((-109.05008 41.00066, -108.2506...\n\n\n20\nUtah\nUT\n49\nMULTIPOLYGON (((-114.04172 41.99372, -113.8932...\n\n\n21\nOklahoma\nOK\n40\nMULTIPOLYGON (((-103.0022 37.0001, -102.04224 ...\n\n\n22\nTennessee\nTN\n47\nMULTIPOLYGON (((-89.7331 36.00061, -89.594 36....\n\n\n23\nWest Virginia\nWV\n54\nMULTIPOLYGON (((-82.59367 38.42181, -82.52958 ...\n\n\n24\nNew York\nNY\n36\nMULTIPOLYGON (((-79.76195 42.26986, -79.42912 ...\n\n\n25\nIndiana\nIN\n18\nMULTIPOLYGON (((-87.52404 41.70833, -87.42344 ...\n\n\n26\nKansas\nKS\n20\nMULTIPOLYGON (((-102.05174 40.00308, -100.4687...\n\n\n27\nNevada\nNV\n32\nMULTIPOLYGON (((-119.99917 41.99454, -118.6964...\n\n\n28\nIllinois\nIL\n17\nMULTIPOLYGON (((-91.41942 40.37826, -91.37575 ...\n\n\n29\nVermont\nVT\n50\nMULTIPOLYGON (((-73.34312 45.01084, -72.84563 ...\n\n\n30\nConnecticut\nCT\n09\nMULTIPOLYGON (((-73.48731 42.04964, -73.05329 ...\n\n\n31\nMontana\nMT\n30\nMULTIPOLYGON (((-116.04909 49.00085, -115.1298...\n\n\n32\nMinnesota\nMN\n27\nMULTIPOLYGON (((-97.22872 49.00056, -96.40541 ...\n\n\n33\nMaryland\nMD\n24\nMULTIPOLYGON (((-79.47666 39.72108, -77.53488 ...\n\n\n34\nHawaii\nHI\n15\nMULTIPOLYGON (((-156.05722 19.74254, -155.9403...\n\n\n35\nArizona\nAZ\n04\nMULTIPOLYGON (((-114.71963 32.71876, -114.5390...\n\n\n36\nRhode Island\nRI\n44\nMULTIPOLYGON (((-71.19564 41.67509, -71.13289 ...\n\n\n37\nMissouri\nMO\n29\nMULTIPOLYGON (((-95.76564 40.58521, -94.48928 ...\n\n\n38\nNorth Carolina\nNC\n37\nMULTIPOLYGON (((-83.10023 35.77474, -82.99205 ...\n\n\n39\nVirginia\nVA\n51\nMULTIPOLYGON (((-81.9683 37.5378, -81.91668 37...\n\n\n40\nWyoming\nWY\n56\nMULTIPOLYGON (((-104.0577 44.99743, -104.05313...\n\n\n41\nLouisiana\nLA\n22\nMULTIPOLYGON (((-94.04296 33.01922, -92.22282 ...\n\n\n42\nMichigan\nMI\n26\nMULTIPOLYGON (((-86.82483 41.76024, -86.61944 ...\n\n\n43\nMassachusetts\nMA\n25\nMULTIPOLYGON (((-73.26496 42.74594, -72.45852 ...\n\n\n44\nIdaho\nID\n16\nMULTIPOLYGON (((-116.91599 45.99541, -116.9595...\n\n\n45\nFlorida\nFL\n12\nMULTIPOLYGON (((-85.0025 31.00068, -84.93696 3...\n\n\n46\nAlaska\nAK\n02\nMULTIPOLYGON (((-168.12893 65.65574, -167.9798...\n\n\n47\nNew Jersey\nNJ\n34\nMULTIPOLYGON (((-75.50974 39.68611, -75.41506 ...\n\n\n48\nNorth Dakota\nND\n38\nMULTIPOLYGON (((-104.04869 48.99959, -102.2180...\n\n\n49\nIowa\nIA\n19\nMULTIPOLYGON (((-96.45326 43.50039, -95.51477 ...\n\n\n\n\n\n\n\nIn the output above, we can see that this object already has the spatial information attached to it. Unlike our previous dataset, the geometry type here consists of polygons and multipolygons, to indicate that each row is associated with a region rather than an individual point. The prefix “multi” is used to indicate that some of our observations may be associated with multiple separate polygons. For example, Hawaii requires at least one polygon for each of the islands that make up the state. As with spatial points, we can create a spatial plot of the regions using geopandas plotting methods.\nAs with the spatial points, it will be helpful to transform our data before plotting it. In fact, it is often much more clear with polygons how distorted the default projection makes everything. Below is the code to produce a plot of the states as regions:\n\n# Transform to better projection and plot\nstate_projected = state.to_crs('EPSG:5069')\n\nfig, ax = plt.subplots(figsize=(15, 10))\nstate_projected.plot(ax=ax, edgecolor='black', facecolor='lightblue', alpha=0.7)\nax.set_title(\"United States State Boundaries (Albers Projection)\")\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also add labels to show state abbreviations at the center of each polygon:\n\n# Plot states with labels, excluding Alaska and Hawaii for cleaner display\ncontinental_states = state_projected[~state_projected['abb'].isin(['AK', 'HI'])]\n\nfig, ax = plt.subplots(figsize=(15, 10))\ncontinental_states.plot(ax=ax, edgecolor='black', facecolor='lightblue', alpha=0.7)\n\n# Add state abbreviations at polygon centroids\nfor idx, row in continental_states.iterrows():\n    centroid = row.geometry.centroid\n    ax.annotate(row['abb'], \n                xy=(centroid.x, centroid.y), \n                ha='center', va='center',\n                fontsize=8, fontweight='bold')\n\nax.set_title(\"Continental United States with State Abbreviations\")\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nAs we have been calling them throughout this text, the CBSA regions are, in fact, regions. The United States Census Bureau provides shape files that show the actual regions defined for each CBSA. Let’s read this data and see how we can visualize the regions themselves:\n\n# Read CBSA regions\ncbsa_reg = gpd.read_file(\"data/acs_cbsa_geo.geojson\")\n\n# Merge with CBSA data to get names and other attributes\ncbsa_data = pd.read_csv(\"data/acs_cbsa.csv\")\ncbsa_reg = cbsa_reg.merge(cbsa_data[['geoid', 'name']], on='geoid', how='left')\n\ncbsa_reg\n\n\n\n\n\n\n\n\ngeoid\nquad\npop\ngeometry\nname\n\n\n\n\n0\n35620.0\nNE\n20.011812\nMULTIPOLYGON (((-72.03683 41.24984, -72.03496 ...\nNew York\n\n\n1\n31080.0\nW\n13.202558\nMULTIPOLYGON (((-118.60442 33.47855, -118.5987...\nLos Angeles\n\n\n2\n16980.0\nNC\n9.607711\nMULTIPOLYGON (((-88.94215 42.06505, -88.93894 ...\nChicago\n\n\n3\n19100.0\nS\n7.543340\nMULTIPOLYGON (((-98.0656 32.59502, -98.06486 3...\nDallas\n\n\n4\n26420.0\nS\n7.048954\nMULTIPOLYGON (((-94.7183 29.72886, -94.71721 2...\nHouston\n\n\n...\n...\n...\n...\n...\n...\n\n\n934\n49820.0\nS\n0.013945\nMULTIPOLYGON (((-99.4538 27.26506, -99.42542 2...\nZapata\n\n\n935\n28540.0\nO\n0.013939\nMULTIPOLYGON (((-130.98311 55.36598, -130.9809...\nKetchikan\n\n\n936\n18780.0\nW\n0.013240\nMULTIPOLYGON (((-109.05095 40.44437, -109.0503...\nCraig\n\n\n937\n46900.0\nS\n0.012887\nMULTIPOLYGON (((-99.47514 33.90105, -99.47522 ...\nVernon\n\n\n938\n29500.0\nS\n0.012371\nMULTIPOLYGON (((-102.20852 32.95896, -102.1989...\nLamesa\n\n\n\n\n939 rows × 5 columns\n\n\n\nThe structure of this dataset is similar to the dataset of state regions, with a geometry column containing the region shapes and metadata indicating that these are polygons and multipolygons. A common technique with spatial information is to encode a variable of interest using the color of each polygon. Since both the x- and y-coordinates are used to represent where a region is, color becomes the aesthetic that holds the metric of interest that would usually go on one of the primary axes. For example, consider trying to show the population of each of the CBSA regions. We can do this by setting the color based on the population variable. We’ll also add the state boundaries as a reference:\n\n# Filter to continental US and transform projections\ncbsa_continental = cbsa_reg[cbsa_reg['quad'] != 'O'].to_crs('EPSG:5069')\nstates_continental = state_projected[~state_projected['abb'].isin(['AK', 'HI'])]\n\nfig, ax = plt.subplots(figsize=(15, 10))\n\n# Plot state boundaries first (as background)\nstates_continental.plot(ax=ax, facecolor='white', edgecolor='gray', linewidth=0.5)\n\n# Plot CBSA regions colored by population\ncbsa_continental.plot(ax=ax, column='pop', cmap='Blues', \n                     edgecolor='none', alpha=0.8, legend=True)\n\nax.set_title(\"CBSA Regions Colored by Population\")\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nThe spatial part of the plot works quite well, showing the coverage of the CBSA regions over the country. It is possible to pick out the largest regions on the map corresponding to New York, Los Angeles, and Chicago. However, the default color scale is fairly difficult to interpret because the population values are highly skewed. We can improve this by using a logarithmic scale and a better color scheme:\n\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(15, 10))\n\n# Plot state boundaries\nstates_continental.plot(ax=ax, facecolor='white', edgecolor='gray', linewidth=0.5)\n\n# Plot CBSA regions with log scale and better colormap\ncbsa_continental_copy = cbsa_continental.copy()\ncbsa_continental_copy['log_pop'] = np.log10(cbsa_continental_copy['pop'])\n\nim = cbsa_continental_copy.plot(ax=ax, column='log_pop', cmap='Spectral_r', \n                               edgecolor='none', alpha=0.8, legend=True,\n                               legend_kwds={'label': 'Population (log10 scale)'})\n\nax.set_title(\"CBSA Regions Colored by Population (Log Scale)\")\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nThe new visualization of the population of each CBSA region is much nicer to look at and it is much easier to see all of the core population centers in the country as a continuum from the largest to the smallest. This kind of spatial plot is often called a choropleth map. These are frequently used by popular media to illustrate spatial patterns, particularly during events such as elections. Depending on the application, it might have made more sense to plot the figure here using the density of each region rather than its overall population. This requires knowing the area of each region, from which we can define the population density. A running (sort of) joke in the spatial analysis community is that most maps claiming to reveal a different spatial feature are actually just population maps. We want to be cognizant of this. In the next section we will see how to derive metrics such as area from a spatial dataset.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "09_spatial_data.html#spatial-metrics",
    "href": "09_spatial_data.html#spatial-metrics",
    "title": "9  Spatial Data",
    "section": "9.4 Spatial Metrics",
    "text": "9.4 Spatial Metrics\nNow that we have seen how to read in spatial polygon data and plot it, we can begin to show how we can analyze the spatial data beyond visualizations. Geopandas provides a number of methods that we can apply to GeoDataFrames to compute summary information about each of the geometries. For example, the .area property will return the total area of each of the polygons associated with every row of a spatial dataset. When working with a projected coordinate system (like our Albers projection), the area will be in the units of that projection (typically square meters). To convert to square kilometers, we can divide by one million. Let’s compute the area of each CBSA region in square kilometers and then arrange in ascending order to see the smallest regions by area:\n\n# Compute areas (ensuring we're in a projected CRS for accurate area calculation)\ncbsa_with_area = cbsa_continental.copy()\ncbsa_with_area['area_km2'] = cbsa_with_area.geometry.area / 1e6\n\n# Show smallest areas\nsmallest_areas = (cbsa_with_area\n    .sort_values('area_km2')\n    .loc[:, ['name', 'area_km2', 'pop']]\n    .head(10)\n)\n\nsmallest_areas\n\n\n\n\n\n\n\n\nname\narea_km2\npop\n\n\n\n\n915\nLos Alamos\n282.833496\n0.019169\n\n\n909\nVineyard Haven\n306.672129\n0.020277\n\n\n550\nCarson City\n406.922179\n0.057957\n\n\n847\nToccoa\n476.393809\n0.026641\n\n\n872\nScottsburg\n499.172628\n0.024290\n\n\n427\nOak Harbor\n549.398746\n0.085938\n\n\n876\nConnersville\n557.136473\n0.023393\n\n\n141\nTrenton\n592.734769\n0.384951\n\n\n924\nMaysville\n638.328765\n0.017103\n\n\n922\nFitzgerald\n657.561503\n0.017237\n\n\n\n\n\n\n\nAnother useful metric that we can compute from spatial polygon objects are the centroids, points at the geographic center of each region. It can be useful if we want to treat the polygon as a single point or add a label such as a placename in the center of the region. Computing this is straightforward using the .centroid property:\n\n# Compute centroids\ncentroids = cbsa_continental.geometry.centroid\n\n# Extract x, y coordinates\ncbsa_with_centroids = cbsa_continental.copy()\ncbsa_with_centroids['centroid_x'] = centroids.x\ncbsa_with_centroids['centroid_y'] = centroids.y\n\ncbsa_with_centroids[['name', 'centroid_x', 'centroid_y']]\n\n\n\n\n\n\n\n\nname\ncentroid_x\ncentroid_y\n\n\n\n\n0\nNew York\n1.817142e+06\n2.183304e+06\n\n\n1\nLos Angeles\n-2.006862e+06\n1.474388e+06\n\n\n2\nChicago\n6.635662e+05\n2.105417e+06\n\n\n3\nDallas\n-9.027537e+04\n1.086927e+06\n\n\n4\nHouston\n5.784893e+04\n7.459226e+05\n\n\n...\n...\n...\n...\n\n\n933\nPecos\n-7.255826e+05\n9.575672e+05\n\n\n934\nZapata\n-3.163617e+05\n4.429799e+05\n\n\n936\nCraig\n-1.021286e+06\n2.021907e+06\n\n\n937\nVernon\n-2.967750e+05\n1.228991e+06\n\n\n938\nLamesa\n-5.537604e+05\n1.091841e+06\n\n\n\n\n919 rows × 3 columns\n\n\n\nThese are the exact centroids that were pre-populated in the cbsa dataset that we started with at the beginning of the chapter. In fact, we computed those centroids when building the dataset for the book directly from the polygons provided by the United States Census Bureau. Centroids are very helpful for many kinds of analysis. If the regions are fairly small in area compared to the total area of analysis, it may be easier and more straightforward to treat each observation as a point rather than as a complex region.\nIn addition to points and polygons, it is also possible to have spatial data that represents spatial lines. These can define, for example, roads, metro lines, railways, rivers, or the path of a moving object such as a hurricane. As an example, let’s read in a dataset of roads from New York City:\n\n# Read road network data\nroads = gpd.read_file(\"data/geo_ny_roads.geojson\")\nroads\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nWooster St Exd\nLINESTRING (-73.99712 40.72893, -73.99657 40.7...\n\n\n1\nW 158th St Exn\nLINESTRING (-73.9485 40.83641, -73.94879 40.83...\n\n\n2\nStanton St Exn\nLINESTRING (-73.97942 40.71867, -73.98022 40.7...\n\n\n3\nStaten St Exn\nLINESTRING (-73.98193 40.71946, -73.98122 40.7...\n\n\n4\nFirst Ave Lp\nLINESTRING (-73.98071 40.73381, -73.98002 40.7...\n\n\n...\n...\n...\n\n\n2233\nNone\nLINESTRING (-73.93644 40.818, -73.9368 40.8174...\n\n\n2234\nNone\nLINESTRING (-73.96461 40.79422, -73.96445 40.7...\n\n\n2235\nNone\nLINESTRING (-73.96248 40.76637, -73.96294 40.7...\n\n\n2236\nNone\nLINESTRING (-73.95422 40.82021, -73.9543 40.82...\n\n\n2237\nNone\nLINESTRING (-73.97806 40.77842, -73.97782 40.7...\n\n\n\n\n2238 rows × 2 columns\n\n\n\nOne useful application of spatial lines is to add them on top of other spatial data visualizations to better understand where different regions are located. This is particularly useful when looking at regions within a city where it can be hard to understand how to match the plot up with our understanding of the city’s geography without lines or other markers to orient ourselves. We can also compute metrics associated with lines. For example, the .length property functions similarly to the .area property to give the length of each line. Let’s apply it to find the longest streets in New York City, grouping together parts of streets that have the same name:\n\n# Compute road lengths and find longest streets\nroads_with_length = roads.copy()\nroads_with_length['length'] = roads_with_length.geometry.length\n\n# Group by name and sum lengths\nlongest_streets = (roads_with_length\n    .dropna(subset=['name'])  # Remove unnamed roads\n    .groupby('name')['length']\n    .sum()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\nlongest_streets\n\nname\nState Rte 9a         0.405296\nF D R Dr             0.353201\nHenry Hudson Pkwy    0.236432\nBroadway             0.219867\nRiverside Dr         0.162182\nHarlem River Dr      0.144414\nAmsterdam Ave        0.106750\n5th Ave              0.104180\n1st Ave              0.103039\n2nd Ave              0.101780\nName: length, dtype: float64\n\n\nFor those familiar with the geography of New York City, these longest streets roughly match the names of the main streets that run the length of Manhattan. There are several other metrics that can be used to manipulate spatial objects. The main goal of most of these other ones are to find distances and overlaps between pairs of geometries. We will cover these as a group in the next section.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "09_spatial_data.html#spatial-joins",
    "href": "09_spatial_data.html#spatial-joins",
    "title": "9  Spatial Data",
    "section": "9.5 Spatial Joins",
    "text": "9.5 Spatial Joins\nOne of the most common operations that we can perform on spatial data is to combine information between two different GeoDataFrames. For example, we might want to associate the points in one dataset with the polygons that they are contained within another dataset. Or we might want to filter one set of polygons based on those that intersect another set of polygons. For example, let’s say we have a dataset of historical battles or birth places, we can then associate them with modern political boundaries. Self-joins are also common with spatial data, such as in applications that need to find the distances between all pairs of points within a dataset of spatial points.\nBefore looking at joining spatial datasets by the spatial information, it will be useful to see what happens if we do a traditional table join with spatial information. Both the cbsa and cbsa_geo datasets contain a column called geoid that can be used to combine them together using an ordinary merge(), or any other table join function. Because GeoDataFrames are just DataFrames with extra information, performing a key-based join works, but we need to be careful about maintaining the spatial properties:\n\n# Traditional key-based join with spatial data\ncbsa_subset = cbsa[['geoid', 'name']].copy()  # Remove duplicate columns\nmerged_spatial = cbsa_geo.merge(cbsa_subset, on='geoid', suffixes=('', '_extra'))\n\nprint(f\"Merged data type: {type(merged_spatial)}\")\nprint(f\"Has geometry column: {'geometry' in merged_spatial.columns}\")\nprint(f\"Is GeoDataFrame: {isinstance(merged_spatial, gpd.GeoDataFrame)}\")\n\nMerged data type: &lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nHas geometry column: True\nIs GeoDataFrame: True\n\n\nNotice that the output maintains its GeoDataFrame properties because we started with a GeoDataFrame and merged regular DataFrame information into it.\nNow, we can move onto joins that function by considering the spatial relationships between two datasets. Geopandas provides a function sjoin() (spatial join) to combine two datasets based on their spatial relationships. For example, we can join the spatial points cbsa_geo dataset with the spatial polygons state to find which state each CBSA center point falls within:\n\n# Spatial join: find which state each CBSA center is in\n# First ensure both datasets have the same CRS\ncbsa_geo_proj = cbsa_geo.to_crs('EPSG:5069')\nstate_proj = state.to_crs('EPSG:5069')\n\n# Perform spatial join\ncbsa_with_states = gpd.sjoin(cbsa_geo_proj, state_proj, how='left', predicate='within')\n\n# Show results\nresult_sample = cbsa_with_states[['name_left', 'name_right', 'abb']].head(10)\nresult_sample\n\n\n\n\n\n\n\n\nname_left\nname_right\nabb\n\n\n\n\n0\nNew York\nNew Jersey\nNJ\n\n\n1\nLos Angeles\nCalifornia\nCA\n\n\n2\nChicago\nIllinois\nIL\n\n\n3\nDallas\nTexas\nTX\n\n\n4\nHouston\nTexas\nTX\n\n\n5\nWashington\nVirginia\nVA\n\n\n6\nPhiladelphia\nPennsylvania\nPA\n\n\n7\nMiami\nFlorida\nFL\n\n\n8\nAtlanta\nGeorgia\nGA\n\n\n9\nBoston\nMassachusetts\nMA\n\n\n\n\n\n\n\nAs confirmation of our join, we see that each CBSA region is now associated with the state it falls within. The New York City CBSA region might be associated with New Jersey if its center point falls there, even though the metropolitan area spans multiple states.\nThe spatial join supports different types of spatial relationships through the predicate parameter. For example, we can use 'touches' to find geometries that share a border. Let’s use this to join the state dataset to itself by finding all pairs of states that border one another:\n\n# Find states that border each other\nbordering_states = gpd.sjoin(state_proj, state_proj, how='inner', predicate='touches')\n\n# Remove self-matches and show results\nborder_pairs = (bordering_states\n    .query('abb_left != abb_right')\n    .loc[:, ['name_left', 'abb_left', 'name_right', 'abb_right']]\n    .head(10)\n)\n\nborder_pairs\n\n\n\n\n\n\n\n\nname_left\nabb_left\nname_right\nabb_right\n\n\n\n\n0\nMaine\nME\nNew Hampshire\nNH\n\n\n1\nNew Hampshire\nNH\nMassachusetts\nMA\n\n\n1\nNew Hampshire\nNH\nVermont\nVT\n\n\n1\nNew Hampshire\nNH\nMaine\nME\n\n\n2\nDelaware\nDE\nMaryland\nMD\n\n\n2\nDelaware\nDE\nNew Jersey\nNJ\n\n\n2\nDelaware\nDE\nPennsylvania\nPA\n\n\n3\nSouth Carolina\nSC\nGeorgia\nGA\n\n\n3\nSouth Carolina\nSC\nNorth Carolina\nNC\n\n\n4\nNebraska\nNE\nMissouri\nMO\n\n\n\n\n\n\n\nAnother useful spatial relationship is finding points that are within a certain distance of each other. We can compute distances between spatial objects using various methods:\n\n# Find distances between CBSA centers\n# For demonstration, let's find the closest CBSA to each one\n\n# Sample a few CBSAs for efficiency\ncbsa_sample = cbsa_geo_proj.head(20).copy()\n\n# Compute distance matrix\ndistances = cbsa_sample.geometry.apply(\n    lambda geom: cbsa_sample.geometry.distance(geom)\n)\n\n# Find closest pairs (excluding self-matches)\nclosest_pairs = []\nfor i, row in cbsa_sample.iterrows():\n    distances_to_others = cbsa_sample.geometry.distance(row.geometry)\n    # Exclude self (distance = 0)\n    closest_idx = distances_to_others[distances_to_others &gt; 0].idxmin()\n    closest_pairs.append({\n        'cbsa1': row['name'],\n        'cbsa2': cbsa_sample.loc[closest_idx, 'name'],\n        'distance_km': distances_to_others.loc[closest_idx] / 1000\n    })\n\nclosest_df = pd.DataFrame(closest_pairs).head()\nclosest_df\n\n\n\n\n\n\n\n\ncbsa1\ncbsa2\ndistance_km\n\n\n\n\n0\nNew York\nPhiladelphia\n140.000260\n\n\n1\nLos Angeles\nSan Diego\n185.744711\n\n\n2\nChicago\nDetroit\n403.856526\n\n\n3\nDallas\nHouston\n371.802326\n\n\n4\nHouston\nDallas\n371.802326\n\n\n\n\n\n\n\nWe can also find the farthest pairs of points by looking at the maximum distances:\n\n# Find the farthest CBSA pairs from our sample\nfarthest_pairs = []\nfor i, row in cbsa_sample.iterrows():\n    distances_to_others = cbsa_sample.geometry.distance(row.geometry)\n    farthest_idx = distances_to_others.idxmax()\n    if distances_to_others.loc[farthest_idx] &gt; 0:  # Exclude self-matches\n        farthest_pairs.append({\n            'cbsa1': row['name'],\n            'cbsa2': cbsa_sample.loc[farthest_idx, 'name'],\n            'distance_km': distances_to_others.loc[farthest_idx] / 1000\n        })\n\nfarthest_df = pd.DataFrame(farthest_pairs).sort_values('distance_km', ascending=False).head()\nfarthest_df\n\n\n\n\n\n\n\n\ncbsa1\ncbsa2\ndistance_km\n\n\n\n\n14\nSeattle\nMiami\n4305.998690\n\n\n7\nMiami\nSeattle\n4305.998690\n\n\n11\nSan Francisco\nBoston\n4290.724372\n\n\n9\nBoston\nSan Francisco\n4290.724372\n\n\n1\nLos Angeles\nBoston\n4129.790860\n\n\n\n\n\n\n\nBeing able to analyze proximity and distance between objects offers a way to explore humanities data. Spatial joins can be a powerful type of analysis, for often spatial analysis can be valuable beyond producing visualizations through maps.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "09_spatial_data.html#raster-maps",
    "href": "09_spatial_data.html#raster-maps",
    "title": "9  Spatial Data",
    "section": "9.6 Raster Maps",
    "text": "9.6 Raster Maps\nWe will finish this chapter by considering a completely different way to visualize spatial points. In the introduction, we mentioned that it is possible to build a spatial visualization of points by plotting the longitude and latitude on a scatterplot. If the points are in a relatively small region of the world not too close to either the North or South Pole, the projection of the data will not significantly affect the visualization. However, it can be difficult to understand a scatter plot of longitude and latitude pairs without polygons or lines to orient ourselves. One solution is to plot the points on top of a fixed map. One nice aspect of this approach is that we can grab map images from the entire world from a single source without having to hunt down polygon or line shapefiles for each application [4]. Raster maps are composed of pixels; they are images.\nIn Python, we can use the contextily library to add basemap tiles to our spatial plots. This library can fetch map tiles from various providers and overlay them with our spatial data:\n\nimport contextily as cx\n\n# For demonstration, let's use a dataset with smaller geographic scope\n# We'll create a sample of French cities\nfrench_city = pd.read_csv(\"data/geo_french_city.csv\")\nfrench_city\n\n\n\n\n\n\n\n\ncity\nlat\nlon\npopulation\nadmin_name\n\n\n\n\n0\nParis\n48.8667\n2.3333\n9904000\nIle-de-France\n\n\n1\nLyon\n45.7700\n4.8300\n1423000\nAuvergne-Rhone-Alpes\n\n\n2\nMarseille\n43.2900\n5.3750\n1400000\nProvence-Alpes-Cote d'Azur\n\n\n3\nLille\n50.6500\n3.0800\n1044000\nHauts-de-France\n\n\n4\nNice\n43.7150\n7.2650\n927000\nProvence-Alpes-Cote d'Azur\n\n\n5\nToulouse\n43.6200\n1.4499\n847000\nOccitanie\n\n\n6\nBordeaux\n44.8500\n-0.5950\n803000\nNouvelle-Aquitaine\n\n\n7\nRouen\n49.4304\n1.0800\n532559\nNormandie\n\n\n8\nStrasbourg\n48.5800\n7.7500\n439972\nGrand Est\n\n\n9\nNantes\n47.2104\n-1.5900\n438537\nPays de la Loire\n\n\n10\nMetz\n49.1203\n6.1800\n409186\nGrand Est\n\n\n11\nGrenoble\n45.1804\n5.7200\n388574\nAuvergne-Rhone-Alpes\n\n\n12\nToulon\n43.1342\n5.9188\n357693\nProvence-Alpes-Cote d'Azur\n\n\n13\nMontpellier\n43.6104\n3.8700\n327254\nOccitanie\n\n\n14\nNancy\n48.6837\n6.2000\n268976\nGrand Est\n\n\n15\nSaint-Etienne\n45.4304\n4.3800\n265684\nAuvergne-Rhone-Alpes\n\n\n16\nMelun\n48.5333\n2.6666\n249432\nIle-de-France\n\n\n17\nLe Havre\n49.5050\n0.1050\n242124\nNormandie\n\n\n18\nTours\n47.3804\n0.6999\n236096\nCentre-Val de Loire\n\n\n19\nClermont-Ferrand\n45.7800\n3.0800\n233050\nAuvergne-Rhone-Alpes\n\n\n20\nOrleans\n47.9004\n1.9000\n217301\nCentre-Val de Loire\n\n\n21\nMulhouse\n47.7504\n7.3500\n215454\nGrand Est\n\n\n22\nRennes\n48.1000\n-1.6700\n209375\nBretagne\n\n\n23\nReims\n49.2504\n4.0300\n196565\nGrand Est\n\n\n24\nCaen\n49.1838\n-0.3500\n190099\nNormandie\n\n\n25\nAngers\n47.4800\n-0.5300\n188380\nPays de la Loire\n\n\n26\nDijon\n47.3304\n5.0300\n169946\nBourgogne-Franche-Comte\n\n\n27\nNimes\n43.8304\n4.3500\n169547\nOccitanie\n\n\n28\nLimoges\n45.8300\n1.2500\n152199\nNouvelle-Aquitaine\n\n\n29\nAix-en-Provence\n43.5200\n5.4500\n146821\nProvence-Alpes-Cote d'Azur\n\n\n30\nPerpignan\n42.7000\n2.9000\n146620\nOccitanie\n\n\n31\nBiarritz\n43.4733\n-1.5616\n145348\nNouvelle-Aquitaine\n\n\n32\nBrest\n48.3904\n-4.4950\n144899\nBretagne\n\n\n33\nLe Mans\n48.0004\n0.1000\n144515\nPays de la Loire\n\n\n34\nAmiens\n49.9004\n2.3000\n143086\nHauts-de-France\n\n\n35\nBesancon\n47.2300\n6.0300\n128426\nBourgogne-Franche-Comte\n\n\n36\nAnnecy\n45.9000\n6.1167\n105749\nAuvergne-Rhone-Alpes\n\n\n37\nCalais\n50.9504\n1.8333\n92201\nHauts-de-France\n\n\n38\nPoitiers\n46.5833\n0.3333\n85960\nNouvelle-Aquitaine\n\n\n39\nVersailles\n48.8005\n2.1333\n85416\nIle-de-France\n\n\n40\nLorient\n47.7504\n-3.3666\n84952\nBretagne\n\n\n41\nLa Rochelle\n46.1667\n-1.1500\n76997\nNouvelle-Aquitaine\n\n\n42\nRoanne\n46.0333\n4.0667\n73315\nAuvergne-Rhone-Alpes\n\n\n43\nArras\n50.2833\n2.7833\n64165\nHauts-de-France\n\n\n44\nTroyes\n48.3404\n4.0834\n61703\nGrand Est\n\n\n45\nCherbourg\n49.6504\n-1.6500\n60991\nNormandie\n\n\n46\nAgen\n44.2004\n0.6333\n58223\nNouvelle-Aquitaine\n\n\n47\nTarbes\n43.2333\n0.0833\n54854\nOccitanie\n\n\n48\nAjaccio\n41.9271\n8.7283\n54364\nCorsica\n\n\n49\nSaint-Brieuc\n48.5167\n-2.7833\n53223\nBretagne\n\n\n50\nNevers\n46.9837\n3.1667\n45929\nBourgogne-Franche-Comte\n\n\n51\nVichy\n46.1171\n3.4167\n43158\nAuvergne-Rhone-Alpes\n\n\n52\nDieppe\n49.9337\n1.0833\n42461\nNormandie\n\n\n53\nBastia\n42.7032\n9.4500\n41001\nCorsica\n\n\n54\nBeziers\n43.3505\n3.2100\n81438\nOccitanie\n\n\n55\nBourges\n47.0837\n2.4000\n72340\nCentre-Val de Loire\n\n\n56\nBrive-la-Gaillarde\n45.1504\n1.5333\n55448\nNouvelle-Aquitaine\n\n\n57\nAuxerre\n47.8004\n3.5666\n41516\nBourgogne-Franche-Comte\n\n\n\n\n\n\n\nWe can plot these cities on top of a map without needing complex setup. Contextily works well with geopandas to automatically grab map tiles that correspond to our data extent:\n\n# Create GeoDataFrame from French cities\nfrench_geo = gpd.GeoDataFrame(\n    french_city, \n    geometry=gpd.points_from_xy(french_city.lon, french_city.lat),\n    crs='EPSG:4326'\n)\n\n# Transform to Web Mercator (required for contextily)\nfrench_geo_mercator = french_geo.to_crs('EPSG:3857')\n\n# Create plot with basemap\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Plot the points\nfrench_geo_mercator.plot(ax=ax, color='red', markersize=50, alpha=0.7)\n\n# Add basemap\ncx.add_basemap(ax, crs=french_geo_mercator.crs, source=cx.providers.OpenStreetMap.Mapnik)\n\nax.set_title(\"French Cities with OpenStreetMap Basemap\")\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also use different basemap providers and styles. Contextily supports many different tile sources:\n\n# Plot with a different basemap style\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Color points by administrative region\nfrench_geo_mercator.plot(ax=ax, column='admin_name', markersize=50, alpha=0.8, legend=True)\n\n# Add a terrain basemap\ntry:\n    cx.add_basemap(ax, crs=french_geo_mercator.crs, source=cx.providers.Stamen.Terrain)\n    ax.set_title(\"French Cities by Region (Terrain Basemap)\")\nexcept:\n    # Fallback to OpenStreetMap if Stamen is unavailable\n    cx.add_basemap(ax, crs=french_geo_mercator.crs, source=cx.providers.OpenStreetMap.Mapnik)\n    ax.set_title(\"French Cities by Region (OpenStreetMap Basemap)\")\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nFor more complex visualizations, we can work with line data. Let’s demonstrate with a Paris metro dataset:\n\n# Load Paris metro data\nparis_metro = pd.read_csv(\"data/geo_paris_metro.csv\")\nparis_metro\n\n\n\n\n\n\n\n\nname\nline\nline_color\nlon\nlat\nlon_end\nlat_end\n\n\n\n\n0\nLa Defense - Grande Arche\n1\n#ffbe00\n2.237018\n48.892187\n2.247932\n48.888631\n\n\n1\nEsplanade de la Defense\n1\n#ffbe00\n2.247932\n48.888631\n2.260515\n48.884708\n\n\n2\nPont de Neuilly (Avenue de Madrid)\n1\n#ffbe00\n2.260515\n48.884708\n2.271687\n48.881192\n\n\n3\nLes Sablons (Jardin d'acclimatation)\n1\n#ffbe00\n2.271687\n48.881192\n2.289323\n48.875594\n\n\n4\nArgentine\n1\n#ffbe00\n2.289323\n48.875594\n2.295905\n48.875150\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n366\nGare de Lyon\n14\n#640082\n2.373014\n48.843986\n2.379554\n48.840001\n\n\n367\nBercy\n14\n#640082\n2.379554\n48.840001\n2.386632\n48.833339\n\n\n368\nCour Saint-Emilion\n14\n#640082\n2.386632\n48.833339\n2.375748\n48.829990\n\n\n369\nBibliotheque Francois Mitterrand\n14\n#640082\n2.375748\n48.829990\n2.368033\n48.827271\n\n\n370\nOlympiades\n14\n#640082\n2.368033\n48.827271\nNaN\nNaN\n\n\n\n\n371 rows × 7 columns\n\n\n\nWe can create a more complex visualization showing metro lines with their official colors:\n\n# Filter for a few metro lines and create visualization\nmetro_subset = paris_metro[paris_metro['line'] &lt;= 4].copy()\n\n# Create geometries for start and end points\nstart_points = gpd.GeoDataFrame(\n    metro_subset,\n    geometry=gpd.points_from_xy(metro_subset.lon, metro_subset.lat),\n    crs='EPSG:4326'\n).to_crs('EPSG:3857')\n\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Plot points colored by line\nfor line_num in sorted(metro_subset['line'].unique()):\n    line_data = start_points[start_points['line'] == line_num]\n    line_color = line_data['line_color'].iloc[0]\n    line_data.plot(ax=ax, color=line_color, markersize=30, alpha=0.8, label=f'Line {line_num}')\n\n# Add basemap\ncx.add_basemap(ax, crs=start_points.crs, source=cx.providers.OpenStreetMap.Mapnik, alpha=0.7)\n\nax.set_title(\"Paris Metro Lines (Sample)\")\nax.legend()\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also create faceted plots to show different metro lines separately:\n\n# Create subplots for different metro lines\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\naxes = axes.flatten()\n\nfor i, line_num in enumerate(sorted(metro_subset['line'].unique())[:4]):\n    ax = axes[i]\n    \n    # Filter data for this line\n    line_data = start_points[start_points['line'] == line_num]\n    line_color = line_data['line_color'].iloc[0]\n    \n    # Plot line data\n    line_data.plot(ax=ax, color=line_color, markersize=30, alpha=0.8)\n    \n    # Add basemap\n    cx.add_basemap(ax, crs=line_data.crs, source=cx.providers.OpenStreetMap.Mapnik, alpha=0.7)\n    \n    ax.set_title(f'Metro Line {line_num}')\n    ax.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nRaster maps when layered with point and line data offer a way to quickly garner insights from humanities data. The combination of real-world geographic context (through basemap tiles) with our specific data points creates rich, interpretable visualizations.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "09_spatial_data.html#extensions",
    "href": "09_spatial_data.html#extensions",
    "title": "9  Spatial Data",
    "section": "9.7 Extensions",
    "text": "9.7 Extensions\nSpatial analysis is a large area with many exciting avenues for humanities data. The spatial turn in the discipline of History along with award-winning digital humanities projects like University of Richmond’s American Panorama project are just a few examples of how humanities fields have been embracing spatial analysis [5]. Concepts like thick mapping and deep mapping are also providing exciting theoretical interventions [6] [7].\nFor extending the methods mentioned in this chapter, consider exploring:\nPython Libraries: - GeoPandas: Core spatial data manipulation (used throughout this chapter) - Shapely: Geometric operations and analysis - Contextily: Basemap tiles and web map integration\n- Folium: Interactive web mapping - PyProj: Coordinate system transformations - Rasterio: Working with raster/satellite imagery data - OSMnx: Working with OpenStreetMap data for network analysis - PySAL: Spatial statistics and econometrics\nTheoretical Resources: A next step from this chapter would be exploring more advanced spatial statistics and modeling techniques. The concepts translate well from R to Python, with similar analytical capabilities available through the PySAL ecosystem. The text Applied Spatial Data Analysis with R [2] provides excellent conceptual background that applies to Python workflows as well. Spatial statistics and modeling by Carlo Gaetan and Xavier Guyon provides a more extensive introduction to spatial statistics [3]. For an introductory theoretical text, we recommend Mapping: A Critical Introduction to Cartography and GIS [8].\nThe Python spatial ecosystem is rapidly evolving, with particularly strong capabilities in areas like satellite imagery analysis, urban analytics, and large-scale spatial data processing that continue to grow.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "09_spatial_data.html#references",
    "href": "09_spatial_data.html#references",
    "title": "9  Spatial Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Franklin, C and Hane, P (1992 ). An introduction to geographic information systems: Linking maps to databases [and] maps for the rest of us: Affordable and fun. Database. ERIC. 15 12–5\n\n\n[2] Bivand, R S, Pebesma, E J, Gómez-Rubio, V and Pebesma, E J (2008 ). Applied Spatial Data Analysis with r. Springer\n\n\n[3] Gaetan, C, Guyon, X and others (2010 ). Spatial Statistics and Modeling. Springer\n\n\n[4] Nie, Y F, Xu, H and Liu, H L (2011 ). The design and implementation of tile map service. Advanced Materials Research. Trans Tech Publ. 159 714–9\n\n\n[5] Connolly, N, Winling, L, Nelson, R K and Marciano, R (2018 ). Mapping inequality:‘big data’meets social history in the story of redlining. The routledge companion to spatial history. Routledge. 502–24\n\n\n[6] Bodenhamer, D J, Harris, T M and Corrigan, J (2013 ). Deep mapping and the spatial humanities. International Journal of Humanities and Arts Computing. Edinburgh University Press. 7 170–5\n\n\n[7] Presner, T and Shepard, D (2015 ). Mapping the geospatial turn. A new companion to digital humanities. Wiley Online Library. 199–212\n\n\n[8] Crampton, J W (2010 ). Mapping: A Critical Introduction to Cartography and GIS. John Wiley & Sons",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "10_image_data.html",
    "href": "10_image_data.html",
    "title": "10  Image Data",
    "section": "",
    "text": "10.1 Introduction\nA large amount of humanities data consists of digitized image data, and there is an active push to digitize even more. Examples of large corpora include Google Books, HathiTrust, U.S. Library of Congress, the Getty Museum, Europeana, Wikimedia, and the Rijksmuseum. In some cases, these image collections represent scans of mostly textual data. In others, the images represent digitized art works or photographic prints; in these cases the images serve as direct historical evidence, objects of study in their own right, or both. Converting images with text into raw text data is an interesting problem in computer vision, known as optical character recognition or OCR. However, we will concentrate in this chapter only on the cases where images directly represent artwork and historical documents that are known for their visual semantics.\nWhile many humanities projects have worked with image data over the years, it has only been recently that there has been a large push to analyze the actual images themselves. Our own work has theorized and offered a method for computationally working with digital images called Distant Viewing, which we expand on in an open access book by the same name [1]. Others have offered concepts such as deep watching and cultural analytics [2] [3]. All are concerned with how to computationally analyze images for there are many possibilities from fields such as art history, film and media studies, visual culture studies and more.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "10_image_data.html#loading-images",
    "href": "10_image_data.html#loading-images",
    "title": "10  Image Data",
    "section": "10.2 Loading Images",
    "text": "10.2 Loading Images\nWe will be working with a new dataset in this chapter. Our collection consists of the images taken by the photographic unit of the United States Farm Securities Administration and Office of War Information, commonly called the FSA-OWI. While best known for the over 170 thousand black-and-white images documenting the United States during the Great Depression and World War II, the FSA-OWI also created over sixteen hundred color photographs. Both of these collections are ones that we have worked extensively with in a variety of projects. The color subset is a perfect size to demonstrate the methods in this chapter and, unlike the black-and-white images, will allow us to start with a study of several color-based analyses.\nOur analysis of image data in Python will in many ways follow the pattern seen in the previous chapters. We work to organize our data into structured, tabular datasets that capture information and models about the data. Then, we use the visualization and manipulation approaches in the first five chapters to understand the information in each table. To start, we will load a CSV file into Python that contains metadata about each of the images:\n\nfsac = pd.read_csv(\"data/fsac_metadata.csv.bz2\")\nfsac.sample(10)\n\n\n\n\n\n\n\n\nfilename\nyear\ntitle\nheight\nwidth\nthm_path\nmed_path\n\n\n\n\n1161\n2017878929\n1939\nCarefully trained women inspectors check and i...\n827\n1024\ndata/fsac/thm/2017878929.jpg\ndata/fsac/med/2017878929.jpg\n\n\n15\n2017877467\n1939\nBoys fishing in a bayou, Schriever, La.\n1024\n727\ndata/fsac/thm/2017877467.jpg\ndata/fsac/med/2017877467.jpg\n\n\n655\n2017877864\n1939\n[Two people kneeling, working in a field, poss...\n710\n1024\ndata/fsac/thm/2017877864.jpg\ndata/fsac/med/2017877864.jpg\n\n\n831\n2017877528\n1939\nLandscape on the Jackson farm, vicinity of Whi...\n787\n1024\ndata/fsac/thm/2017877528.jpg\ndata/fsac/med/2017877528.jpg\n\n\n109\n2017877569\n1939\nA Fourth of July celebration, St. Helena's Isl...\n721\n1024\ndata/fsac/thm/2017877569.jpg\ndata/fsac/med/2017877569.jpg\n\n\n1513\n2017878201\n1939\nUnloading a lake freighter at the Pennsylvania...\n796\n1024\ndata/fsac/thm/2017878201.jpg\ndata/fsac/med/2017878201.jpg\n\n\n1463\n2017878187\n1939\nGeneral view of part of the Bensenville freigh...\n795\n1024\ndata/fsac/thm/2017878187.jpg\ndata/fsac/med/2017878187.jpg\n\n\n64\n2017877495\n1939\nBackyard of Negro tenant's home, Marcella Plan...\n718\n1024\ndata/fsac/thm/2017877495.jpg\ndata/fsac/med/2017877495.jpg\n\n\n1067\n2017878419\n1939\nLight tank, Ft. Knox, Ky.\n823\n1024\ndata/fsac/thm/2017878419.jpg\ndata/fsac/med/2017878419.jpg\n\n\n929\n2017878660\n1939\nMarine with the training gliders at Page Field...\n1024\n719\ndata/fsac/thm/2017878660.jpg\ndata/fsac/med/2017878660.jpg\n\n\n\n\n\n\n\nThe metadata includes a unique identifier for each image in the first column. It also contains the year in which the photograph was taken and a short title of the photograph. This table also has the dimensions of the image and columns that have the file path to two versions of each image. One copy is a small thumbnail that will be useful to efficiently create visualizations. The other image is the larger version that we will use for most of the actual analyses. Unlike other data types that we have seen, image data is usually stored as a set of separate files, with one file storing the information about a single image. It is not necessary when working with an image collection to have two versions of each image, but if we are given them, as is this case with this collection, it can be helpful to keep both to speed up the visualization techniques.\nOur work with images in this chapter will use several powerful Python libraries. PIL (Python Imaging Library) and OpenCV will handle image loading and basic processing. scikit-image provides additional image processing capabilities. For computer vision tasks, we’ll use OpenCV and potentially MediaPipe for more advanced features. matplotlib will handle visualization, and we’ll use numpy extensively for array operations since images are represented as numerical arrays.\n\nfrom PIL import Image\nimport cv2\nfrom skimage import color\nimport matplotlib.pyplot as plt\n\nAs a starting point, we will read a single image into Python and see how it is represented. Python’s PIL library makes this straightforward:\n\n# Load the first image\nimage_path = fsac['thm_path'].iloc[0]\n\n# Load with PIL\nimg_pil = Image.open(image_path)\nprint(f\"PIL Image size: {img_pil.size}\")\nprint(f\"PIL Image mode: {img_pil.mode}\")\n\n# Convert to numpy array for analysis\nimg_array = np.array(img_pil)\nprint(f\"Array shape: {img_array.shape}\")\nprint(f\"Array data type: {img_array.dtype}\")\n\nPIL Image size: (150, 105)\nPIL Image mode: RGB\nArray shape: (105, 150, 3)\nArray data type: uint8\n\n\nImage data is represented as one or more rectangular grids of pixels, the smallest identifiable locations of an image. A pixel is defined by a set of three numbers giving the intensity of the red, green, and blue lights needed to represent the color of that part of the image on a digital display. The array that we now have in Python represents our image in pixels. We can see that the thumbnail has dimensions corresponding to height, width, and the usual three color channels for red, green, and blue intensities. Let’s examine the pixel values:\n\nprint(img_array[:4, :4, :])\nprint(f\"\\nPixel value range: {img_array.min()} to {img_array.max()}\")\n\n[[[31 51  0]\n  [36 56  5]\n  [37 56 10]\n  [25 43  1]]\n\n [[37 57  6]\n  [20 40  0]\n  [32 51  6]\n  [44 62 22]]\n\n [[36 55  9]\n  [45 64 19]\n  [42 58 19]\n  [33 49 13]]\n\n [[34 52 10]\n  [31 49  9]\n  [34 50 14]\n  [36 51 20]]]\n\nPixel value range: 0 to 255\n\n\nWe see that each pixel intensity is given as a number between 0 and 255 for uint8 images (the most common format). A value of 255 indicates maximum intensity, while 0 indicates no intensity for that color channel.\nWorking directly with the array format gives us full control, but for analysis it’s often useful to convert to a tabular format with one row per pixel:\n\ndef image_to_dataframe(image_path, image_id=None):\n    \"\"\"Convert an image to a DataFrame with one row per pixel.\"\"\"\n    # Load image\n    img = Image.open(image_path)\n    img_array = np.array(img)\n    \n    # Get dimensions\n    height, width = img_array.shape[:2]\n    \n    # Handle grayscale vs color images\n    if len(img_array.shape) == 3:\n        channels = img_array.shape[2]\n    else:\n        channels = 1\n        img_array = img_array[:, :, np.newaxis]\n    \n    # Create coordinate grids\n    rows, cols = np.mgrid[0:height, 0:width]\n    \n    # Flatten everything\n    pixel_data = {\n        'image_id': image_id or 0,\n        'row': rows.flatten(),\n        'col': cols.flatten(),\n        'height': height,\n        'width': width\n    }\n    \n    # Add color channels\n    if channels &gt;= 3:\n        pixel_data['red'] = img_array[:, :, 0].flatten()\n        pixel_data['green'] = img_array[:, :, 1].flatten()\n        pixel_data['blue'] = img_array[:, :, 2].flatten()\n        \n        # Convert RGB to HSV for analysis\n        img_hsv = color.rgb2hsv(img_array / 255.0)  # Convert to 0-1 range for HSV\n        pixel_data['hue'] = img_hsv[:, :, 0].flatten()\n        pixel_data['saturation'] = img_hsv[:, :, 1].flatten()\n        pixel_data['value'] = img_hsv[:, :, 2].flatten()\n        \n        # Create hex color representation\n        hex_colors = []\n        for r, g, b in zip(pixel_data['red'], pixel_data['green'], pixel_data['blue']):\n            hex_colors.append(f'#{r:02x}{g:02x}{b:02x}')\n        pixel_data['hex'] = hex_colors\n    \n    return pd.DataFrame(pixel_data)\n\n# Convert our first image to DataFrame format\npix_single = image_to_dataframe(fsac['thm_path'].iloc[0], fsac['filename'].iloc[0])\npix_single\n\n\n\n\n\n\n\n\nimage_id\nrow\ncol\nheight\nwidth\nred\ngreen\nblue\nhue\nsaturation\nvalue\nhex\n\n\n\n\n0\n2017877351\n0\n0\n105\n150\n31\n51\n0\n0.232026\n1.000000\n0.200000\n#1f3300\n\n\n1\n2017877351\n0\n1\n105\n150\n36\n56\n5\n0.232026\n0.910714\n0.219608\n#243805\n\n\n2\n2017877351\n0\n2\n105\n150\n37\n56\n10\n0.235507\n0.821429\n0.219608\n#25380a\n\n\n3\n2017877351\n0\n3\n105\n150\n25\n43\n1\n0.238095\n0.976744\n0.168627\n#192b01\n\n\n4\n2017877351\n0\n4\n105\n150\n38\n56\n16\n0.241667\n0.714286\n0.219608\n#263810\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15745\n2017877351\n104\n145\n105\n150\n30\n44\n8\n0.231481\n0.818182\n0.172549\n#1e2c08\n\n\n15746\n2017877351\n104\n146\n105\n150\n29\n43\n7\n0.231481\n0.837209\n0.168627\n#1d2b07\n\n\n15747\n2017877351\n104\n147\n105\n150\n32\n46\n10\n0.231481\n0.782609\n0.180392\n#202e0a\n\n\n15748\n2017877351\n104\n148\n105\n150\n33\n47\n11\n0.231481\n0.765957\n0.184314\n#212f0b\n\n\n15749\n2017877351\n104\n149\n105\n150\n32\n46\n10\n0.231481\n0.782609\n0.180392\n#202e0a\n\n\n\n\n15750 rows × 12 columns\n\n\n\nNotice that the output has one row for each pixel. The first column identifies the image, followed by row and column coordinates, image dimensions, and then the RGB color values. We’ve also computed HSV (Hue, Saturation, Value) representations and hex color codes. Four other derived measurements are provided in the last columns. We will investigate these measurements in the next section.\nNow let’s create a function to process multiple images:\n\ndef process_image_collection(image_paths, image_ids, max_images=None):\n    \"\"\"Process a collection of images into a combined pixel DataFrame.\"\"\"\n    if max_images:\n        image_paths = image_paths[:max_images]\n        image_ids = image_ids[:max_images]\n    \n    pixel_dataframes = []\n    \n    for i, (path, img_id) in enumerate(zip(image_paths, image_ids)):\n        try:\n            df = image_to_dataframe(path, img_id)\n            pixel_dataframes.append(df)\n            if (i + 1) % 100 == 0:\n                print(f\"Processed {i + 1} images...\")\n        except Exception as e:\n            print(f\"Error processing {path}: {e}\")\n    \n    return pd.concat(pixel_dataframes, ignore_index=True)\n\n# For demonstration, let's process just the first 10 images\nprint(\"Processing first 10 images...\")\npix_sample = process_image_collection(\n    fsac['thm_path'].iloc[:10].tolist(),\n    fsac['filename'].iloc[:10].tolist()\n)\n\npix_sample\n\nProcessing first 10 images...\n\n\n\n\n\n\n\n\n\nimage_id\nrow\ncol\nheight\nwidth\nred\ngreen\nblue\nhue\nsaturation\nvalue\nhex\n\n\n\n\n0\n2017877351\n0\n0\n105\n150\n31\n51\n0\n0.232026\n1.000000\n0.200000\n#1f3300\n\n\n1\n2017877351\n0\n1\n105\n150\n36\n56\n5\n0.232026\n0.910714\n0.219608\n#243805\n\n\n2\n2017877351\n0\n2\n105\n150\n37\n56\n10\n0.235507\n0.821429\n0.219608\n#25380a\n\n\n3\n2017877351\n0\n3\n105\n150\n25\n43\n1\n0.238095\n0.976744\n0.168627\n#192b01\n\n\n4\n2017877351\n0\n4\n105\n150\n38\n56\n16\n0.241667\n0.714286\n0.219608\n#263810\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n158845\n2017877478\n108\n145\n109\n150\n51\n87\n26\n0.265027\n0.701149\n0.341176\n#33571a\n\n\n158846\n2017877478\n108\n146\n109\n150\n51\n85\n24\n0.259563\n0.717647\n0.333333\n#335518\n\n\n158847\n2017877478\n108\n147\n109\n150\n51\n85\n24\n0.259563\n0.717647\n0.333333\n#335518\n\n\n158848\n2017877478\n108\n148\n109\n150\n55\n87\n24\n0.251323\n0.724138\n0.341176\n#375718\n\n\n158849\n2017877478\n108\n149\n109\n150\n56\n88\n25\n0.251323\n0.715909\n0.345098\n#385819\n\n\n\n\n158850 rows × 12 columns",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "10_image_data.html#pixels-and-color",
    "href": "10_image_data.html#pixels-and-color",
    "title": "10  Image Data",
    "section": "10.3 Pixels and Color",
    "text": "10.3 Pixels and Color\nLet’s now see if we can use the pixel-level data to do some exploratory data analysis with the images. Since processing all 1,600 images would create an enormous dataset, we’ll work with subsets and demonstrate the techniques that could be scaled up.\nIt is tempting to use the red, green, and blue pixel intensities directly to compute summary values for the images. For example, we might want to group by filename and determine which images have the highest average values of green. Unfortunately, our current calculations will not actually indicate very clearly which images contain the color that we would perceive as green. The color of a pixel can be represented as three numbers because the human eye has three different kinds of cells, called cones, that are sensitive to three different wavelengths of light.\nThe blending of colors is what makes it challenging to summarize raw pixel intensities. An image that has a large average green intensity could have a lot of green in it. But, if the green is always blended with red, it could be primarily yellow. Or, if all three intensities are high, the image might only have a large amount of white. In order to work around this issue, it is useful to transform the pixel intensities into a new set of numbers that more closely represent the way that we think about color working. We will work with the HSV representation, which stands for hue, saturation, and value.\nThe value of a pixel represents how bright or intense the pixel is. The saturation measures the richness of a color, with zero being a shade of grey and one being a “pure” color. The hue corresponds to information about where a pixel sits in the rainbow of colors, roughly corresponding to a color wheel.\nLet’s visualize the pixel colors from our sample images:\n\n# Create HSV visualization for the first few images\nsample_images = pix_sample[pix_sample['image_id'].isin(pix_sample['image_id'].unique()[:6])]\n\n# Create the plot\np = (ggplot(sample_images, aes(x='hue', y='saturation')) +\n     geom_point(aes(color='hex'), alpha=0.6, size=0.5) +\n     scale_color_identity() +\n     facet_wrap('~image_id', ncol=3) +\n     labs(title=\"Pixel Colors in Sample Images (HSV Space)\",\n          x=\"Hue\", y=\"Saturation\") +\n     theme_minimal())\n\np\n\n\n\n\n\n\n\n\nLet’s also look at the actual images for reference:\n\n# Display the actual images\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, img_id in enumerate(pix_sample['image_id'].unique()[:6]):\n    img_path = fsac[fsac['filename'] == img_id]['thm_path'].iloc[0]\n    img = Image.open(img_path)\n    axes[i].imshow(img)\n    axes[i].set_title(f\"Image: {img_id}\")\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow we can use the HSV measurements to analyze our image collection. Let’s find the darkest images by computing average brightness (value):\n\n# Compute average brightness for each image\nbrightness_summary = (pix_sample\n    .groupby('image_id')\n    .agg({'value': 'mean'})\n    .reset_index()\n    .sort_values('value')\n)\n\n# Join with metadata to see titles\ndark_images = (brightness_summary\n    .merge(fsac, left_on='image_id', right_on='filename')\n    .head(10)\n)\n\ndark_images[['filename', 'title', 'value']]\n\n\n\n\n\n\n\n\nfilename\ntitle\nvalue\n\n\n\n\n0\n2017877351\nCommuters, who have just come off the train, w...\n0.200132\n\n\n1\n2017877502\nGoing to town on Saturday afternoon, Greene Co...\n0.320172\n\n\n2\n2017877477\nNegroes fishing in creek near cotton plantatio...\n0.334167\n\n\n3\n2017877476\nSouthern U.S., Mississippi?\n0.340388\n\n\n4\n2017877453\nA train bringing copper ore out of the mine, D...\n0.356685\n\n\n5\n2017877451\nCopper mining and sulfuric acid plant, Copperh...\n0.391784\n\n\n6\n2017877454\nCopper mining and sulfuric acid plant, Copperh...\n0.410381\n\n\n7\n2017877478\nNegroes fishing in creek near cotton plantatio...\n0.423666\n\n\n8\n2017877359\nRailroad cars and factory buildings in Lawrenc...\n0.455252\n\n\n9\n2017877452\nCopper mining and sulfuric acid plant, Copperh...\n0.462848\n\n\n\n\n\n\n\nSimilarly, we can look at images with highly saturated colors:\n\n# Find images with saturated colors\nsaturation_summary = (pix_sample\n    .query('saturation &gt; 0.8 and value &gt; 0.5')  # Bright and saturated pixels\n    .groupby('image_id')\n    .size()\n    .reset_index(name='saturated_pixel_count')\n)\n\n# Calculate proportion of saturated pixels\nimage_totals = (pix_sample\n    .groupby('image_id')\n    .size()\n    .reset_index(name='total_pixels')\n)\n\nsaturated_analysis = (saturation_summary\n    .merge(image_totals, on='image_id')\n    .assign(saturated_proportion = lambda df: df['saturated_pixel_count'] / df['total_pixels'])\n    .sort_values('saturated_proportion', ascending=False)\n    .merge(fsac, left_on='image_id', right_on='filename')\n)\n\nsaturated_analysis[['filename', 'title', 'saturated_proportion']]\n\n\n\n\n\n\n\n\nfilename\ntitle\nsaturated_proportion\n\n\n\n\n0\n2017877478\nNegroes fishing in creek near cotton plantatio...\n0.012232\n\n\n1\n2017877476\nSouthern U.S., Mississippi?\n0.000881\n\n\n\n\n\n\n\nFor hue analysis, we can group similar hues into buckets and analyze color distribution:\n\ndef analyze_dominant_hues(pixel_df, n_hue_bins=12):\n    \"\"\"Analyze dominant hues in images.\"\"\"\n    # Filter to sufficiently bright and saturated pixels\n    filtered_pixels = pixel_df.query('value &gt; 0.2 and saturation &gt; 0.2').copy()\n    \n    # Create hue bins\n    filtered_pixels['hue_bin'] = np.floor(filtered_pixels['hue'] * n_hue_bins).astype(int)\n    \n    # Calculate proportions for each image and hue bin\n    hue_props = (filtered_pixels\n        .groupby(['image_id', 'hue_bin'])\n        .size()\n        .reset_index(name='pixel_count')\n    )\n    \n    # Add total pixels per image for proportion calculation\n    total_pixels = (filtered_pixels\n        .groupby('image_id')\n        .size()\n        .reset_index(name='total_pixels')\n    )\n    \n    hue_analysis = (hue_props\n        .merge(total_pixels, on='image_id')\n        .assign(proportion = lambda df: df['pixel_count'] / df['total_pixels'])\n        .sort_values('proportion', ascending=False)\n    )\n    \n    return hue_analysis\n\n# Analyze hues in our sample\nhue_results = analyze_dominant_hues(pix_sample)\nhue_results.head(10)\n\n\n\n\n\n\n\n\nimage_id\nhue_bin\npixel_count\ntotal_pixels\nproportion\n\n\n\n\n25\n2017877454\n0\n5152\n5246\n0.982082\n\n\n19\n2017877452\n0\n6395\n6880\n0.929506\n\n\n43\n2017877478\n1\n12608\n15047\n0.837908\n\n\n17\n2017877451\n0\n4294\n5344\n0.803518\n\n\n22\n2017877453\n0\n6672\n8505\n0.784480\n\n\n12\n2017877359\n7\n6255\n9508\n0.657867\n\n\n39\n2017877477\n1\n6147\n10608\n0.579468\n\n\n1\n2017877351\n1\n527\n955\n0.551832\n\n\n29\n2017877476\n1\n3445\n7920\n0.434975\n\n\n38\n2017877477\n0\n4435\n10608\n0.418081\n\n\n\n\n\n\n\nLet’s create a more comprehensive analysis with a larger sample:\n\n# Process more images for better analysis (first 50 to keep manageable)\nprint(\"Processing first 50 images for comprehensive analysis...\")\npix_extended = process_image_collection(\n    fsac['thm_path'].iloc[:50].tolist(),\n    fsac['filename'].iloc[:50].tolist()\n)\n\n# Analyze color characteristics\ncolor_summary = (pix_extended\n    .groupby('image_id')\n    .agg({\n        'value': ['mean', 'std'],\n        'saturation': ['mean', 'std'], \n        'hue': ['mean', 'std']\n    })\n    .round(3)\n)\n\n# Flatten column names\ncolor_summary.columns = ['_'.join(col).strip() for col in color_summary.columns]\ncolor_summary = color_summary.reset_index()\n\ncolor_summary\n\nProcessing first 50 images for comprehensive analysis...\n\n\n\n\n\n\n\n\n\nimage_id\nvalue_mean\nvalue_std\nsaturation_mean\nsaturation_std\nhue_mean\nhue_std\n\n\n\n\n0\n2017877342\n0.204\n0.088\n0.362\n0.108\n0.097\n0.096\n\n\n1\n2017877343\n0.443\n0.296\n0.409\n0.241\n0.245\n0.326\n\n\n2\n2017877344\n0.409\n0.115\n0.434\n0.146\n0.231\n0.133\n\n\n3\n2017877345\n0.432\n0.213\n0.434\n0.230\n0.234\n0.296\n\n\n4\n2017877346\n0.454\n0.233\n0.343\n0.128\n0.483\n0.235\n\n\n5\n2017877347\n0.428\n0.221\n0.312\n0.136\n0.347\n0.253\n\n\n6\n2017877348\n0.356\n0.174\n0.424\n0.284\n0.209\n0.221\n\n\n7\n2017877349\n0.363\n0.265\n0.417\n0.233\n0.216\n0.352\n\n\n8\n2017877351\n0.200\n0.164\n0.210\n0.207\n0.221\n0.176\n\n\n9\n2017877359\n0.455\n0.169\n0.353\n0.212\n0.510\n0.265\n\n\n10\n2017877360\n0.373\n0.205\n0.383\n0.231\n0.327\n0.263\n\n\n11\n2017877361\n0.405\n0.256\n0.324\n0.197\n0.391\n0.254\n\n\n12\n2017877362\n0.400\n0.213\n0.256\n0.215\n0.190\n0.058\n\n\n13\n2017877363\n0.454\n0.215\n0.375\n0.165\n0.471\n0.243\n\n\n14\n2017877451\n0.392\n0.289\n0.286\n0.201\n0.166\n0.147\n\n\n15\n2017877452\n0.463\n0.296\n0.313\n0.210\n0.250\n0.285\n\n\n16\n2017877453\n0.357\n0.164\n0.334\n0.183\n0.102\n0.170\n\n\n17\n2017877454\n0.410\n0.233\n0.303\n0.197\n0.375\n0.403\n\n\n18\n2017877455\n0.437\n0.193\n0.536\n0.185\n0.169\n0.083\n\n\n19\n2017877456\n0.479\n0.212\n0.321\n0.159\n0.341\n0.386\n\n\n20\n2017877457\n0.341\n0.222\n0.479\n0.201\n0.365\n0.418\n\n\n21\n2017877458\n0.427\n0.316\n0.262\n0.181\n0.326\n0.377\n\n\n22\n2017877459\n0.518\n0.240\n0.359\n0.187\n0.169\n0.236\n\n\n23\n2017877460\n0.314\n0.216\n0.342\n0.159\n0.157\n0.294\n\n\n24\n2017877464\n0.184\n0.128\n0.409\n0.228\n0.165\n0.174\n\n\n25\n2017877465\n0.414\n0.210\n0.524\n0.200\n0.238\n0.130\n\n\n26\n2017877466\n0.222\n0.229\n0.405\n0.203\n0.141\n0.229\n\n\n27\n2017877467\n0.137\n0.083\n0.316\n0.151\n0.258\n0.128\n\n\n28\n2017877468\n0.556\n0.320\n0.455\n0.191\n0.125\n0.117\n\n\n29\n2017877469\n0.138\n0.101\n0.459\n0.223\n0.259\n0.166\n\n\n30\n2017877470\n0.274\n0.161\n0.467\n0.204\n0.214\n0.073\n\n\n31\n2017877471\n0.514\n0.284\n0.332\n0.224\n0.094\n0.037\n\n\n32\n2017877472\n0.180\n0.108\n0.469\n0.225\n0.098\n0.039\n\n\n33\n2017877473\n0.414\n0.339\n0.412\n0.274\n0.153\n0.247\n\n\n34\n2017877474\n0.518\n0.278\n0.527\n0.167\n0.112\n0.052\n\n\n35\n2017877476\n0.340\n0.227\n0.325\n0.180\n0.252\n0.164\n\n\n36\n2017877477\n0.334\n0.175\n0.501\n0.156\n0.096\n0.037\n\n\n37\n2017877478\n0.424\n0.152\n0.683\n0.123\n0.134\n0.051\n\n\n38\n2017877479\n0.456\n0.246\n0.564\n0.185\n0.115\n0.102\n\n\n39\n2017877480\n0.524\n0.312\n0.442\n0.191\n0.092\n0.037\n\n\n40\n2017877481\n0.475\n0.242\n0.483\n0.156\n0.079\n0.029\n\n\n41\n2017877482\n0.329\n0.243\n0.356\n0.170\n0.139\n0.113\n\n\n42\n2017877502\n0.320\n0.233\n0.375\n0.211\n0.250\n0.224\n\n\n43\n2017877503\n0.664\n0.290\n0.230\n0.218\n0.355\n0.388\n\n\n44\n2017877504\n0.522\n0.350\n0.330\n0.293\n0.087\n0.152\n\n\n45\n2017877505\n0.440\n0.281\n0.303\n0.266\n0.139\n0.182\n\n\n46\n2017877506\n0.483\n0.233\n0.190\n0.210\n0.387\n0.270\n\n\n47\n2017877507\n0.456\n0.318\n0.317\n0.290\n0.189\n0.192\n\n\n48\n2017877567\n0.275\n0.227\n0.486\n0.171\n0.103\n0.037\n\n\n49\n2017877568\n0.371\n0.221\n0.280\n0.176\n0.151\n0.177",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "10_image_data.html#computer-vision-with-opencv",
    "href": "10_image_data.html#computer-vision-with-opencv",
    "title": "10  Image Data",
    "section": "10.4 Computer Vision with OpenCV",
    "text": "10.4 Computer Vision with OpenCV\nWorking with pixel-level data gives us insights into color and basic image properties, but to access higher-level aspects like objects, faces, and poses, we need computer vision algorithms. Python’s OpenCV library provides excellent built-in capabilities for many computer vision tasks.\n\nimport cv2\n\ndef detect_objects_simple(image_path, min_area=1000):\n    \"\"\"Simple object detection using contour finding.\"\"\"\n    # Load image\n    img = cv2.imread(image_path)\n    if img is None:\n        return []\n    \n    # Convert to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Apply threshold to get binary image\n    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n    \n    # Find contours\n    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Extract bounding boxes for large contours\n    objects = []\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if area &gt; min_area:\n            x, y, w, h = cv2.boundingRect(contour)\n            objects.append({\n                'x0': x, 'y0': y, 'x1': x + w, 'y1': y + h,\n                'area': area, 'confidence': area / (img.shape[0] * img.shape[1])\n            })\n    \n    return objects\n\n# Test simple object detection\nsample_image = fsac['med_path'].iloc[0]\nsimple_objects = detect_objects_simple(sample_image)\nsimple_objects\n\n[{'x0': 430,\n  'y0': 142,\n  'x1': 950,\n  'y1': 258,\n  'area': 11556.5,\n  'confidence': 0.015784118225524474},\n {'x0': 394,\n  'y0': 61,\n  'x1': 442,\n  'y1': 109,\n  'area': 1469.5,\n  'confidence': 0.0020070749562937063},\n {'x0': 20,\n  'y0': 24,\n  'x1': 411,\n  'y1': 260,\n  'area': 33478.0,\n  'confidence': 0.04572497814685315}]\n\n\nFor more sophisticated object detection, we can use pre-trained models. Here’s an example using OpenCV’s DNN module with a pre-trained model:\n\ndef setup_yolo_detection():\n    \"\"\"Set up YOLO object detection (if model files are available).\"\"\"\n    try:\n        # Note: In practice, you would download these files\n        # net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n        # classes = open('coco.names').read().strip().split('\\n')\n        # return net, classes\n        print(\"YOLO model files not available in this demo\")\n        return None, None\n    except:\n        print(\"YOLO model setup failed - using placeholder\")\n        return None, None\n\ndef detect_faces_opencv(image_path):\n    \"\"\"Detect faces using OpenCV's built-in cascade classifier.\"\"\"\n    # Load the pre-trained face cascade\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n    \n    # Load image\n    img = cv2.imread(image_path)\n    if img is None:\n        return []\n    \n    # Convert to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Detect faces\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n    \n    # Convert to our standard format\n    face_detections = []\n    for (x, y, w, h) in faces:\n        face_detections.append({\n            'x0': x, 'y0': y, 'x1': x + w, 'y1': y + h,\n            'confidence': 0.8  # OpenCV doesn't provide confidence scores for Haar cascades\n        })\n    \n    return face_detections\n\n# Test face detection\nfaces_detected = detect_faces_opencv(sample_image)\nfaces_detected\n\n[{'x0': np.int32(360),\n  'y0': np.int32(133),\n  'x1': np.int32(423),\n  'y1': np.int32(196),\n  'confidence': 0.8}]\n\n\nLet’s create a visualization function to show detection results:\n\ndef visualize_detections(image_path, detections, detection_type=\"objects\"):\n    \"\"\"Visualize detection results on an image.\"\"\"\n    # Load image\n    img = cv2.imread(image_path)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    # Create plot\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    ax.imshow(img_rgb)\n    \n    # Draw bounding boxes\n    for detection in detections:\n        x0, y0, x1, y1 = detection['x0'], detection['y0'], detection['x1'], detection['y1']\n        conf = detection.get('confidence', 0)\n        \n        # Draw rectangle\n        rect = plt.Rectangle((x0, y0), x1-x0, y1-y0, \n                           fill=False, color='orange', linewidth=2)\n        ax.add_patch(rect)\n        \n        # Add label\n        ax.text(x0, y0-10, f\"{detection_type} ({conf:.2f})\", \n               color='orange', fontsize=10, fontweight='bold')\n    \n    ax.set_title(f\"{detection_type.title()} Detection Results\")\n    ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Visualize face detection results\nif faces_detected:\n    visualize_detections(sample_image, faces_detected, \"face\")\nelse:\n    print(\"No faces detected in sample image\")\n\n\n\n\n\n\n\n\nFor pose detection, we can use MediaPipe (if available) or create a simplified version:\n\ndef detect_keypoints_simple(image_path):\n    \"\"\"Simple keypoint detection using corner detection.\"\"\"\n    # Load image\n    img = cv2.imread(image_path)\n    if img is None:\n        return []\n    \n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Detect corners using Harris corner detection\n    corners = cv2.goodFeaturesToTrack(gray, maxCorners=100, qualityLevel=0.01, minDistance=10)\n    \n    keypoints = []\n    if corners is not None:\n        for corner in corners:\n            x, y = corner.ravel()\n            keypoints.append({\n                'x': float(x), 'y': float(y), \n                'type': 'corner', 'confidence': 0.5\n            })\n    \n    return keypoints\n\n# Test keypoint detection\nkeypoints = detect_keypoints_simple(sample_image)\nprint(f\"Found {len(keypoints)} keypoints\")\n\n# Visualize keypoints\nif keypoints:\n    img = cv2.imread(sample_image)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    ax.imshow(img_rgb)\n    \n    for kp in keypoints[:20]:  # Show first 20 keypoints\n        ax.plot(kp['x'], kp['y'], 'ro', markersize=4)\n    \n    ax.set_title(\"Detected Keypoints\")\n    ax.axis('off')\n    plt.show()\n\nFound 100 keypoints\n\n\n\n\n\n\n\n\n\nLet’s create a comprehensive computer vision analysis pipeline:\n\ndef analyze_image_collection_cv(image_paths, image_ids, max_images=10):\n    \"\"\"Run computer vision analysis on a collection of images.\"\"\"\n    results = []\n    \n    for i, (path, img_id) in enumerate(zip(image_paths[:max_images], image_ids[:max_images])):\n        try:\n            # Basic image info\n            img = cv2.imread(path)\n            if img is None:\n                continue\n                \n            height, width = img.shape[:2]\n            \n            # Face detection\n            faces = detect_faces_opencv(path)\n            \n            # Simple object detection\n            objects = detect_objects_simple(path)\n            \n            # Keypoint detection\n            keypoints = detect_keypoints_simple(path)\n            \n            results.append({\n                'image_id': img_id,\n                'width': width,\n                'height': height,\n                'num_faces': len(faces),\n                'num_objects': len(objects),\n                'num_keypoints': len(keypoints),\n                'faces': faces,\n                'objects': objects,\n                'keypoints': keypoints\n            })\n            \n            print(f\"Processed {img_id}: {len(faces)} faces, {len(objects)} objects, {len(keypoints)} keypoints\")\n            \n        except Exception as e:\n            print(f\"Error processing {path}: {e}\")\n    \n    return results\n\n# Run computer vision analysis on sample images\ncv_results = analyze_image_collection_cv(\n    fsac['med_path'].iloc[:10].tolist(),\n    fsac['filename'].iloc[:10].tolist()\n)\n\n# Create summary DataFrame\ncv_summary = pd.DataFrame([\n    {k: v for k, v in result.items() \n     if k not in ['faces', 'objects', 'keypoints']}\n    for result in cv_results\n])\n\ncv_summary\n\nProcessed 2017877351: 1 faces, 3 objects, 100 keypoints\n\n\n[ WARN:0@2.437] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877359.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@2.437] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877451.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@2.437] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877502.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@2.437] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877452.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@2.438] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877453.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@2.438] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877454.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@2.438] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877476.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@2.438] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877477.jpg'): can't open/read file: check file path/integrity\n[ WARN:0@2.438] global loadsave.cpp:268 findDecoder imread_('data/fsac/med/2017877478.jpg'): can't open/read file: check file path/integrity\n\n\n\n\n\n\n\n\n\nimage_id\nwidth\nheight\nnum_faces\nnum_objects\nnum_keypoints\n\n\n\n\n0\n2017877351\n1024\n715\n1\n3\n100",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "10_image_data.html#embeddings-and-similarity",
    "href": "10_image_data.html#embeddings-and-similarity",
    "title": "10  Image Data",
    "section": "10.5 Embeddings and Similarity",
    "text": "10.5 Embeddings and Similarity\nFor more sophisticated analysis, we can compute image embeddings using pre-trained deep learning models. These embeddings capture high-level semantic features and can be used for similarity analysis and clustering:\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\ndef compute_simple_image_features(image_path):\n    \"\"\"Compute simple statistical features from an image.\"\"\"\n    img = cv2.imread(image_path)\n    if img is None:\n        return None\n    \n    # Convert to different color spaces\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    features = {}\n    \n    # Color statistics\n    for i, color in enumerate(['red', 'green', 'blue']):\n        features[f'{color}_mean'] = np.mean(img_rgb[:, :, i])\n        features[f'{color}_std'] = np.std(img_rgb[:, :, i])\n    \n    # HSV statistics\n    for i, channel in enumerate(['hue', 'saturation', 'value']):\n        features[f'{channel}_mean'] = np.mean(img_hsv[:, :, i])\n        features[f'{channel}_std'] = np.std(img_hsv[:, :, i])\n    \n    # Texture features (simple)\n    features['brightness_mean'] = np.mean(img_gray)\n    features['brightness_std'] = np.std(img_gray)\n    \n    # Edge density\n    edges = cv2.Canny(img_gray, 50, 150)\n    features['edge_density'] = np.sum(edges &gt; 0) / (edges.shape[0] * edges.shape[1])\n    \n    return features\n\ndef compute_collection_features(image_paths, image_ids, max_images=50):\n    \"\"\"Compute features for a collection of images.\"\"\"\n    feature_list = []\n    \n    for path, img_id in zip(image_paths[:max_images], image_ids[:max_images]):\n        features = compute_simple_image_features(path)\n        if features:\n            features['image_id'] = img_id\n            feature_list.append(features)\n    \n    return pd.DataFrame(feature_list)\n\nfeature_df = compute_collection_features(\n    fsac['thm_path'].iloc[:50].tolist(),\n    fsac['filename'].iloc[:50].tolist()\n)\nfeature_df\n\n\n\n\n\n\n\n\nred_mean\nred_std\ngreen_mean\ngreen_std\nblue_mean\nblue_std\nhue_mean\nhue_std\nsaturation_mean\nsaturation_std\nvalue_mean\nvalue_std\nbrightness_mean\nbrightness_std\nedge_density\nimage_id\n\n\n\n\n0\n48.400508\n41.475860\n49.190667\n40.614180\n42.465016\n41.579156\n39.775937\n31.769777\n53.497968\n52.802824\n51.033778\n41.706693\n48.188381\n40.814655\n0.113206\n2017877351\n\n\n1\n81.737016\n34.318035\n92.810222\n33.641454\n107.396762\n52.315815\n91.775365\n47.634042\n90.064635\n53.936588\n116.089206\n42.968344\n91.170984\n33.932754\n0.167683\n2017877359\n\n\n2\n97.744050\n70.886472\n91.299315\n75.889849\n82.681433\n75.932099\n29.883863\n26.440304\n73.054891\n51.338140\n99.904984\n73.569285\n92.242866\n74.179275\n0.158442\n2017877451\n\n\n3\n73.825016\n51.435218\n70.491238\n50.981097\n65.652190\n60.002446\n45.058286\n40.352350\n95.624317\n53.756969\n81.643937\n59.347840\n70.930286\n51.548233\n0.248254\n2017877502\n\n\n4\n114.542704\n70.704723\n102.673774\n73.155934\n98.347862\n83.224801\n45.060189\n51.326862\n79.936541\n53.647151\n118.026289\n75.601807\n105.729497\n73.363660\n0.167044\n2017877452\n\n\n5\n90.941807\n41.897665\n75.283427\n39.747231\n66.043676\n42.456560\n18.371464\n30.693423\n85.291464\n46.579038\n90.954704\n41.917770\n78.925857\n40.368988\n0.184299\n2017877453\n\n\n6\n104.533019\n59.477602\n86.994214\n55.888445\n85.665472\n64.829710\n67.479434\n72.607520\n77.228113\n50.234260\n104.647044\n59.514555\n92.084906\n57.737912\n0.168302\n2017877454\n\n\n7\n78.401761\n58.667437\n77.667044\n49.386170\n63.018994\n47.942323\n45.443270\n29.527433\n82.890440\n45.851045\n86.798868\n57.863629\n76.222767\n50.998238\n0.235094\n2017877476\n\n\n8\n85.103689\n44.593655\n66.680518\n36.546528\n46.175987\n31.868088\n17.351974\n6.734589\n127.793010\n39.809223\n85.212492\n44.509765\n69.849579\n38.230333\n0.198317\n2017877477\n\n\n9\n103.995107\n42.500780\n86.104343\n27.884789\n34.435535\n17.689883\n24.068746\n9.131813\n174.141468\n31.383043\n108.034862\n38.775356\n85.568379\n29.842472\n0.270887\n2017877478\n\n\n10\n46.029221\n31.148973\n40.476822\n31.229591\n33.451963\n33.979990\n29.774891\n31.326865\n104.225545\n58.087727\n46.894143\n32.561435\n41.329533\n31.331805\n0.219502\n2017877464\n\n\n11\n88.117547\n58.875582\n99.467233\n48.976304\n67.708302\n46.038911\n42.815346\n23.450040\n133.517233\n51.010242\n105.664465\n53.483034\n92.440000\n49.643096\n0.178616\n2017877465\n\n\n12\n80.464403\n53.644418\n79.787925\n49.894952\n81.904214\n48.724543\n58.980503\n47.347917\n97.597673\n58.791086\n95.154214\n52.215025\n80.216415\n50.056749\n0.141509\n2017877360\n\n\n13\n56.463585\n58.369474\n47.968176\n55.052475\n39.383522\n47.941876\n25.389119\n41.353267\n103.409623\n51.688912\n56.499560\n58.359475\n49.513648\n55.166156\n0.174969\n2017877466\n\n\n14\n51.283333\n20.744105\n42.429811\n18.389210\n34.882893\n20.235963\n17.420000\n17.254465\n92.220314\n27.652024\n52.006352\n22.368386\n44.225157\n19.091725\n0.138931\n2017877342\n\n\n15\n30.806791\n20.283864\n33.733333\n19.997104\n26.770218\n19.833903\n46.528349\n22.967456\n80.602181\n38.444274\n34.837009\n21.188158\n32.065047\n19.762150\n0.154766\n2017877467\n\n\n16\n140.041132\n83.657134\n116.557987\n69.169553\n83.198553\n60.791438\n22.443019\n21.081217\n115.921195\n48.575539\n141.748868\n81.713885\n119.781258\n71.880525\n0.178113\n2017877468\n\n\n17\n29.393832\n26.267049\n32.070093\n20.981113\n20.809533\n14.670913\n46.635140\n29.843974\n117.120935\n56.957853\n35.202243\n25.740949\n29.992835\n20.992187\n0.140810\n2017877469\n\n\n18\n114.696699\n64.524285\n86.043107\n51.108452\n53.625761\n39.613265\n20.471650\n17.487386\n143.762265\n47.188142\n116.154693\n62.774065\n90.923430\n52.586304\n0.225178\n2017877479\n\n\n19\n133.595937\n79.780805\n108.270032\n69.069737\n84.055873\n61.292142\n16.655429\n6.647568\n112.837587\n48.741106\n133.745587\n79.664680\n113.081905\n71.063085\n0.241778\n2017877480\n\n\n20\n121.080312\n61.701089\n91.278505\n50.547611\n68.514393\n43.820969\n14.248474\n5.178310\n123.070530\n39.742824\n121.091215\n61.683820\n97.595888\n52.810299\n0.243489\n2017877481\n\n\n21\n63.123654\n41.499078\n67.705000\n38.578434\n42.180256\n34.951649\n38.582821\n13.073875\n119.070833\n52.056702\n69.988077\n40.957797\n63.436154\n38.553044\n0.173013\n2017877470\n\n\n22\n112.844843\n75.487786\n93.777296\n72.134660\n80.970000\n80.597369\n44.146541\n58.727533\n104.437673\n61.520492\n113.003333\n75.465808\n98.015220\n73.709413\n0.194340\n2017877343\n\n\n23\n83.955409\n47.792707\n87.647484\n54.406412\n95.537107\n71.284159\n70.519497\n45.667125\n82.656792\n50.236148\n103.149057\n65.184305\n87.420314\n54.102078\n0.252390\n2017877361\n\n\n24\n104.506231\n55.376375\n92.864735\n41.541625\n53.817695\n39.107500\n30.355701\n14.916474\n136.764860\n47.032520\n111.503988\n49.272488\n91.906355\n43.659127\n0.156075\n2017877455\n\n\n25\n131.048553\n72.518141\n118.334025\n69.348181\n102.083333\n73.681824\n16.896855\n6.660344\n84.622138\n57.147190\n131.070755\n72.499301\n120.282013\n70.675849\n0.145094\n2017877471\n\n\n26\n88.017524\n36.580735\n93.506921\n29.687433\n65.010857\n25.655225\n41.531111\n23.905056\n110.723175\n37.161830\n104.220000\n29.348683\n88.615429\n28.466124\n0.094032\n2017877344\n\n\n27\n118.074167\n50.137306\n90.503910\n44.484185\n93.987372\n55.462395\n61.407821\n69.468353\n81.986346\n40.526672\n122.123846\n54.057065\n99.156859\n46.321126\n0.175769\n2017877456\n\n\n28\n107.324969\n51.047631\n82.669434\n49.075978\n73.490314\n59.307326\n42.140000\n53.256418\n110.607987\n58.586672\n110.083648\n54.318929\n89.003585\n49.756400\n0.199937\n2017877345\n\n\n29\n85.194032\n55.983384\n53.284762\n40.397759\n51.987746\n39.404361\n64.951683\n75.012786\n122.258222\n51.126180\n87.053651\n56.545085\n62.678222\n43.669581\n0.248698\n2017877457\n\n\n30\n108.352516\n79.922961\n95.744780\n76.768345\n96.656226\n85.737011\n58.679874\n67.826045\n66.842138\n46.196735\n108.805975\n80.528130\n99.629748\n78.684406\n0.157107\n2017877458\n\n\n31\n131.498679\n61.323750\n105.265849\n53.244667\n90.536792\n56.800095\n30.496604\n42.434521\n91.494403\n47.751074\n132.156855\n61.293471\n111.435723\n55.296977\n0.229057\n2017877459\n\n\n32\n45.761761\n27.493175\n38.016478\n26.769528\n28.693270\n27.423319\n17.626226\n7.086064\n119.520189\n57.403468\n45.775535\n27.493985\n39.232138\n26.905713\n0.212390\n2017877472\n\n\n33\n80.010692\n54.969323\n62.417987\n51.127407\n58.610252\n54.909580\n28.257736\n52.926363\n87.285409\n40.673930\n80.068553\n55.012647\n67.255975\n52.453130\n0.194969\n2017877460\n\n\n34\n99.555389\n53.394896\n100.796573\n54.881495\n85.679315\n55.164719\n34.291526\n10.411720\n65.177695\n54.740345\n102.049470\n54.370190\n98.713209\n54.386002\n0.264860\n2017877362\n\n\n35\n84.579231\n45.528596\n93.027372\n52.577467\n111.196538\n63.506668\n86.903077\n42.282229\n87.402885\n32.531759\n115.887308\n59.344075\n92.579551\n51.261890\n0.088333\n2017877346\n\n\n36\n90.322804\n43.053227\n94.376760\n50.783611\n97.602430\n61.829366\n62.398754\n45.545427\n79.625358\n34.789595\n109.144611\n56.291571\n93.536075\n49.034670\n0.140000\n2017877347\n\n\n37\n88.765309\n43.570880\n76.093765\n42.608659\n62.137593\n44.942232\n37.559753\n39.817389\n108.256481\n72.285519\n90.678086\n44.265872\n78.307160\n42.365317\n0.182469\n2017877348\n\n\n38\n105.534642\n86.401182\n92.838442\n84.042813\n80.757570\n76.714898\n27.611215\n44.547005\n105.054829\n69.975641\n105.564424\n86.373293\n95.252150\n83.825594\n0.171589\n2017877473\n\n\n39\n92.470818\n67.667208\n71.538365\n64.420894\n64.385723\n64.140160\n38.828491\n63.431995\n106.236604\n59.359783\n92.507296\n67.636949\n76.988302\n64.900958\n0.245157\n2017877349\n\n\n40\n130.924112\n72.598522\n105.936822\n59.958414\n68.812773\n52.343166\n20.118069\n9.414985\n134.368660\n42.538129\n132.216449\n70.958982\n109.182056\n62.398751\n0.248536\n2017877474\n\n\n41\n83.497072\n61.303352\n76.018380\n59.838535\n60.752773\n60.077649\n25.000561\n20.286489\n90.802430\n43.277838\n83.862804\n61.846403\n76.500748\n60.107136\n0.249782\n2017877482\n\n\n42\n70.171006\n57.888985\n59.330063\n53.629693\n41.949811\n48.892699\n18.583648\n6.622317\n123.992956\n43.570178\n70.174465\n57.892657\n60.553459\n54.240271\n0.180943\n2017877567\n\n\n43\n94.099748\n56.198674\n87.077044\n56.124668\n76.402516\n59.515188\n27.279748\n31.800421\n71.458805\n44.877237\n94.490881\n56.308112\n87.936918\n56.484562\n0.243648\n2017877568\n\n\n44\n168.825109\n73.687059\n153.781059\n74.056368\n145.944486\n84.331388\n63.854330\n69.786665\n58.674579\n55.574706\n169.404112\n73.941302\n157.375514\n74.742409\n0.141994\n2017877503\n\n\n45\n76.960127\n36.188942\n86.720952\n40.734122\n111.875683\n58.086648\n84.822794\n43.809358\n95.568254\n42.008150\n115.743492\n54.796816\n86.664317\n40.626921\n0.096698\n2017877363\n\n\n46\n133.133580\n89.401335\n119.274877\n91.495096\n111.279815\n93.325801\n15.625370\n27.375764\n84.168580\n74.792512\n133.177778\n89.336956\n122.491235\n90.884541\n0.111235\n2017877504\n\n\n47\n111.843899\n71.251858\n101.079748\n76.245654\n93.466415\n79.543390\n24.982893\n32.840163\n77.272327\n67.767440\n112.257547\n71.599541\n103.443836\n74.930240\n0.182767\n2017877505\n\n\n48\n119.622099\n58.099497\n115.272840\n60.504509\n110.890062\n66.012334\n69.721358\n48.616354\n48.421111\n53.642553\n123.180432\n59.428493\n116.087778\n60.107041\n0.119630\n2017877506\n\n\n49\n114.958069\n79.891562\n106.626044\n85.110892\n98.143115\n87.225182\n33.995265\n34.686077\n80.942181\n74.002898\n116.392835\n81.056980\n108.146293\n83.496321\n0.113707\n2017877507\n\n\n\n\n\n\n\nNow let’s apply dimensionality reduction to visualize image similarities:\n\n# Prepare features for PCA\nfeature_columns = [col for col in feature_df.columns if col != 'image_id']\nX = feature_df[feature_columns].fillna(0)\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Create PCA results DataFrame\npca_results = pd.DataFrame({\n    'image_id': feature_df['image_id'],\n    'pca_1': X_pca[:, 0],\n    'pca_2': X_pca[:, 1]\n})\n\npca_results\n\n\n\n\n\n\n\n\nimage_id\npca_1\npca_2\n\n\n\n\n0\n2017877351\n-3.127072\n1.665120\n\n\n1\n2017877359\n-0.517014\n2.935728\n\n\n2\n2017877451\n2.182675\n-0.230172\n\n\n3\n2017877502\n-0.764665\n-0.001976\n\n\n4\n2017877452\n3.276433\n0.628718\n\n\n5\n2017877453\n-1.626051\n-0.114354\n\n\n6\n2017877454\n1.235817\n2.093015\n\n\n7\n2017877476\n-0.766439\n-0.115455\n\n\n8\n2017877477\n-2.806650\n-1.711391\n\n\n9\n2017877478\n-3.120399\n-2.976461\n\n\n10\n2017877464\n-4.592952\n-0.079803\n\n\n11\n2017877465\n-0.264505\n-0.788520\n\n\n12\n2017877360\n-0.278584\n1.376681\n\n\n13\n2017877466\n-2.038128\n-0.372860\n\n\n14\n2017877342\n-5.865411\n0.374106\n\n\n15\n2017877467\n-6.333913\n1.437603\n\n\n16\n2017877468\n3.392902\n-2.002574\n\n\n17\n2017877469\n-6.191182\n0.862197\n\n\n18\n2017877479\n-0.156366\n-2.541662\n\n\n19\n2017877480\n2.828061\n-2.782250\n\n\n20\n2017877481\n0.175330\n-2.611386\n\n\n21\n2017877470\n-3.040966\n-0.487636\n\n\n22\n2017877343\n2.840998\n-0.069829\n\n\n23\n2017877361\n0.740963\n1.222461\n\n\n24\n2017877455\n-0.898692\n-1.308494\n\n\n25\n2017877471\n3.441209\n-1.187316\n\n\n26\n2017877344\n-2.383073\n0.805373\n\n\n27\n2017877456\n0.752780\n1.947859\n\n\n28\n2017877345\n0.256852\n0.211766\n\n\n29\n2017877457\n-1.645298\n0.660495\n\n\n30\n2017877458\n3.486211\n1.530484\n\n\n31\n2017877459\n1.792946\n-0.527547\n\n\n32\n2017877472\n-5.386289\n-1.103713\n\n\n33\n2017877460\n-1.076615\n0.246536\n\n\n34\n2017877362\n0.611694\n-0.668645\n\n\n35\n2017877346\n0.681692\n2.974980\n\n\n36\n2017877347\n0.357926\n2.017856\n\n\n37\n2017877348\n-1.173296\n0.134348\n\n\n38\n2017877473\n3.401338\n-1.014099\n\n\n39\n2017877349\n0.772427\n-0.348489\n\n\n40\n2017877474\n1.598779\n-2.943730\n\n\n41\n2017877482\n-0.273172\n-1.280374\n\n\n42\n2017877567\n-1.910482\n-1.780760\n\n\n43\n2017877568\n0.249913\n-0.383483\n\n\n44\n2017877503\n6.551172\n2.086539\n\n\n45\n2017877363\n-0.215369\n3.046338\n\n\n46\n2017877504\n5.726072\n-0.918033\n\n\n47\n2017877505\n3.097225\n-0.485688\n\n\n48\n2017877506\n2.728169\n2.541285\n\n\n49\n2017877507\n4.272999\n0.037212\n\n\n\n\n\n\n\nLet’s visualize the PCA results:\n\n# Create PCA visualization with actual images\nfig, ax = plt.subplots(1, 1, figsize=(15, 12))\n\n# Plot points\nscatter = ax.scatter(pca_results['pca_1'], pca_results['pca_2'], alpha=0.7, s=100)\n\n# Add a few sample images to the plot\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n# Select a few representative points\nsample_indices = np.random.choice(len(pca_results), size=min(12, len(pca_results)), replace=False)\n\nfor idx in sample_indices:\n    row = pca_results.iloc[idx]\n    img_path = fsac[fsac['filename'] == row['image_id']]['thm_path'].iloc[0]\n    \n    try:\n        img = plt.imread(img_path)\n        \n        # Resize image for display\n        img_resized = cv2.resize(img, (50, 50))\n        \n        imagebox = OffsetImage(img_resized, zoom=1)\n        ab = AnnotationBbox(imagebox, (row['pca_1'], row['pca_2']), frameon=False)\n        ax.add_artist(ab)\n    except:\n        # If image loading fails, just plot a point\n        ax.plot(row['pca_1'], row['pca_2'], 'rx', markersize=8)\n\nax.set_xlabel(f'First Principal Component (explains {pca.explained_variance_ratio_[0]:.1%} of variance)')\nax.set_ylabel(f'Second Principal Component (explains {pca.explained_variance_ratio_[1]:.1%} of variance)')\nax.set_title('Image Collection Visualization using PCA')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFor comparison, let’s also try UMAP:\n# Apply UMAP for comparison\numap_reducer = umap.UMAP(n_components=2, random_state=42)\nX_umap = umap_reducer.fit_transform(X_scaled)\n\numap_results = pd.DataFrame({\n    'image_id': feature_df['image_id'],\n    'umap_1': X_umap[:, 0],\n    'umap_2': X_umap[:, 1]\n})\n\nprint(\"UMAP results:\")\nprint(umap_results.head())\n\n# Plot UMAP results\np_umap = (ggplot(umap_results, aes(x='umap_1', y='umap_2')) +\n          geom_point(size=3, alpha=0.7) +\n          labs(title=\"Image Collection UMAP Visualization\",\n               x=\"UMAP Dimension 1\", y=\"UMAP Dimension 2\") +\n          theme_minimal())\n\np_umap\nFinally, let’s create a simple image similarity function:\ndef find_similar_images(target_image_id, feature_df, pca_results, n_similar=5):\n    \"\"\"Find images similar to a target image based on PCA coordinates.\"\"\"\n    from sklearn.metrics.pairwise import euclidean_distances\n    \n    # Get target image features\n    target_row = pca_results[pca_results['image_id'] == target_image_id]\n    if target_row.empty:\n        return None\n    \n    target_coords = target_row[['pca_1', 'pca_2']].values\n    all_coords = pca_results[['pca_1', 'pca_2']].values\n    \n    # Compute distances\n    distances = euclidean_distances(target_coords, all_coords)[0]\n    \n    # Get indices of most similar images (excluding the target itself)\n    similar_indices = np.argsort(distances)[1:n_similar+1]\n    \n    similar_images = pca_results.iloc[similar_indices]\n    similar_images['distance'] = distances[similar_indices]\n    \n    return similar_images\n\n# Find similar images for the first image\ntarget_id = feature_df['image_id'].iloc[0]\nsimilar = find_similar_images(target_id, feature_df, pca_results)\n\nprint(f\"Images similar to {target_id}:\")\nprint(similar)\n\n# Display the target and similar images\nif similar is not None:\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    # Show target image first\n    target_path = fsac[fsac['filename'] == target_id]['thm_path'].iloc[0]\n    target_img = plt.imread(target_path)\n    axes[0].imshow(target_img)\n    axes[0].set_title(f\"Target: {target_id}\")\n    axes[0].axis('off')\n    \n    # Show similar images\n    for i, (_, row) in enumerate(similar.iterrows()):\n        if i &gt;= 5:  # Only show 5 similar images\n            break\n        img_path = fsac[fsac['filename'] == row['image_id']]['thm_path'].iloc[0]\n        img = plt.imread(img_path)\n        axes[i+1].imshow(img)\n        axes[i+1].set_title(f\"Similar: {row['image_id'][:8]}\\nDist: {row['distance']:.2f}\")\n        axes[i+1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "10_image_data.html#extensions",
    "href": "10_image_data.html#extensions",
    "title": "10  Image Data",
    "section": "10.6 Extensions",
    "text": "10.6 Extensions\nIn this chapter we have shown how Python’s rich ecosystem enables direct analysis of image collections without requiring external wrappers. Our focus has been on practical techniques for extracting meaningful information from visual data using established computer vision and machine learning approaches.\nFor deeper computer vision work:\n\nOpenCV provides comprehensive computer vision functionality\nMediaPipe offers robust pose detection, face mesh, and holistic analysis\nscikit-image has extensive image processing capabilities\nPillow (PIL) handles image I/O and basic manipulations\n\nFor advanced deep learning:\n\nPyTorch and torchvision provide state-of-the-art pre-trained models\nTensorFlow/Keras offers alternative deep learning frameworks\nYOLO, RCNN, and Detectron2 for object detection\nFaceNet, ArcFace for face recognition\nOpenPose, MediaPipe for pose estimation\n\nFor large-scale analysis:\n\nDask for processing image collections larger than memory\nRay for distributed computing across multiple machines\nHugging Face Transformers for vision transformer models\nCLIP for text-image similarity\n\nThe Python ecosystem provides unparalleled capabilities for image analysis that continue to evolve rapidly. Unlike the R version that required Python wrappers, Python offers direct access to cutting-edge computer vision research and production-ready tools.\nFor theoretical background, we recommend our Distant Viewing [1] text as well as Lev Manovich’s Cultural Analytics [3]. For technical deep learning background, see Deep Learning by Ian Goodfellow [4] and Computer Vision: Algorithms and Applications by Richard Szeliski [5].\nThe combination of Python’s mature scientific computing ecosystem with rapidly advancing computer vision research makes it an ideal platform for computational analysis of visual culture, art history, media studies, and other image-rich humanities domains.",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "10_image_data.html#references",
    "href": "10_image_data.html#references",
    "title": "10  Image Data",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Arnold, T and Tilton, L (2023 ). Distant Viewing: Computational Exploration of Digital Images. MIT Press\n\n\n[2] Bermeitinger, B, Gassner, S, Handschuh, S, Howanitz, G, Radisch, E and Rehbein, M (2019 ). Deep watching: Towards new methods of analyzing visual media in cultural studies. Proceedings of ADHO DH Conference\n\n\n[3] Manovich, L (2020 ). Cultural Analytics. Mit Press\n\n\n[4] Goodfellow, I, Bengio, Y, Courville, A and Bengio, Y (2016 ). Deep Learning. MIT press Cambridge\n\n\n[5] Szeliski, R (2010 ). Computer Vision: Algorithms and Applications. Springer Nature",
    "crumbs": [
      "Part II: Data Types",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "18_program.html",
    "href": "18_program.html",
    "title": "18  Programming",
    "section": "",
    "text": "18.1 Introduction\nThe majority of the functions and methods that have been introduced throughout the first two parts of this text are provided by user-contributed packages. Most of these come from a core set of packages that together comprise the modern Python data science ecosystem. Included in this set of packages are pandas, numpy, matplotlib, and plotnine. Benefits of using these libraries include consistent APIs, excellent documentation, and the fact that they are often built to express theoretical models for data analysis (for example, relational database techniques encoded in pandas and vectorized operations in numpy). Downsides can include their computational overhead for simple operations and the learning curve required to understand their abstractions.\nThere are various opinions about the best approaches to data science in Python, from pure pandas workflows to functional programming approaches to object-oriented designs. We will avoid a lengthy discussion of these debates here. As should be clear at this point, this text has been written with the opinion that pandas and the broader scientific Python ecosystem provide an excellent way to do data analysis and an ideal starting point for learning data science in Python. However, eventually it will be useful to learn the underlying built-in methods available within the Python programming language itself.\nThe functions and data structures available directly from Python without importing any third-party packages, commonly known as built-in Python or core Python, will become particularly important as we learn how to do more complex programming and data scraping within this part of the book. In this chapter we will restart from the very basics by describing the fundamental data types and objects within Python. These topics will be made easier by the fact that we have seen many of them indirectly in the preceding chapters. We will also provide an overview of introductory computer science concepts such as control flow and function definition. The material is intended for readers who had no prior programming experience.",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "18_program.html#data-structures",
    "href": "18_program.html#data-structures",
    "title": "18  Programming",
    "section": "18.2 Data Structures",
    "text": "18.2 Data Structures\nA list is one of the most fundamental and versatile data structures in Python. It consists of an ordered sequence of values that can be of different types. Unlike some other languages, Python lists can contain a mixture of numbers, strings, and other objects. To create a list, we use square brackets with comma-separated values:\n\nlist_example = [1, 2, 3, 4, 10, 20]\nlist_example\n\n[1, 2, 3, 4, 10, 20]\n\n\nWe should recognize the square bracket notation from our work with pandas, where we used it for selecting columns and filtering data. Lists are fundamental to Python and appear throughout data science workflows.\nMathematical operations can be applied to lists, though the behavior is different from what we might expect coming from R or working with pandas. Adding a number to a list will produce an error, but we can use list comprehensions (which we’ll see later) or convert to NumPy arrays for element-wise operations:\n\n# This would cause an error:\n# list_example + 1\n\n# Instead, we can use a list comprehension:\nresult = [x + 1 for x in list_example]\nresult\n\n[2, 3, 4, 5, 11, 21]\n\n\nWe can also combine lists using the + operator, which concatenates them:\n\nlist1 = [1, 2, 3]\nlist2 = [4, 5, 6]\ncombined = list1 + list2\ncombined\n\n[1, 2, 3, 4, 5, 6]\n\n\nLists can be multiplied by integers to repeat their contents:\n\nrepeated = [1, 2] * 3\nrepeated\n\n[1, 2, 1, 2, 1, 2]\n\n\nPython also has other built-in sequence types. Tuples are similar to lists but are immutable (cannot be changed after creation). They use parentheses:\n\ntuple_example = (1, 2, 3, 4, 10, 20)\ntuple_example\n\n(1, 2, 3, 4, 10, 20)\n\n\nStrings are also sequences in Python, consisting of characters:\n\nstring_example = \"Hello, World!\"\nstring_example\n\n'Hello, World!'\n\n\nPython has several built-in scalar data types that we work with regularly. We can check the type of any object using the type() function or isinstance() for more sophisticated type checking. Here are the main built-in types:\nNumeric types:\n\n# Integers\ninteger_example = 42\nprint(f\"Integer: {integer_example}, type: {type(integer_example)}\")\n\n# Floats (decimal numbers)\nfloat_example = 3.14159\nprint(f\"Float: {float_example}, type: {type(float_example)}\")\n\n# Complex numbers\ncomplex_example = 3 + 4j\nprint(f\"Complex: {complex_example}, type: {type(complex_example)}\")\n\nInteger: 42, type: &lt;class 'int'&gt;\nFloat: 3.14159, type: &lt;class 'float'&gt;\nComplex: (3+4j), type: &lt;class 'complex'&gt;\n\n\nBoolean and string types:\n\n# Booleans\nbool_example = True\nprint(f\"Boolean: {bool_example}, type: {type(bool_example)}\")\n\n# Strings\nstr_example = \"Data Science\"\nprint(f\"String: {str_example}, type: {type(str_example)}\")\n\nBoolean: True, type: &lt;class 'bool'&gt;\nString: Data Science, type: &lt;class 'str'&gt;\n\n\nCollections:\n\n# Lists (mutable sequences)\nlist_example = [1, 2, 3]\nprint(f\"List: {list_example}, type: {type(list_example)}\")\n\n# Tuples (immutable sequences)\ntuple_example = (1, 2, 3)\nprint(f\"Tuple: {tuple_example}, type: {type(tuple_example)}\")\n\n# Dictionaries (key-value pairs)\ndict_example = {\"name\": \"Alice\", \"age\": 30}\nprint(f\"Dict: {dict_example}, type: {type(dict_example)}\")\n\n# Sets (unique elements)\nset_example = {1, 2, 3, 3, 2}  # Note: duplicates are removed\nprint(f\"Set: {set_example}, type: {type(set_example)}\")\n\nList: [1, 2, 3], type: &lt;class 'list'&gt;\nTuple: (1, 2, 3), type: &lt;class 'tuple'&gt;\nDict: {'name': 'Alice', 'age': 30}, type: &lt;class 'dict'&gt;\nSet: {1, 2, 3}, type: &lt;class 'set'&gt;\n\n\nData types are important because they determine what operations we can perform. For example, we can add numbers together but not a number and a string directly:\n\n# This works:\nresult1 = 5 + 3\nprint(f\"Number addition: {result1}\")\n\n# This would cause an error:\ntry:\n    result2 = \"5\" + 3\nexcept TypeError as e:\n    print(f\"Error: {e}\")\n\n# But this works (string concatenation):\nresult3 = \"5\" + \"3\"\nprint(f\"String concatenation: {result3}\")\n\nNumber addition: 8\nError: can only concatenate str (not \"int\") to str\nString concatenation: 53\n\n\nThe concept of data types should seem familiar as we used the same idea when describing the types of data stored in pandas DataFrame columns (which are called “dtypes” in pandas).\nDictionaries deserve special attention as they’re fundamental to Python and very useful for data science. They store key-value pairs and are similar to what other languages might call “hash maps” or “associative arrays”:\n\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\",\n    \"occupation\": \"Data Scientist\"\n}\n\nprint(person)\nprint(f\"Name: {person['name']}\")\nprint(f\"Keys: {list(person.keys())}\")\nprint(f\"Values: {list(person.values())}\")\n\n{'name': 'Alice', 'age': 30, 'city': 'New York', 'occupation': 'Data Scientist'}\nName: Alice\nKeys: ['name', 'age', 'city', 'occupation']\nValues: ['Alice', 30, 'New York', 'Data Scientist']",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "18_program.html#indexing-and-slicing",
    "href": "18_program.html#indexing-and-slicing",
    "title": "18  Programming",
    "section": "18.3 Indexing and Slicing",
    "text": "18.3 Indexing and Slicing\nOne of the most important differences between Python and R is that Python uses zero-based indexing. This means the first element of a sequence is at position 0, not position 1. Let’s explore how to select and modify elements in Python sequences:\n\n# Create an example list\ndata = [10, 20, 30, 40, 50, 60]\nprint(f\"Original list: {data}\")\n\n# Access single elements (zero-based indexing)\nprint(f\"First element (index 0): {data[0]}\")\nprint(f\"Third element (index 2): {data[2]}\")\nprint(f\"Last element (index -1): {data[-1]}\")\nprint(f\"Second to last (index -2): {data[-2]}\")\n\nOriginal list: [10, 20, 30, 40, 50, 60]\nFirst element (index 0): 10\nThird element (index 2): 30\nLast element (index -1): 60\nSecond to last (index -2): 50\n\n\nSlicing allows us to select ranges of elements using the syntax [start:stop:step]:\n\n# Slicing examples\nprint(f\"First three elements [0:3]: {data[0:3]}\")\nprint(f\"Elements from index 2 onward [2:]: {data[2:]}\")\nprint(f\"Elements up to index 4 [:4]: {data[:4]}\")\nprint(f\"Every second element [::2]: {data[::2]}\")\nprint(f\"Reverse the list [::-1]: {data[::-1]}\")\n\nFirst three elements [0:3]: [10, 20, 30]\nElements from index 2 onward [2:]: [30, 40, 50, 60]\nElements up to index 4 [:4]: [10, 20, 30, 40]\nEvery second element [::2]: [10, 30, 50]\nReverse the list [::-1]: [60, 50, 40, 30, 20, 10]\n\n\nWe can modify elements in lists (but not tuples, which are immutable):\n\n# Modify single elements\ndata[1] = 100\nprint(f\"After changing index 1: {data}\")\n\n# Modify slices\ndata[2:4] = [300, 400]\nprint(f\"After changing slice [2:4]: {data}\")\n\n# Add elements\ndata.append(70)\nprint(f\"After appending 70: {data}\")\n\nAfter changing index 1: [10, 100, 30, 40, 50, 60]\nAfter changing slice [2:4]: [10, 100, 300, 400, 50, 60]\nAfter appending 70: [10, 100, 300, 400, 50, 60, 70]\n\n\nList comprehensions provide a powerful way to create new lists based on existing ones:\n\n# Create a new list with each element doubled\ndoubled = [x * 2 for x in data]\nprint(f\"Doubled: {doubled}\")\n\n# Create a list with only even numbers\nevens = [x for x in data if x % 2 == 0]\nprint(f\"Even numbers: {evens}\")\n\n# More complex transformations\nsquares_of_evens = [x**2 for x in data if x % 2 == 0]\nprint(f\"Squares of even numbers: {squares_of_evens}\")\n\nDoubled: [20, 200, 600, 800, 100, 120, 140]\nEven numbers: [10, 100, 300, 400, 50, 60, 70]\nSquares of even numbers: [100, 10000, 90000, 160000, 2500, 3600, 4900]\n\n\nWorking with dictionaries:\n\n# Dictionary access and modification\nperson = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\n\n# Access values\nprint(f\"Name: {person['name']}\")\nprint(f\"Age using get(): {person.get('age', 'Unknown')}\")\n\n# Modify values\nperson['age'] = 31\nperson['country'] = 'USA'  # Add new key-value pair\nprint(f\"Updated person: {person}\")\n\n# Dictionary comprehensions\nsquared_dict = {k: v**2 for k, v in {\"a\": 1, \"b\": 2, \"c\": 3}.items()}\nprint(f\"Squared dictionary: {squared_dict}\")\n\nName: Alice\nAge using get(): 30\nUpdated person: {'name': 'Alice', 'age': 31, 'city': 'New York', 'country': 'USA'}\nSquared dictionary: {'a': 1, 'b': 4, 'c': 9}",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "18_program.html#numpy-arrays",
    "href": "18_program.html#numpy-arrays",
    "title": "18  Programming",
    "section": "18.4 NumPy Arrays",
    "text": "18.4 NumPy Arrays\nWhile Python’s built-in lists are versatile, they’re not optimized for numerical computation. When working with data science, we often use NumPy arrays, which are similar to R’s vectors and matrices. NumPy arrays are homogeneous (all elements have the same type) and support efficient element-wise operations:\n\nimport numpy as np\n\n# Create arrays from lists\narray_example = np.array([1, 2, 3, 4, 10, 20])\nprint(f\"NumPy array: {array_example}\")\nprint(f\"Type: {type(array_example)}\")\nprint(f\"Data type: {array_example.dtype}\")\n\nNumPy array: [ 1  2  3  4 10 20]\nType: &lt;class 'numpy.ndarray'&gt;\nData type: int64\n\n\nMathematical operations work element-wise on NumPy arrays:\n\n# Element-wise operations\nprint(f\"Add 1: {array_example + 1}\")\nprint(f\"Multiply by 2: {array_example * 2}\")\nprint(f\"Square: {array_example ** 2}\")\n\n# Operations between arrays\narray2 = np.array([1, 1, 1, 1, 2, 2])\nprint(f\"Add arrays: {array_example + array2}\")\nprint(f\"Multiply arrays: {array_example * array2}\")\n\nAdd 1: [ 2  3  4  5 11 21]\nMultiply by 2: [ 2  4  6  8 20 40]\nSquare: [  1   4   9  16 100 400]\nAdd arrays: [ 2  3  4  5 12 22]\nMultiply arrays: [ 1  2  3  4 20 40]\n\n\nBoolean operations create boolean arrays:\n\n# Boolean operations\nmask = array_example &gt; 4\nprint(f\"Elements &gt; 4: {mask}\")\nprint(f\"Values where &gt; 4: {array_example[mask]}\")\n\nElements &gt; 4: [False False False False  True  True]\nValues where &gt; 4: [10 20]\n\n\nTwo-dimensional arrays (matrices) are created and manipulated similarly:\n\n# Create a 2D array (matrix)\nmatrix_example = np.array([[1, 2, 3], [4, 5, 6]])\nprint(f\"Matrix shape: {matrix_example.shape}\")\nprint(f\"Matrix:\\n{matrix_example}\")\n\n# Or create using reshape\nmatrix_from_range = np.arange(1, 7).reshape(2, 3)\nprint(f\"Matrix from range:\\n{matrix_from_range}\")\n\nMatrix shape: (2, 3)\nMatrix:\n[[1 2 3]\n [4 5 6]]\nMatrix from range:\n[[1 2 3]\n [4 5 6]]\n\n\nMatrix operations:\n\n# Element-wise operations\nprint(f\"Add 1 to matrix:\\n{matrix_example + 1}\")\nprint(f\"Matrix squared:\\n{matrix_example ** 2}\")\n\n# Selecting rows and columns\nprint(f\"First row: {matrix_example[0, :]}\")\nprint(f\"First column: {matrix_example[:, 0]}\")\nprint(f\"Specific element [1,2]: {matrix_example[1, 2]}\")\n\nAdd 1 to matrix:\n[[2 3 4]\n [5 6 7]]\nMatrix squared:\n[[ 1  4  9]\n [16 25 36]]\nFirst row: [1 2 3]\nFirst column: [1 4]\nSpecific element [1,2]: 6",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "18_program.html#control-flow",
    "href": "18_program.html#control-flow",
    "title": "18  Programming",
    "section": "18.5 Control Flow",
    "text": "18.5 Control Flow\nWhen writing more complex programs, we need ways to control the execution flow of our code. Python provides several control structures for this purpose.\nFor loops allow us to repeat code for each item in a sequence:\n\n# Simple for loop\ndata = [10, 20, 30, 40]\nprint(\"Original data:\", data)\n\n# Loop through indices\nfor i in range(len(data)):\n    data[i] = data[i] + i + 1  # Add position + 1 to each element\n\nprint(\"After modification:\", data)\n\nOriginal data: [10, 20, 30, 40]\nAfter modification: [11, 22, 33, 44]\n\n\nIf statements allow conditional execution:\n\n# Reset data\ndata = [10, 20, 30, 40]\n\n# Loop with conditional logic\nfor i in range(len(data)):\n    if i &gt; 1:  # Only modify elements at index 2 and beyond\n        data[i] = data[i] + i + 1\n    print(f\"Index {i}: {data[i]}\")\n\nprint(\"Final data:\", data)\n\nIndex 0: 10\nIndex 1: 20\nIndex 2: 33\nIndex 3: 44\nFinal data: [10, 20, 33, 44]\n\n\nIf-else statements provide alternative actions:\n\n# Reset data\ndata = [10, 20, 30, 40]\n\nfor i in range(len(data)):\n    if i &gt;= 2:\n        data[i] = data[i] + i + 1\n    else:\n        data[i] = i + 1  # Set first two elements to index + 1\n    print(f\"Index {i}: {data[i]}\")\n\nprint(\"Final data:\", data)\n\nIndex 0: 1\nIndex 1: 2\nIndex 2: 33\nIndex 3: 44\nFinal data: [1, 2, 33, 44]\n\n\nWhile loops repeat while a condition is true:\n\n# Count down example\ncount = 5\nwhile count &gt; 0:\n    print(f\"Count: {count}\")\n    count -= 1\nprint(\"Done!\")\n\nCount: 5\nCount: 4\nCount: 3\nCount: 2\nCount: 1\nDone!\n\n\nFunctions allow us to package code for reuse:\n\ndef add_one(input_value):\n    \"\"\"Add one to the input value.\"\"\"\n    result = input_value + 1\n    return result\n\n# Test the function\nprint(f\"add_one(30) = {add_one(30)}\")\nprint(f\"add_one(5) = {add_one(5)}\")\n\nadd_one(30) = 31\nadd_one(5) = 6\n\n\nFunctions with default arguments:\n\ndef add_something(input_value, something=1):\n    \"\"\"Add 'something' to the input value. Default is 1.\"\"\"\n    result = input_value + something\n    return result\n\n# Test with and without the optional argument\nprint(f\"add_something(30) = {add_something(30)}\")\nprint(f\"add_something(30, 4) = {add_something(30, 4)}\")\n\nadd_something(30) = 31\nadd_something(30, 4) = 34\n\n\nFunctions with multiple return values:\n\ndef analyze_list(data):\n    \"\"\"Return basic statistics about a list.\"\"\"\n    total = sum(data)\n    count = len(data)\n    average = total / count if count &gt; 0 else 0\n    return total, count, average\n\n# Test the function\nnumbers = [1, 2, 3, 4, 5]\ntotal, count, avg = analyze_list(numbers)\nprint(f\"Total: {total}, Count: {count}, Average: {avg}\")\n\nTotal: 15, Count: 5, Average: 3.0",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "18_program.html#functional-programming",
    "href": "18_program.html#functional-programming",
    "title": "18  Programming",
    "section": "18.6 Functional Programming",
    "text": "18.6 Functional Programming\nPython supports functional programming concepts that allow us to apply functions to collections of data efficiently. This is particularly useful for data processing.\nThe map() function applies a function to each element of a sequence:\n\n# Using map with a function\nnumbers = [1, 2, 3, 4, 5]\nsquared = list(map(lambda x: x**2, numbers))\nprint(f\"Original: {numbers}\")\nprint(f\"Squared: {squared}\")\n\n# Map with a custom function\ndef double_and_add_one(x):\n    return 2 * x + 1\n\nresult = list(map(double_and_add_one, numbers))\nprint(f\"Doubled and add one: {result}\")\n\nOriginal: [1, 2, 3, 4, 5]\nSquared: [1, 4, 9, 16, 25]\nDoubled and add one: [3, 5, 7, 9, 11]\n\n\nLambda functions provide a way to create small anonymous functions:\n\n# Lambda functions for simple operations\nadd_ten = lambda x: x + 10\nprint(f\"Add 10 to 5: {add_ten(5)}\")\n\n# Using lambda with map\ncubed = list(map(lambda x: x**3, numbers))\nprint(f\"Cubed: {cubed}\")\n\nAdd 10 to 5: 15\nCubed: [1, 8, 27, 64, 125]\n\n\nList comprehensions are often more “Pythonic” than map():\n\n# List comprehension equivalents\nsquared_comp = [x**2 for x in numbers]\ncubed_comp = [x**3 for x in numbers]\ndoubled_plus_one = [2*x + 1 for x in numbers]\n\nprint(f\"Squared (comprehension): {squared_comp}\")\nprint(f\"Cubed (comprehension): {cubed_comp}\")\nprint(f\"Doubled plus one: {doubled_plus_one}\")\n\nSquared (comprehension): [1, 4, 9, 16, 25]\nCubed (comprehension): [1, 8, 27, 64, 125]\nDoubled plus one: [3, 5, 7, 9, 11]\n\n\nFilter operations:\n\n# Using filter() function\nevens = list(filter(lambda x: x % 2 == 0, numbers))\nprint(f\"Even numbers: {evens}\")\n\n# List comprehension with condition (more Pythonic)\nevens_comp = [x for x in numbers if x % 2 == 0]\nprint(f\"Even numbers (comprehension): {evens_comp}\")\n\n# More complex filtering\nlarge_evens = [x for x in range(1, 21) if x % 2 == 0 and x &gt; 10]\nprint(f\"Even numbers &gt; 10: {large_evens}\")\n\nEven numbers: [2, 4]\nEven numbers (comprehension): [2, 4]\nEven numbers &gt; 10: [12, 14, 16, 18, 20]\n\n\nWorking with nested data:\n\n# List of lists\ndata_nested = [[1, 2, 3], [4, 5, 6], [7, 8, 9, 10]]\n\n# Calculate sum of each sublist\nsums = [sum(sublist) for sublist in data_nested]\nprint(f\"Sums of sublists: {sums}\")\n\n# Calculate length of each sublist\nlengths = [len(sublist) for sublist in data_nested]\nprint(f\"Lengths of sublists: {lengths}\")\n\n# Flatten nested lists\nflattened = [item for sublist in data_nested for item in sublist]\nprint(f\"Flattened: {flattened}\")\n\nSums of sublists: [6, 15, 34]\nLengths of sublists: [3, 3, 4]\nFlattened: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nDictionary operations:\n\n# Working with dictionaries functionally\nstudents = [\n    {\"name\": \"Alice\", \"grade\": 85},\n    {\"name\": \"Bob\", \"grade\": 92},\n    {\"name\": \"Charlie\", \"grade\": 78},\n    {\"name\": \"Diana\", \"grade\": 96}\n]\n\n# Extract names\nnames = [student[\"name\"] for student in students]\nprint(f\"Names: {names}\")\n\n# Filter high performers\nhigh_performers = [s for s in students if s[\"grade\"] &gt;= 90]\nprint(f\"High performers: {high_performers}\")\n\n# Calculate average grade\naverage_grade = sum(s[\"grade\"] for s in students) / len(students)\nprint(f\"Average grade: {average_grade:.1f}\")\n\nNames: ['Alice', 'Bob', 'Charlie', 'Diana']\nHigh performers: [{'name': 'Bob', 'grade': 92}, {'name': 'Diana', 'grade': 96}]\nAverage grade: 87.8\n\n\nNumPy functional operations (for numerical data):\n\n# NumPy array operations\narr = np.array([1, 2, 3, 4, 5])\n\n# Apply functions element-wise\nprint(f\"Square root: {np.sqrt(arr)}\")\nprint(f\"Exponential: {np.exp(arr)}\")\nprint(f\"Natural log: {np.log(arr)}\")\n\n# Aggregate functions\nprint(f\"Sum: {np.sum(arr)}\")\nprint(f\"Mean: {np.mean(arr)}\")\nprint(f\"Standard deviation: {np.std(arr)}\")\n\n# Apply custom function\ndef custom_transform(x):\n    return x**2 + 2*x + 1\n\ntransformed = np.vectorize(custom_transform)(arr)\nprint(f\"Custom transform: {transformed}\")\n\nSquare root: [1.         1.41421356 1.73205081 2.         2.23606798]\nExponential: [  2.71828183   7.3890561   20.08553692  54.59815003 148.4131591 ]\nNatural log: [0.         0.69314718 1.09861229 1.38629436 1.60943791]\nSum: 15\nMean: 3.0\nStandard deviation: 1.4142135623730951\nCustom transform: [ 4  9 16 25 36]",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "18_program.html#working-with-files-and-data",
    "href": "18_program.html#working-with-files-and-data",
    "title": "18  Programming",
    "section": "18.7 Working with Files and Data",
    "text": "18.7 Working with Files and Data\nPython provides excellent built-in support for file operations and data processing:\n\n# Working with files (example - would need actual file)\n# with open('data.txt', 'r') as file:\n#     content = file.read()\n#     lines = content.split('\\n')\n\n# Simulating file content\nlines = [\"Alice,25,Engineer\", \"Bob,30,Teacher\", \"Charlie,35,Doctor\"]\n\n# Parse CSV-like data\npeople = []\nfor line in lines:\n    parts = line.split(',')\n    person = {\n        'name': parts[0],\n        'age': int(parts[1]),\n        'job': parts[2]\n    }\n    people.append(person)\n\nprint(\"Parsed people:\")\nfor person in people:\n    print(f\"  {person}\")\n\nParsed people:\n  {'name': 'Alice', 'age': 25, 'job': 'Engineer'}\n  {'name': 'Bob', 'age': 30, 'job': 'Teacher'}\n  {'name': 'Charlie', 'age': 35, 'job': 'Doctor'}\n\n\nError handling:\n\ndef safe_divide(a, b):\n    \"\"\"Safely divide two numbers.\"\"\"\n    try:\n        result = a / b\n        return result\n    except ZeroDivisionError:\n        print(\"Error: Cannot divide by zero\")\n        return None\n    except TypeError:\n        print(\"Error: Invalid input types\")\n        return None\n\n# Test error handling\nprint(f\"10 / 2 = {safe_divide(10, 2)}\")\nprint(f\"10 / 0 = {safe_divide(10, 0)}\")\nprint(f\"'10' / 2 = {safe_divide('10', 2)}\")\n\n10 / 2 = 5.0\nError: Cannot divide by zero\n10 / 0 = None\nError: Invalid input types\n'10' / 2 = None",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "18_program.html#extensions",
    "href": "18_program.html#extensions",
    "title": "18  Programming",
    "section": "18.8 Extensions",
    "text": "18.8 Extensions\nWe have given only a brief introduction to core Python programming concepts. There are many excellent resources for learning Python at a much deeper level.\nFor learning core Python: - Python Crash Course by Eric Matthes provides an excellent beginner-friendly introduction - Fluent Python by Luciano Ramalho offers deep insights into Python’s more advanced features - Effective Python by Brett Slatkin provides best practices and idioms\nFor data science specific Python: - Python for Data Analysis by Wes McKinney (creator of pandas) is the definitive guide - Python Data Science Handbook by Jake VanderPlas covers the core scientific Python stack\nOnline resources:\n\nThe official Python tutorial at docs.python.org\nReal Python (realpython.com) for practical tutorials\nPython’s built-in help() function and documentation\n\nThe concepts covered in this chapter - data structures, control flow, functions, and functional programming - form the foundation for more advanced topics like object-oriented programming, decorators, generators, and the async/await paradigm. Understanding these basics will make it much easier to work effectively with pandas, NumPy, and other data science libraries, as well as to write custom functions for data analysis tasks.\nIn the context of data science, these programming fundamentals become particularly important when:\n\nWriting custom data processing functions\nHandling complex data cleaning tasks\nBuilding data pipelines\nCreating reusable analysis code\nDebugging issues in data workflows\n\nThe transition from interactive data analysis to programmatic data science requires comfort with these core programming concepts, which will serve as the foundation for the more advanced topics we’ll cover in subsequent chapters.",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "18_program.html#references",
    "href": "18_program.html#references",
    "title": "18  Programming",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html",
    "href": "19_jsonxml.html",
    "title": "19  JSON + XML",
    "section": "",
    "text": "19.1 Introduction\nIn this text we have primarily worked with tabular data stored in a CSV file. Tabular data is arranged into rows and columns, which we went into further detail in Chap. 5. As we have seen, this format is surprisingly flexible while also keeping the data organized in a way that is optimized for using the grammar of graphics and data manipulation verbs to do exploratory data analysis. Getting data organized for analysis often takes significant time. In fact, it often takes us more time than the analysis. Whether creating our own data or moving between formats, collecting and organizing data is a time consuming, yet key, task.\nThere are many other formats available for storing tabular datasets. We mentioned the use of the Excel format in Chap. 5 as a good option for data entry and we used the special GeoJSON format for storing tabular data along with geospatial information. For most other common tabular data formats there is likely to be at least one Python function or package that is able to read, and in most cases write, tabular data stored in it. The pandas library can load many variations on CSV, such as tables that use other delimiters or fixed-width columns. The geopandas library that we used for GeoJSON will automatically read many other spatial formats such as ESRI shapefiles. For formats created by SAS, SPSS, or Stata, pandas has functions for getting the data into Python DataFrames [1]. If we have one or more data tables in a database, packages such as sqlalchemy and database-specific libraries can query the database and return tables as pandas DataFrames [2] [3]. For other tabular data files that we may run into, searching Python’s package index (PyPI) will usually reveal a package that can help get data into the desired format.\nSometimes, data will be available in a format that is not initially organized into the kinds of tables we introduced in Chap. 1 and have used throughout this book. The most common types of non-tabular data that we may run into include raw text, JSON (Javascript Object Notation), XML (Extensible Markup Language), and HTML (HyperText Markup Language). All of these can be loaded into Python using built-in functions and packages specifically designed to parse them. The challenge, however, is that when read into Python these formats will be in the form of dictionaries, lists, or other custom objects. Parsing these formats into a DataFrame requires writing custom code that takes into account the specific information stored in the data. Often this requires using new functions or query languages, such as regular expressions or XPath queries, to facilitate the conversion process. In this chapter, we will introduce these four common formats and show examples of how they can be used to represent different kinds of information. Along the way, we show specific functions for parsing data stored in these file types.",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html#strings",
    "href": "19_jsonxml.html#strings",
    "title": "19  JSON + XML",
    "section": "19.2 Strings",
    "text": "19.2 Strings\nThe term string typically refers to a sequence of characters. We have been using variables of string type throughout this book. We have used string variables in several different ways. First, as in Chaps. 2-5, as categories to group, filter, and summarize a dataset. There we treated the string variables as coming from a fixed set of values (such as the name of a region of the United States). In Chap. 5, we noted that when creating data it is important to document and use a consistent set of codes in order facilitate this kind of usage. Another way that we have used strings are as labels or ids. Here, typically, we expect each row of a dataset to have a unique value for a specific variable and we use this value to label points in a graphic. We used this approach in Chap. 2 in connection with geom_text layers and to link together datasets through joins in Chap. 4. Identifiers also were used through the application chapters to identify each document in a textual corpus, nodes in the network data, and the images in our image dataset. In the latter case, strings were also used to indicate the file path where the actual images could be found on our local machine. Another usage of strings came from the application in Chap. 6, where each string recorded long, free-form text that constituted the main object of study in that chapter. We saw that working with the latter required first pre-processing the data through an NLP pipeline to create an annotation table with one row per token.\nThe case we consider here is where strings fall somewhere in-between the free-form text in Chap. 6 and the structured categories in the opening chapters. In other words, each string might contain structured data that needs to be modified before we can use it in data summaries, visualizations, and models. Understanding how to parse apart raw strings is a key step in learning how to collect and organize large datasets. In this and the following section, we will share several techniques for manipulating strings that we will be able to put to use in cleaning data in the subsequent chapters.\nTo illustrate the string manipulation functions presented here, we will again use a dataset created from Wikipedia related to British authors. In the previous examples, we used a relatively small set of 75 authors. Each of the authors was associated with several metadata categories such as the years of birth and death and a category describing the era in which they were active. We created those records manually using the data entry methods described in Chap. 5. The metadata were stored as a CSV file that we then read into Python and then joined with the dataset that had the textual documents. Wikipedia has a much larger list of authors that we could use to create a much larger set of pages. As we will see below, creating the documents for this larger set is relatively straightforward and just involves a longer loop through the set of pages. Grabbing the metadata for each author, however, is a bit more challenging because we want to try to do this automatically rather than entering the thousands of records manually.\nWe have saved one of the intermediate steps in grabbing the larger set of Wikipedia pages as a text file, with one row for each author. (We will show later how to generate this file.) We can load this dataset into Python by reading the file line by line:\n\n# Read lines from file\nwith open(\"data/wiki_uk_long_meta.txt\", \"r\", encoding=\"utf-8\") as f:\n    wiki = f.read().strip().split('\\n')\n\n# Show first 10 lines\nfor i in range(10):\n    print(f\"{i}: {wiki[i]}\")\n\n0: A. W. (fl. 1602), poet\n1: Edwin Abbott Abbott (1838-1926), theologian and novelist\n2: Gilbert Abbott à Beckett (1811-1856), humorist\n3: George Abbot (1562-1633), writer, AV translator and cleric\n4: Kia Abdullah (born 1982), novelist and feature writer\n5: Lascelles Abercrombie (1881-1938), poet and critic\n6: Faridah Àbíké-Íyímídé (born 1998), novelist\n7: Paul Ableman (1927-2006), playwright and novelist\n8: J. R. Ackerley (1896-1967), autobiographer, novelist and playwright\n9: Rodney Ackland (1908-1991), playwright, actor and screenwriter\n\n\nAlready we see that there is quite a lot of interesting metadata in this list. However, it is not sufficiently structured to immediately turn it into a tabular dataset with all of the features as individual columns. As a first step, we will create a DataFrame that contains the text as its single column:\n\nwiki_df = pd.DataFrame({'desc': wiki})\nwiki_df\n\n\n\n\n\n\n\n\ndesc\n\n\n\n\n0\nA. W. (fl. 1602), poet\n\n\n1\nEdwin Abbott Abbott (1838-1926), theologian an...\n\n\n2\nGilbert Abbott à Beckett (1811-1856), humorist\n\n\n3\nGeorge Abbot (1562-1633), writer, AV translato...\n\n\n4\nKia Abdullah (born 1982), novelist and feature...\n\n\n...\n...\n\n\n4231\nBenjamin Zephaniah (born 1958), dub poet\n\n\n4232\nPhilip Ziegler (1929-2023), biographer and his...\n\n\n4233\nAlfred Eckhard Zimmern (1879-1957), classicist...\n\n\n4234\nAlice Zimmern (1855-1939), writer and translator\n\n\n4235\nHelen Zimmern (1846-1934), writer and translator\n\n\n\n\n4236 rows × 1 columns\n\n\n\nIn order to extract the structured information from this text, we need to use specific functions for working with strings. Python has excellent built-in string methods and the re module for regular expressions. All string objects in Python have many useful methods. For example, we can find the length of each string:\n\n# Add string length column\nwiki_df['desc_len'] = wiki_df['desc'].str.len()\nwiki_df[['desc', 'desc_len']]\n\n\n\n\n\n\n\n\ndesc\ndesc_len\n\n\n\n\n0\nA. W. (fl. 1602), poet\n22\n\n\n1\nEdwin Abbott Abbott (1838-1926), theologian an...\n56\n\n\n2\nGilbert Abbott à Beckett (1811-1856), humorist\n46\n\n\n3\nGeorge Abbot (1562-1633), writer, AV translato...\n58\n\n\n4\nKia Abdullah (born 1982), novelist and feature...\n53\n\n\n...\n...\n...\n\n\n4231\nBenjamin Zephaniah (born 1958), dub poet\n40\n\n\n4232\nPhilip Ziegler (1929-2023), biographer and his...\n52\n\n\n4233\nAlfred Eckhard Zimmern (1879-1957), classicist...\n60\n\n\n4234\nAlice Zimmern (1855-1939), writer and translator\n48\n\n\n4235\nHelen Zimmern (1846-1934), writer and translator\n48\n\n\n\n\n4236 rows × 2 columns\n\n\n\nThere are also methods to extract substrings. Python uses zero-based indexing, so the first character is at position 0. We can extract substrings using slice notation:\n\n# Extract first three characters\nwiki_df['desc_first_three'] = wiki_df['desc'].str[:3]\nwiki_df[['desc', 'desc_first_three']]\n\n\n\n\n\n\n\n\ndesc\ndesc_first_three\n\n\n\n\n0\nA. W. (fl. 1602), poet\nA.\n\n\n1\nEdwin Abbott Abbott (1838-1926), theologian an...\nEdw\n\n\n2\nGilbert Abbott à Beckett (1811-1856), humorist\nGil\n\n\n3\nGeorge Abbot (1562-1633), writer, AV translato...\nGeo\n\n\n4\nKia Abdullah (born 1982), novelist and feature...\nKia\n\n\n...\n...\n...\n\n\n4231\nBenjamin Zephaniah (born 1958), dub poet\nBen\n\n\n4232\nPhilip Ziegler (1929-2023), biographer and his...\nPhi\n\n\n4233\nAlfred Eckhard Zimmern (1879-1957), classicist...\nAlf\n\n\n4234\nAlice Zimmern (1855-1939), writer and translator\nAli\n\n\n4235\nHelen Zimmern (1846-1934), writer and translator\nHel\n\n\n\n\n4236 rows × 2 columns\n\n\n\nWe can also extract from the end of strings using negative indices:\n\n# Extract last three characters\nwiki_df['desc_last_three'] = wiki_df['desc'].str[-3:]\nwiki_df[['desc_last_three']]\n\n\n\n\n\n\n\n\ndesc_last_three\n\n\n\n\n0\noet\n\n\n1\nist\n\n\n2\nist\n\n\n3\nric\n\n\n4\nter\n\n\n...\n...\n\n\n4231\noet\n\n\n4232\nian\n\n\n4233\nian\n\n\n4234\ntor\n\n\n4235\ntor\n\n\n\n\n4236 rows × 1 columns\n\n\n\nThere are many other useful string methods for searching and replacing:\n\n# Check if strings contain specific text\nwiki_df['has_and'] = wiki_df['desc'].str.contains('and')\n\n# Count occurrences\nwiki_df['count_and'] = wiki_df['desc'].str.count('and')\n\n# Replace text\nwiki_df['desc_replaced'] = wiki_df['desc'].str.replace('ï', 'o')\n\nwiki_df[['has_and', 'count_and', 'desc_replaced']]\n\n\n\n\n\n\n\n\nhas_and\ncount_and\ndesc_replaced\n\n\n\n\n0\nFalse\n0\nA. W. (fl. 1602), poet\n\n\n1\nTrue\n1\nEdwin Abbott Abbott (1838-1926), theologian an...\n\n\n2\nFalse\n0\nGilbert Abbott à Beckett (1811-1856), humorist\n\n\n3\nTrue\n1\nGeorge Abbot (1562-1633), writer, AV translato...\n\n\n4\nTrue\n1\nKia Abdullah (born 1982), novelist and feature...\n\n\n...\n...\n...\n...\n\n\n4231\nFalse\n0\nBenjamin Zephaniah (born 1958), dub poet\n\n\n4232\nTrue\n1\nPhilip Ziegler (1929-2023), biographer and his...\n\n\n4233\nTrue\n1\nAlfred Eckhard Zimmern (1879-1957), classicist...\n\n\n4234\nTrue\n1\nAlice Zimmern (1855-1939), writer and translator\n\n\n4235\nTrue\n1\nHelen Zimmern (1846-1934), writer and translator\n\n\n\n\n4236 rows × 3 columns\n\n\n\nThese basic string methods are great building blocks for extracting text. However, we need a more flexible approach to parse the Wikipedia data in our example.",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html#regular-expressions",
    "href": "19_jsonxml.html#regular-expressions",
    "title": "19  JSON + XML",
    "section": "19.3 Regular Expressions",
    "text": "19.3 Regular Expressions\nPython’s re module provides powerful regular expression capabilities. A regular expression is a way of describing patterns in strings. The language of regular expressions can become quite complex; here we will focus on a subset of components that are more frequently used for cleaning data.\n\nimport re\n\n# Let's work with a sample of our data\nsample_desc = wiki_df['desc'].iloc[0]\nsample_desc\n\n'A. W. (fl. 1602), poet'\n\n\nThere are several special commands in a regular expression that stand for sets of characters. A period . stands for any character, \\w for word characters (letters and numbers), and \\W for non-word characters such as spaces. Putting any sequence of characters in square brackets will search for any matching character in the set. If the first term in the brackets is the caret ^, this will instead match anything not in the set. There is also a special shorthand, [0-9], to match any digit. If we want to match one or more consecutive characters, we can use the plus sign (+). Let’s see examples:\n\n# Extract patterns using pandas string methods with regex\nwiki_sample = wiki_df.head(10).copy()\n\n# Extract first word character\nwiki_sample['first_word_char'] = wiki_sample['desc'].str.extract(r'(\\w)', expand=False)\n\n# Extract first sequence of word characters\nwiki_sample['first_word'] = wiki_sample['desc'].str.extract(r'(\\w+)', expand=False)\n\n# Extract first sequence of digits\nwiki_sample['first_numbers'] = wiki_sample['desc'].str.extract(r'([0-9]+)', expand=False)\n\n# Extract first sequence of vowels\nwiki_sample['first_vowels'] = wiki_sample['desc'].str.extract(r'([aeiou]+)', expand=False)\n\nwiki_sample[['first_word', 'first_numbers', 'first_vowels']]\n\n\n\n\n\n\n\n\nfirst_word\nfirst_numbers\nfirst_vowels\n\n\n\n\n0\nA\n1602\noe\n\n\n1\nEdwin\n1838\ni\n\n\n2\nGilbert\n1811\ni\n\n\n3\nGeorge\n1562\neo\n\n\n4\nKia\n1982\nia\n\n\n5\nLascelles\n1881\na\n\n\n6\nFaridah\n1998\na\n\n\n7\nPaul\n1927\nau\n\n\n8\nJ\n1896\ne\n\n\n9\nRodney\n1908\no\n\n\n\n\n\n\n\nTwo other special regular expression characters are ^ (start of string) and $ (end of string). These are called anchors and can be very useful for grabbing data within a string.\nNow, we have enough tools to try to extract some of the data that is captured inside of the string. To start, at least in the first ten rows, if we extract all of the text up to the parenthesis, we will have the full name of each author. We can do this by finding text that doesn’t include parentheses:\n\nwiki_sample['author_name'] = wiki_sample['desc'].str.extract(r'([^(]+)', expand=False)\nwiki_sample['author_name'] = wiki_sample['author_name'].str.strip()\n\nwiki_sample[['author_name']]\n\n\n\n\n\n\n\n\nauthor_name\n\n\n\n\n0\nA. W.\n\n\n1\nEdwin Abbott Abbott\n\n\n2\nGilbert Abbott à Beckett\n\n\n3\nGeorge Abbot\n\n\n4\nKia Abdullah\n\n\n5\nLascelles Abercrombie\n\n\n6\nFaridah Àbíké-Íyímídé\n\n\n7\nPaul Ableman\n\n\n8\nJ. R. Ackerley\n\n\n9\nRodney Ackland\n\n\n\n\n\n\n\nWhat if we wanted to extract the last name of each author? This could be useful to create a short name for future plots. We can do this by finding word characters at the end of the author name:\n\n# Extract last names (last word before parentheses)\nwiki_sample['last_name'] = wiki_sample['author_name'].str.extract(r'([\\w.]+)$', expand=False)\n\nwiki_sample[['author_name', 'last_name']]\n\n\n\n\n\n\n\n\nauthor_name\nlast_name\n\n\n\n\n0\nA. W.\nW.\n\n\n1\nEdwin Abbott Abbott\nAbbott\n\n\n2\nGilbert Abbott à Beckett\nBeckett\n\n\n3\nGeorge Abbot\nAbbot\n\n\n4\nKia Abdullah\nAbdullah\n\n\n5\nLascelles Abercrombie\nAbercrombie\n\n\n6\nFaridah Àbíké-Íyímídé\nÍyímídé\n\n\n7\nPaul Ableman\nAbleman\n\n\n8\nJ. R. Ackerley\nAckerley\n\n\n9\nRodney Ackland\nAckland\n\n\n\n\n\n\n\nNow, how about the dates that each author was alive? We can look for four-digit numbers, which should correspond to years:\n\n# Extract birth and death years\n# Find all four-digit numbers (years)\nyears_pattern = r'([0-9]{4})'\n\n# Extract first and last occurrence of four-digit numbers\nwiki_sample['birth_year'] = wiki_sample['desc'].str.extract(years_pattern, expand=False)\nwiki_sample['death_year'] = wiki_sample['desc'].str.extractall(years_pattern).groupby(level=0)[0].last()\n\n# Handle cases where birth and death years are the same\nwiki_sample['is_modern'] = wiki_sample['death_year'].str[:2].isin(['19', '20'])\nwiki_sample.loc[\n    (wiki_sample['birth_year'] == wiki_sample['death_year']) & \n    (~wiki_sample['is_modern']), 'birth_year'\n] = None\nwiki_sample.loc[\n    (wiki_sample['birth_year'] == wiki_sample['death_year']) & \n    (wiki_sample['is_modern']), 'death_year'\n] = None\n\nwiki_sample[['author_name', 'birth_year', 'death_year']]\n\n\n\n\n\n\n\n\nauthor_name\nbirth_year\ndeath_year\n\n\n\n\n0\nA. W.\nNone\n1602\n\n\n1\nEdwin Abbott Abbott\n1838\n1926\n\n\n2\nGilbert Abbott à Beckett\n1811\n1856\n\n\n3\nGeorge Abbot\n1562\n1633\n\n\n4\nKia Abdullah\n1982\nNone\n\n\n5\nLascelles Abercrombie\n1881\n1938\n\n\n6\nFaridah Àbíké-Íyímídé\n1998\nNone\n\n\n7\nPaul Ableman\n1927\n2006\n\n\n8\nJ. R. Ackerley\n1896\n1967\n\n\n9\nRodney Ackland\n1908\n1991\n\n\n\n\n\n\n\nWe can also extract all occurrences of a pattern using findall:\n\n# Extract all vowels from each description\nwiki_sample['all_vowels'] = wiki_sample['desc'].str.findall(r'[AEIOUaeiou]')\n\nwiki_sample[['author_name', 'all_vowels']]\n\n\n\n\n\n\n\n\nauthor_name\nall_vowels\n\n\n\n\n0\nA. W.\n[A, o, e]\n\n\n1\nEdwin Abbott Abbott\n[E, i, A, o, A, o, e, o, o, i, a, a, o, e, i]\n\n\n2\nGilbert Abbott à Beckett\n[i, e, A, o, e, e, u, o, i]\n\n\n3\nGeorge Abbot\n[e, o, e, A, o, i, e, A, a, a, o, a, e, i]\n\n\n4\nKia Abdullah\n[i, a, A, u, a, o, o, e, i, a, e, a, u, e, i, e]\n\n\n5\nLascelles Abercrombie\n[a, e, e, A, e, o, i, e, o, e, a, i, i]\n\n\n6\nFaridah Àbíké-Íyímídé\n[a, i, a, o, o, e, i]\n\n\n7\nPaul Ableman\n[a, u, A, e, a, a, i, a, o, e, i]\n\n\n8\nJ. R. Ackerley\n[A, e, e, a, u, o, i, o, a, e, o, e, i, a, a, i]\n\n\n9\nRodney Ackland\n[o, e, A, a, a, i, a, o, a, e, e, i, e]\n\n\n\n\n\n\n\nThe output shows lists of vowels for each author. This type of structure is called nested data. We can “explode” these lists to create multiple rows per author:\n\n# Explode the vowels to create multiple rows\nvowels_exploded = (wiki_sample[['author_name', 'all_vowels']]\n    .explode('all_vowels')\n    .dropna()\n)\n\nvowels_exploded\n\n\n\n\n\n\n\n\nauthor_name\nall_vowels\n\n\n\n\n0\nA. W.\nA\n\n\n0\nA. W.\no\n\n\n0\nA. W.\ne\n\n\n1\nEdwin Abbott Abbott\nE\n\n\n1\nEdwin Abbott Abbott\ni\n\n\n...\n...\n...\n\n\n9\nRodney Ackland\na\n\n\n9\nRodney Ackland\ne\n\n\n9\nRodney Ackland\ne\n\n\n9\nRodney Ackland\ni\n\n\n9\nRodney Ackland\ne\n\n\n\n\n117 rows × 2 columns\n\n\n\nWe can use similar techniques to extract professions. Let’s look for text after the closing parenthesis:\n\nwiki_sample['professions_raw'] = wiki_sample['desc'].str.extract(r'\\)(.+)$', expand=False)\n\nwiki_sample['professions_clean'] = (wiki_sample['professions_raw']\n    .str.strip()\n    .str.replace(' and ', ', ')\n)\n\nwiki_sample['professions'] = wiki_sample['professions_clean'].str.split(', ')\n\nwiki_prof = (wiki_sample[['author_name', 'professions']]\n    .explode('professions')\n    .dropna()\n    .reset_index(drop=True)\n)\n\nwiki_prof\n\n\n\n\n\n\n\n\nauthor_name\nprofessions\n\n\n\n\n0\nA. W.\n\n\n\n1\nA. W.\npoet\n\n\n2\nEdwin Abbott Abbott\n\n\n\n3\nEdwin Abbott Abbott\ntheologian\n\n\n4\nEdwin Abbott Abbott\nnovelist\n\n\n5\nGilbert Abbott à Beckett\n\n\n\n6\nGilbert Abbott à Beckett\nhumorist\n\n\n7\nGeorge Abbot\n\n\n\n8\nGeorge Abbot\nwriter\n\n\n9\nGeorge Abbot\nAV translator\n\n\n10\nGeorge Abbot\ncleric\n\n\n11\nKia Abdullah\n\n\n\n12\nKia Abdullah\nnovelist\n\n\n13\nKia Abdullah\nfeature writer\n\n\n14\nLascelles Abercrombie\n\n\n\n15\nLascelles Abercrombie\npoet\n\n\n16\nLascelles Abercrombie\ncritic\n\n\n17\nFaridah Àbíké-Íyímídé\n\n\n\n18\nFaridah Àbíké-Íyímídé\nnovelist\n\n\n19\nPaul Ableman\n\n\n\n20\nPaul Ableman\nplaywright\n\n\n21\nPaul Ableman\nnovelist\n\n\n22\nJ. R. Ackerley\n\n\n\n23\nJ. R. Ackerley\nautobiographer\n\n\n24\nJ. R. Ackerley\nnovelist\n\n\n25\nJ. R. Ackerley\nplaywright\n\n\n26\nRodney Ackland\n\n\n\n27\nRodney Ackland\nplaywright\n\n\n28\nRodney Ackland\nactor\n\n\n29\nRodney Ackland\nscreenwriter\n\n\n\n\n\n\n\nLet’s see the most common professions:\n\nprofession_counts = (wiki_prof\n    .groupby('professions')\n    .size()\n    .reset_index(name='count')\n    .sort_values('count', ascending=False)\n)\n\nprofession_counts\n\n\n\n\n\n\n\n\nprofessions\ncount\n\n\n\n\n0\n\n10\n\n\n8\nnovelist\n5\n\n\n9\nplaywright\n3\n\n\n10\npoet\n2\n\n\n1\nAV translator\n1\n\n\n2\nactor\n1\n\n\n3\nautobiographer\n1\n\n\n4\ncleric\n1\n\n\n5\ncritic\n1\n\n\n6\nfeature writer\n1\n\n\n7\nhumorist\n1\n\n\n11\nscreenwriter\n1\n\n\n12\ntheologian\n1\n\n\n13\nwriter\n1\n\n\n\n\n\n\n\nAll of these top terms correspond to the professions we would expect from dataset of authors. While a full cleaning would require more sophisticated regular expressions to handle edge cases, our work illustrates the application of regular expression functions and how they can be used to extract data from raw strings. As we can see, getting data organized is a time consuming process and often the longest part of our humanities data work.",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html#json-data",
    "href": "19_jsonxml.html#json-data",
    "title": "19  JSON + XML",
    "section": "19.4 JSON Data",
    "text": "19.4 JSON Data\nThe JavaScript Object Notation (JSON) is a popular open standard data format. It was originally designed for the JavaScript language, but is supported in many programming languages. The format of JSON closely resembles native data structures found in Python. In part because of the importance of JavaScript as a core programming language in modern browsers, JSON has become a very popular format for data storage. This is particularly true for data that are distributed over a web-based API, as we will see later in this chapter.\nAs with CSV, JSON data is stored in a plaintext format. This means that we can open the file in any text editor and see the data in a human-readable format. We created a small example of a JSON file displaying information about two of the authors in our Wikipedia dataset:\n{\n  \"data\": [\n    {\n      \"name\": \"Charlotte Brontë\",\n      \"age_at_death\": 38,\n      \"date\": {\n        \"born\": \"21 April 1816\",\n        \"died\": \"31 March 1855\"\n      },\n      \"profession\": [\n        \"novelist\",\n        \"poet\",\n        \"governess\"\n      ]\n    },\n    {\n      \"name\": \"Virginia Woolf\",\n      \"age_at_death\": 59,\n      \"date\": {\n        \"born\": \"25 January 1882\",\n        \"died\": \"28 March 1941\"\n      },\n      \"profession\": [\n        \"novelist\",\n        \"essayist\",\n        \"critic\"\n      ]\n    }\n  ]\n}\nData stored in the JSON format is highly structured. In some ways, the format is more strict than CSV files and is relatively easy to parse. Looking at the example above, we see many of the basic element types of JSON data. In fact, there are only six core data types available:\n\nan empty value called null\na number\na string\na Boolean value equal to true or false\nan object of named value pairs, with names equal to strings and values equal to any other data type\nan ordered array of objects coming from any other type\n\nObjects are defined by curly braces and arrays are defined with square brackets. The reason that JSON can become complex even with these limited types is that, as in the example above, it is possible to created nested structures using the object and array types. To read a JSON object into Python, we can use the built-in json module:\n\nimport json\n\n# Read JSON file\nwith open(\"data/author.json\", \"r\") as f:\n    obj_json = json.load(f)\n\nprint(f\"Type: {type(obj_json)}\")\nprint(f\"Keys: {list(obj_json.keys())}\")\n\nType: &lt;class 'dict'&gt;\nKeys: ['data']\n\n\nThe output object obj_json is a Python dictionary. In general, Python turns JSON arrays into lists and objects into dictionaries. Numbers, strings, and Boolean objects become the corresponding Python types. To create a structured dataset from the output, we can use standard Python dictionary and list operations:\n\n# Extract author names\nnames = [author['name'] for author in obj_json['data']]\nprint(f\"Author names: {names}\")\n\n# Create a DataFrame with author information\nauthors_data = []\nfor author in obj_json['data']:\n    authors_data.append({\n        'name': author['name'],\n        'age_at_death': author['age_at_death'],\n        'born': author['date']['born'],\n        'died': author['date']['died']\n    })\n\nmeta = pd.DataFrame(authors_data)\nmeta\n\nAuthor names: ['Charlotte Brontë', 'Virginia Woolf']\n\n\n\n\n\n\n\n\n\nname\nage_at_death\nborn\ndied\n\n\n\n\n0\nCharlotte Brontë\n38\n21 April 1816\n31 March 1855\n\n\n1\nVirginia Woolf\n59\n25 January 1882\n28 March 1941\n\n\n\n\n\n\n\nThe JSON object also associates each author with a set of professions. JSON naturally handles nested structures. To create a dataset mapping each author to all of their professions, we can use pandas’ explode functionality:\n\n# Create professions dataset\nprof_data = []\nfor author in obj_json['data']:\n    name = author['name']\n    for profession in author['profession']:\n        prof_data.append({\n            'name': name,\n            'profession': profession\n        })\n\nprof = pd.DataFrame(prof_data)\nprint(prof)\n\n# Alternative approach using pandas\nauthors_with_prof = pd.DataFrame([\n    {\n        'name': author['name'],\n        'profession': author['profession']\n    } for author in obj_json['data']\n])\n\n# Explode the professions\nprof_alt = authors_with_prof.explode('profession').reset_index(drop=True)\nprof_alt\n\n               name profession\n0  Charlotte Brontë   novelist\n1  Charlotte Brontë       poet\n2  Charlotte Brontë  governess\n3    Virginia Woolf   novelist\n4    Virginia Woolf   essayist\n5    Virginia Woolf     critic\n\n\n\n\n\n\n\n\n\nname\nprofession\n\n\n\n\n0\nCharlotte Brontë\nnovelist\n\n\n1\nCharlotte Brontë\npoet\n\n\n2\nCharlotte Brontë\ngoverness\n\n\n3\nVirginia Woolf\nnovelist\n\n\n4\nVirginia Woolf\nessayist\n\n\n5\nVirginia Woolf\ncritic\n\n\n\n\n\n\n\nIt is often the case that one JSON file needs to be turned into multiple tabular datasets with different primary and foreign keys used to link them together. Deciding what tables to build and how to link them together is the core task of turning JSON data into tabular data. The difficulty of this varies greatly on the level of nesting in the JSON data as well as the consistency from record to record.",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html#xml-and-html-formats",
    "href": "19_jsonxml.html#xml-and-html-formats",
    "title": "19  JSON + XML",
    "section": "19.5 XML and HTML Formats",
    "text": "19.5 XML and HTML Formats\nExtensible Markup Language (XML) is another popular format for transferring and storing data. As with JSON data, the format is quite flexible and typically results in nested, tree-like structures that require some work to turn into a rectangular data format. Much of the formal standards for XML are concerned with describing how other groups can produce specific “extensible” dialects of XML that have consistent names and structures to describe particular kinds of data. Popular open examples include XML-RDF (Resource Description Framework) for describing linked open data and XML-TEI (Text Encoding Initiative) for providing context to textual data.\nThe XML format organizes data inside of hierarchically nested tags. Below is an example of how the data from the previous JSON example could have been stored in an XML dataset:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;root&gt;\n  &lt;author&gt;\n    &lt;name&gt;Charlotte Brontë&lt;/name&gt;\n    &lt;life&gt;\n      &lt;item name=\"born\"&gt;21 April 1816&lt;/item&gt;\n      &lt;item name=\"died\"&gt;31 March 1855&lt;/item&gt;\n    &lt;/life&gt;\n    &lt;ageatdeath&gt;38&lt;/ageatdeath&gt;\n    &lt;professions&gt;\n      &lt;profession&gt;novelist&lt;/profession&gt;\n      &lt;profession&gt;poet&lt;/profession&gt;\n      &lt;profession&gt;governess&lt;/profession&gt;\n    &lt;/professions&gt;\n  &lt;/author&gt;\n  &lt;author&gt;\n    &lt;name&gt;Virginia Woolf&lt;/name&gt;\n    &lt;life&gt;\n      &lt;item name=\"born\"&gt;25 January 1882&lt;/item&gt;\n      &lt;item name=\"died\"&gt;28 March 1941&lt;/item&gt;\n    &lt;/life&gt;\n    &lt;ageatdeath&gt;59&lt;/ageatdeath&gt;\n    &lt;professions&gt;\n      &lt;profession&gt;novelist&lt;/profession&gt;\n      &lt;profession&gt;essayist&lt;/profession&gt;\n      &lt;profession&gt;critic&lt;/profession&gt;\n    &lt;/professions&gt;\n  &lt;/author&gt;\n&lt;/root&gt;\nPython provides several ways to parse XML. We can use the built-in xml.etree.ElementTree module or the more powerful lxml library:\n\nimport xml.etree.ElementTree as ET\n\n# Parse XML file\ntree = ET.parse(\"data/author.xml\")\nroot = tree.getroot()\n\nprint(f\"Root tag: {root.tag}\")\nprint(f\"Number of authors: {len(root.findall('author'))}\")\n\nRoot tag: root\nNumber of authors: 2\n\n\nWe can extract data from XML by navigating the tree structure:\n\n# Extract author information\nauthors_xml = []\nfor author in root.findall('author'):\n    name = author.find('name').text\n    age = int(author.find('ageatdeath').text)\n    \n    # Extract birth and death dates\n    life = author.find('life')\n    born = life.find(\"item[@name='born']\").text\n    died = life.find(\"item[@name='died']\").text\n    \n    authors_xml.append({\n        'name': name,\n        'age_at_death': age,\n        'born': born,\n        'died': died\n    })\n\nmeta_xml = pd.DataFrame(authors_xml)\nmeta_xml\n\n\n\n\n\n\n\n\nname\nage_at_death\nborn\ndied\n\n\n\n\n0\nCharlotte Brontë\n38\n21 April 1816\n31 March 1855\n\n\n1\nVirginia Woolf\n59\n25 January 1882\n28 March 1941\n\n\n\n\n\n\n\nFor professions, we need to handle multiple elements:\n\n# Extract professions\nprof_xml = []\nfor author in root.findall('author'):\n    name = author.find('name').text\n    professions = author.find('professions')\n    \n    for prof in professions.findall('profession'):\n        prof_xml.append({\n            'name': name,\n            'profession': prof.text\n        })\n\nprof_xml_df = pd.DataFrame(prof_xml)\nprof_xml_df\n\n\n\n\n\n\n\n\nname\nprofession\n\n\n\n\n0\nCharlotte Brontë\nnovelist\n\n\n1\nCharlotte Brontë\npoet\n\n\n2\nCharlotte Brontë\ngoverness\n\n\n3\nVirginia Woolf\nnovelist\n\n\n4\nVirginia Woolf\nessayist\n\n\n5\nVirginia Woolf\ncritic\n\n\n\n\n\n\n\nHTML (HyperText Markup Language) is a markup language for describing documents intended to be shown in a web browser. While its primary purpose is not to store arbitrary data, we often need to extract data from HTML documents. Here’s an example HTML file with the same author information:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;title&gt;British Authors&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;British Authors&lt;/h1&gt;\n  &lt;div class=\"author\"&gt;\n    &lt;h1&gt;Charlotte Brontë&lt;/h1&gt;\n    &lt;div id=\"life\" class=\"info\"&gt;\n      &lt;p&gt;Lived from &lt;b id=\"born\"&gt;21 April 1816&lt;/b&gt; to &lt;b id=\"died\"&gt;31 March 1855&lt;/b&gt;.&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;div id=\"profession\" class=\"info box\"&gt;\n      &lt;ul&gt;\n        &lt;li&gt;&lt;i&gt;novelist&lt;/i&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;i&gt;poet&lt;/i&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;i&gt;governess&lt;/i&gt;&lt;/li&gt;\n      &lt;/ul&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;!-- Similar structure for Virginia Woolf --&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nFor HTML parsing, BeautifulSoup is the most popular Python library:\n\nfrom bs4 import BeautifulSoup\n\n# Parse HTML file\nwith open(\"data/author.html\", \"r\", encoding=\"utf-8\") as f:\n    html_content = f.read()\n\nsoup = BeautifulSoup(html_content, 'html.parser')\n\nprint(f\"Title: {soup.title.text}\")\nprint(f\"Number of author divs: {len(soup.find_all('div', class_='author'))}\")\n\nTitle: British Authors\nNumber of author divs: 2\n\n\n\n\n\n\n\n\nFigure 19.1: Example display of the British authors HTML file shown in Firefox.\n\n\n\nWe can extract information using BeautifulSoup’s methods:\n\n# Extract author information from HTML\nauthors_html = []\n\nfor author_div in soup.find_all('div', class_='author'):\n    # Get author name from h1 tag\n    name = author_div.find('h1').text\n    \n    # Get birth and death dates\n    life_div = author_div.find('div', id='life')\n    born = life_div.find('b', id='born').text\n    died = life_div.find('b', id='died').text\n    \n    authors_html.append({\n        'name': name,\n        'born': born,\n        'died': died\n    })\n\nmeta_html = pd.DataFrame(authors_html)\nprint(meta_html)\n\n# Extract professions\nprof_html = []\nfor author_div in soup.find_all('div', class_='author'):\n    name = author_div.find('h1').text\n    \n    # Find profession list\n    prof_ul = author_div.find('div', id='profession').find('ul')\n    \n    for li in prof_ul.find_all('li'):\n        profession = li.find('i').text\n        prof_html.append({\n            'name': name,\n            'profession': profession\n        })\n\nprof_html_df = pd.DataFrame(prof_html)\nprof_html_df\n\n               name             born           died\n0  Charlotte Brontë    21 April 1816  31 March 1855\n1    Virginia Woolf  25 January 1882  28 March 1941\n\n\n\n\n\n\n\n\n\nname\nprofession\n\n\n\n\n0\nCharlotte Brontë\nnovelist\n\n\n1\nCharlotte Brontë\npoet\n\n\n2\nCharlotte Brontë\ngoverness\n\n\n3\nVirginia Woolf\nnovelist\n\n\n4\nVirginia Woolf\nessayist\n\n\n5\nVirginia Woolf\ncritic",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html#xpath-with-lxml",
    "href": "19_jsonxml.html#xpath-with-lxml",
    "title": "19  JSON + XML",
    "section": "19.6 XPath with lxml",
    "text": "19.6 XPath with lxml\nFor more complex XML/HTML parsing, we can use XPath expressions with the lxml library. XPath is a powerful query language for XML documents:\n\nfrom lxml import html, etree\n\n# Parse HTML with lxml\nwith open(\"data/author.html\", \"r\", encoding=\"utf-8\") as f:\n    html_content = f.read()\n\ndoc = html.fromstring(html_content)\n\n# XPath examples\n# Find all bold tags\nbold_texts = doc.xpath(\".//b/text()\")\nprint(f\"Bold texts: {bold_texts}\")\n\n# Find all list items in profession divs\nprof_items = doc.xpath(\".//div[@id='profession']//li/i/text()\")\nprint(f\"Professions: {prof_items}\")\n\n# Find birth dates specifically\nbirth_dates = doc.xpath(\".//b[@id='born']/text()\")\nprint(f\"Birth dates: {birth_dates}\")\n\nBold texts: ['21 April 1816', '31 March 1855', '25 January 1882', '28 March 1941']\nProfessions: ['novelist', 'poet', 'governess', 'novelist', 'essayist', 'critic']\nBirth dates: ['21 April 1816', '25 January 1882']\n\n\nXPath provides very precise control over element selection:\n\n# More complex XPath queries\n# Find all author names (h1 tags within author divs)\nauthor_names = doc.xpath(\".//div[@class='author']/h1/text()\")\nprint(f\"Author names: {author_names}\")\n\n# Find elements containing specific text\nlife_divs = doc.xpath(\".//div[contains(@class, 'info')]\")\nprint(f\"Number of info divs: {len(life_divs)}\")\n\n# Combine XPath with iteration\nfor i, author_div in enumerate(doc.xpath(\".//div[@class='author']\")):\n    name = author_div.xpath(\"./h1/text()\")[0]\n    born = author_div.xpath(\".//b[@id='born']/text()\")[0]\n    print(f\"Author {i+1}: {name} (born {born})\")\n\nAuthor names: ['Charlotte Brontë', 'Virginia Woolf']\nNumber of info divs: 4\nAuthor 1: Charlotte Brontë (born 21 April 1816)\nAuthor 2: Virginia Woolf (born 25 January 1882)",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html#using-an-api",
    "href": "19_jsonxml.html#using-an-api",
    "title": "19  JSON + XML",
    "section": "19.7 Using an API",
    "text": "19.7 Using an API\nAn application programming interface (API) is a generic term for a specific interface that two computers can communicate across. Most commonly, APIs communicate over the internet using the Hypertext Transfer Protocol (HTTP). HTTP is a set of standards describing how different computers can communicate with one another over the internet. It is how the vast majority of the internet communicates; for example, our web browser uses HTTP to communicate with other websites.\nThere are many APIs available online for accessing a variety of different types of data. Often these require first setting up an account with the service. Some APIs require payment for each request, though others offer free access or a free-tier of access for occasional users. Most frequently, data from an API would correspond to some data source which is frequently changing, such as news stories or the weather. Increasingly, though, even static datasets are being put behind an API access rather than allowing a straightforward download of a CSV, JSON, or XML file. Fortunately, it is relatively easy to make and parse API calls from within Python using the requests library.\nWe will show two types of API calls that can be made with the MediaWiki API that runs behind Wikipedia. This API is particularly nice because not only is it freely available, it does not require any signup or access to run. Anyone can call the API directly and get data related to the various MediaWiki projects:\n\nimport requests\nfrom urllib.parse import urlencode\n\n# Build API URL for Wikipedia pageviews\nbase_url = \"https://en.wikipedia.org/w/api.php\"\nparams = {\n    'action': 'query',\n    'format': 'json',\n    'prop': 'pageviews',\n    'titles': 'Emily Brontë'\n}\n\n# Make the API request\nresponse = requests.get(base_url, params=params)\nprint(f\"Status code: {response.status_code}\")\nprint(f\"URL: {response.url}\")\n\nStatus code: 200\nURL: https://en.wikipedia.org/w/api.php?action=query&format=json&prop=pageviews&titles=Emily+Bront%C3%AB\n\n\nParse the JSON response:\n\n# Parse JSON response\ndata = response.json()\n\n# Extract pageview data\npages = data['query']['pages']\npage_id = list(pages.keys())[0]\npageviews = pages[page_id].get('pageviews', {})\n\n# Convert to DataFrame\nif pageviews:\n    pageview_data = []\n    for date, views in pageviews.items():\n        pageview_data.append({\n            'date': date,\n            'views': views,\n            'doc_id': 'Emily Brontë'\n        })\n    \n    page_views_df = pd.DataFrame(pageview_data)\n    print(page_views_df.head())\nelse:\n    print(\"No pageview data available\")\n\n         date   views        doc_id\n0  2025-05-31  1784.0  Emily Brontë\n1  2025-06-01  1881.0  Emily Brontë\n2  2025-06-02  1677.0  Emily Brontë\n3  2025-06-03  1643.0  Emily Brontë\n4  2025-06-04  3250.0  Emily Brontë\n\n\nWe can create a function to get pageviews for multiple authors:\n\ndef get_pageviews(titles_list, max_requests=5):\n    \"\"\"Get pageviews for a list of Wikipedia page titles.\"\"\"\n    all_pageviews = []\n    \n    for i, title in enumerate(titles_list[:max_requests]):  # Limit for demo\n        params = {\n            'action': 'query',\n            'format': 'json',\n            'prop': 'pageviews',\n            'titles': title\n        }\n        \n        try:\n            response = requests.get(base_url, params=params)\n            data = response.json()\n            \n            pages = data['query']['pages']\n            page_id = list(pages.keys())[0]\n            pageviews = pages[page_id].get('pageviews', {})\n            \n            for date, views in pageviews.items():\n                all_pageviews.append({\n                    'doc_id': title,\n                    'date': date,\n                    'views': views\n                })\n                \n        except Exception as e:\n            print(f\"Error fetching data for {title}: {e}\")\n    \n    return pd.DataFrame(all_pageviews)\n\n# Test with a few authors\nauthors = ['Emily Brontë', 'Charlotte Brontë', 'Virginia Woolf']\npageviews_df = get_pageviews(authors)\npageviews_df\n\n\n\n\n\n\n\n\ndoc_id\ndate\nviews\n\n\n\n\n0\nEmily Brontë\n2025-05-31\n1784.0\n\n\n1\nEmily Brontë\n2025-06-01\n1881.0\n\n\n2\nEmily Brontë\n2025-06-02\n1677.0\n\n\n3\nEmily Brontë\n2025-06-03\n1643.0\n\n\n4\nEmily Brontë\n2025-06-04\n3250.0\n\n\n...\n...\n...\n...\n\n\n175\nVirginia Woolf\n2025-07-25\n2824.0\n\n\n176\nVirginia Woolf\n2025-07-26\n3114.0\n\n\n177\nVirginia Woolf\n2025-07-27\n3235.0\n\n\n178\nVirginia Woolf\n2025-07-28\n3129.0\n\n\n179\nVirginia Woolf\n2025-07-29\nNaN\n\n\n\n\n180 rows × 3 columns\n\n\n\nWe can also get the actual page content using a different API endpoint:\n\n# Get page content\ndef get_page_content(title):\n    \"\"\"Get the HTML content of a Wikipedia page.\"\"\"\n    params = {\n        'action': 'parse',\n        'format': 'json',\n        'page': title,\n        'redirects': True\n    }\n    \n    response = requests.get(base_url, params=params)\n    data = response.json()\n    \n    if 'parse' in data:\n        html_content = data['parse']['text']['*']\n        return html_content\n    else:\n        return None\n\n# Get content for Emily Brontë\nemily_html = get_page_content('Emily Brontë')\n\nif emily_html:\n    # Parse with BeautifulSoup\n    soup = BeautifulSoup(emily_html, 'html.parser')\n    \n    # Extract paragraphs\n    paragraphs = soup.find_all('p')\n    print(f\"Found {len(paragraphs)} paragraphs\")\n    \n    # Show first paragraph\n    if paragraphs:\n        print(f\"First paragraph: {paragraphs[0].get_text()[:200]}...\")\n    \n    # Extract links\n    links = soup.find_all('a', href=True)\n    wiki_links = [link['href'] for link in links if link['href'].startswith('/wiki/')]\n    print(f\"\\nFound {len(wiki_links)} internal Wikipedia links\")\n    print(\"First 10 links:\")\n    for link in wiki_links[:10]:\n        print(f\"  {link}\")\n\nFound 45 paragraphs\nFirst paragraph: \n...\n\nFound 386 internal Wikipedia links\nFirst 10 links:\n  /wiki/File:Emily_Bront%C3%AB_by_Patrick_Branwell_Bront%C3%AB_restored.jpg\n  /wiki/Branwell_Bront%C3%AB\n  /wiki/Thornton,_West_Yorkshire\n  /wiki/Yorkshire\n  /wiki/Haworth\n  /wiki/Yorkshire\n  /wiki/St_Michael_and_All_Angels%27_Church,_Haworth\n  /wiki/Yorkshire\n  /wiki/Governess\n  /wiki/Cowan_Bridge_School\n\n\nThis approach allows us to recreate the datasets used throughout Chaps. 6-8. The specific query parameters for different APIs will vary, but the general pattern remains the same: construct the API URL, make the request, parse the response (usually JSON), and convert to a structured format.\n\n# Example of building a larger dataset\ndef build_author_dataset(author_list, get_content=False):\n    \"\"\"Build a comprehensive dataset of author information.\"\"\"\n    dataset = []\n    \n    for author in author_list[:3]:  # Limit for demo\n        print(f\"Processing {author}...\")\n        \n        # Get pageviews\n        pageviews = get_pageviews([author], max_requests=1)\n        \n        # Get content if requested\n        content = None\n        if get_content:\n            content = get_page_content(author)\n        \n        dataset.append({\n            'author': author,\n            'pageviews': pageviews,\n            'content': content\n        })\n    \n    return dataset\n\n# Build dataset\nauthors = ['Emily Brontë', 'Charlotte Brontë', 'Virginia Woolf']\nauthor_data = build_author_dataset(authors, get_content=True)\n\nprint(f\"Built dataset for {len(author_data)} authors\")\nfor i, author_info in enumerate(author_data):\n    print(f\"Author {i+1}: {author_info['author']}\")\n    print(f\"  Pageviews records: {len(author_info['pageviews'])}\")\n    print(f\"  Has content: {author_info['content'] is not None}\")\n\nProcessing Emily Brontë...\nProcessing Charlotte Brontë...\nProcessing Virginia Woolf...\nBuilt dataset for 3 authors\nAuthor 1: Emily Brontë\n  Pageviews records: 60\n  Has content: True\nAuthor 2: Charlotte Brontë\n  Pageviews records: 60\n  Has content: True\nAuthor 3: Virginia Woolf\n  Pageviews records: 60\n  Has content: True",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html#extensions",
    "href": "19_jsonxml.html#extensions",
    "title": "19  JSON + XML",
    "section": "19.8 Extensions",
    "text": "19.8 Extensions\nThere is a lot of information covered in this chapter and many different directions that the material can be extended and built upon. Python provides excellent built-in support for regular expressions through the re module, and there are many more advanced pattern matching techniques worth learning [4].\nFor string processing: - The re module has many more functions like re.finditer() for finding all matches with positions - pandas string methods provide vectorized string operations - Libraries like fuzzywuzzy for fuzzy string matching\nFor JSON processing: - Python’s built-in json module handles most use cases - jsonschema for validating JSON structure - pandas.json_normalize() for flattening nested JSON\nFor XML/HTML processing: - lxml provides the most complete XPath support - BeautifulSoup is excellent for HTML parsing and has a gentle learning curve - html5lib for parsing real-world HTML that may not be well-formed - requests-html combines requests with PyQuery for JavaScript-heavy sites\nFor API interaction: - requests is the standard library for HTTP requests - httpx for async HTTP requests - requests-cache for caching API responses - Authentication libraries for OAuth, API keys, etc.\nMany resources exist for extending these methods. The requests documentation (docs.python-requests.org) is excellent for API work. For XML/HTML processing, the BeautifulSoup documentation (beautiful-soup-4.readthedocs.io) and lxml documentation provide comprehensive guides. When combined with the practical information in this chapter, these resources should enable you to work with almost any text-based data format or web API.\nThe benefits of Python’s rich ecosystem make it possible to handle complex data extraction tasks efficiently. Libraries like scrapy for large-scale web scraping, selenium for JavaScript-heavy sites, and pandas for data manipulation provide a complete toolkit for gathering and organizing humanities data from diverse sources.\nAs we’ve seen throughout this chapter, collecting and organizing data is time consuming and often requires careful attention to detail. However, the tools and techniques covered here provide a solid foundation for working with the wide variety of data formats encountered in digital humanities research.",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "19_jsonxml.html#references",
    "href": "19_jsonxml.html#references",
    "title": "19  JSON + XML",
    "section": "References",
    "text": "References\n\n\n\n\n[1] Wickham, H, Miller, E and Smith, D (2023 ). Haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://CRAN.R-project.org/package=haven\n\n\n[2] Mühleisen, H and Raasveldt, M (2023 ). Duckdb: DBI Package for the DuckDB Database Management System. https://CRAN.R-project.org/package=duckdb\n\n\n[3] Ooms, J, James, D, DebRoy, S, Wickham, H and Horner, J (2023 ). RMySQL: Database Interface and ’MySQL’ Driver for r. https://CRAN.R-project.org/package=RMySQL\n\n\n[4] Goyvaerts, J and Levithan, S (2012 ). Regular Expressions Cookbook. O’reilly",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>JSON + XML</span>"
    ]
  },
  {
    "objectID": "20_databases.html",
    "href": "20_databases.html",
    "title": "20  Databases",
    "section": "",
    "text": "References",
    "crumbs": [
      "Part IV: Programming",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Databases</span>"
    ]
  }
]