# Introduction {#sec-ch01}

```{python}
#| include: false
import pandas as pd
import numpy as np
from plotnine import *
```

## Introduction

In this book, we focus on tools and techniques for exploratory data analysis, or EDA.
Initially described in John Tukey's classic text by the same name, EDA is a general
approach to examining data through visualizations and broad summary statistics
[@ch1:tukey1977exploratory] [@brillinger2002john].
It prioritizes studying data directly in order to generate hypotheses
and ascertain general trends prior to, and often in lieu of, formal
statistical modeling. The growth in both data volume and complexity has
further increased the need for a careful application of these exploratory
techniques. In the intervening years, techniques for EDA have enjoyed great
popularity within statistics, computer science, and many other data-driven
fields and professions.

The histories of data programming and EDA are deeply entwined.
Concurrent with Tukey's development of EDA, Rick Becker, John Chambers, and
Allan Wilks of Bell Labs began developing software designed specifically
for statistical computing. By 1980, the 'S' language was
released for general distribution outside Labs. It was followed by a
popular series of books and updates, including 'New S' and 'S-Plus'
[@ch1:becker1984s] [@ch1:becker1985extending] [@ch1:becker1988new] [@ch1:chambers1991statistical].
In the early 1990s, Ross Ihaka and Robert Gentleman produced a fully open-source
implementation of S called 'R'. It is called 'R' for it is both the
"previous letter in the alphabet" and the shared initial in the authors' names.
Their implementation became the de-facto tool in the field of statistics. More
recently, many of the best ideas from R have been extended to other general
purpose programming languages such as Python and JavaScript.

## Data Science Ecosystem

The methods of EDA ideas have been successfully adopted and extended by the
Python programming language through a rich ecosystem of data science libraries.
Python, originally created by Guido van Rossum  in 1991, has evolved into one of
the most popular programming languages for data analysis, machine learning, and
scientific computing. **Pandas**, developed by Wes McKinney starting in 2008,
brought R-like data structures and manipulation capabilities to Python 
[@mckinney2022python]. **Matplotlib** and later **Seaborn** provided
comprehensive plotting capabilities, while **plotnine** specifically
implemented the grammar of graphics approach pioneered by **ggplot2** in R.
More recently, **polars** has emerged as a high-performance alternative to
pandas for large-scale data manipulation.

This Python ecosystem maintains the philosophy behind EDA:
prioritizing interactive exploration, readable code, and the ability to
seamlessly move between data manipulation, visualization, and analysis.
We see this book as contributing to efforts to bring new communities to learn
from and to help shape data analysis by offering the humanities and humanistic
social sciences powerful tools for data-driven inquiry. A visual summary of
the steps of EDA are shown in Fig. 1.1. We will see that the core chapters
in this text map onto the steps outlined in the diagram.

## Setup

While it is possible to read this book as a conceptual text, we expect that
the majority of readers will eventually want to follow along with the code and
examples that are given throughout the text. The first step in doing so is to
obtain a working copy of Python with the necessary data science libraries. 

For readers new to programming, we recommend gettings started by using Google
Colab. It gives free access to a working Python session with no setup required.
For users comfortable with the command-line, the [uv]() library is extremely
powerful and reduces many of the pain-points that other methods of setting up
Python can have.

In addition to the Python software, walking through the examples in this text
requires access to the datasets we explore. Care has been taken to ensure that
these are all in the public domain so as to make it easy for us to redistribut
to readers. The materials and download instructions can be downloaded through
the associated notebooks at the start of each chapter.

A major selling point of Python is its extensive collection of user-contributed
libraries, available through the Python Package Index (PyPI). Details of how to
install packages are included in the supplemental materials, as well as through
examples in the notebooks. Like Python itself, all the
packages used here are free and open-source software, thanks to a robust
community dedicated to developing and expanding Python's data science
capabilities.

As mentioned in the preface, we make heavy use in this text of several key
Python packages: **pandas** for data manipulation, **plotnine** for visualization
using the grammar of graphics, **numpy** for numerical computing, and
**matplotlib** for additional plotting capabilities. We may also use
specialized packages like **scipy** for statistical functions and
**scikit-learn** for machine learning tasks.

Learning to program is hard and invariably questions and issues will arise
in the process (even the most experienced users require help with surprisingly
high frequency). As a first source of help, searching a question or error
message in an search engine or chat-based generative AI software
will often pull up a solution directly related to your question.
If we cannot find an immediate
answer to a question, the next best step is to find some local, in-person
help. While we have done our best with this static text to explain the 
concepts for working with Python, nothing beats talking to a real-life person.
As a final step, we could post questions directly on third-party
sites. It may take a few days to get a response, but usually someone helpful
from the Python community will answer. We invite everyone to participate in the
community by being active on forums, contributing packages, and supporting
colleagues and friends. There are also great groups like PyLadies
([pyladies.com](pyladies.com)) and local Python user groups that can provide further connections.

![Diagram of the process of exploratory data analysis.](img/diagram_eda.png){#fig-elephant .lightbox}

## Working with Notebooks

The supplemental materials for this book include all the data and code
needed to replicate all of the analyses and visualizations in this book. 
We include the exact same code that will be printed in
the book. We have used Jupyter notebooks (with an `.ipynb`
extension) to store this code, with a file corresponding to each chapter
in the text. Jupyter notebooks are an excellent choice for data analysis
because they allow us to mix code, visualizations, and explanations within the
same file [@perez2007ipython]. In fact, the entire data science workflow—from
initial exploration through final presentation—can be contained within a single
notebook. Furthermore, Jupyter notebooks can be directly executed from Google
Colab, making it possible to do the associated notebooks directly through a 
one-click link.

The Jupyter environment offers a convenient format for viewing and
editing notebooks. When we open a Jupyter notebook, we see an interface
with cells that can contain either code or markdown text. Running a code cell
executes the Python code and displays the output directly below the cell.
This interactive approach is ideal for exploratory data analysis because we
can experiment with different approaches and immediately see the results.
Looking at a Jupyter notebook, we'll see that it consists of cells that contain
either code or formatted text (markdown). Code cells have a gray background
and can be executed by clicking the run button or pressing Shift+Enter.
When we run code to read or create a new dataset, we can examine the data by
simply typing the variable name in a cell.

## Running Python Code

Now, let's see some examples of how to run Python code. In this book, we
will show snippets of Python code and the output. Though, know that we should
think of each of the snippets as occurring inside of a code cell in a Jupyter
notebook. In one of its most basic forms, Python can be used as a fancy
calculator. We can add 1 and 1 by typing `1+1` into a code cell.
Running the cell will display the output (`2`) below.
In the book, we will write this code and output using a black box with the Python
code written inside of it. Any output will be shown below. An example is
given below.

```{python}
1 + 1
```

In addition to just returning a value, running Python code can also result
in storing values through the creation of new variables. Variables in
Python are used to store anything—such as numbers, datasets, functions, or 
models—that we want to use again later. Each variable has a name associated
with it that we can use to access it in future code. To create a variable,
we use the `=` (equals) symbol with the name on the left-hand side of
the equals sign and code that produces the value on the right-hand side. For
example, we can create a new variable called `mynum` with a value of `8` by
running the following code.

```{python}
mynum = 3 + 5
```

Notice that the code here did not print any results because the result
was saved as a new variable. We can now use our new variable `mynum` exactly
the same way that we would use the number 8. For example, adding it to 1
to get the number nine:

```{python}
mynum + 1
```

Variable names must start with a letter or underscore, but can also use numbers
after the first character. We recommend using only lowercase letters
and underscores. That makes it easier to read the code later on without
needing to remember if and where we used capital letters.

## Functions in Python

A function in Python is something that takes a set of input values and returns
an output value. Generally, a function will have a format similar to that
given in the code here:

```{python}
#| eval: false
function_name(arg1=input1, arg2=input2)
```

Where `arg1` and `arg2` are the names of the inputs to the function (they
are fixed) and `input1` and `input2` are the values that we will assign
to them. The number of arguments is not always two, however. There may be any
number of arguments, including zero. Also, there may be additional optional
arguments that have default values that can be modified. Let us look at an
example function: `round`. This function returns a rounded version of a number.
We can give the function a single number, which returns the nearest integer to
it. For example, here is the rounded value of π:

```{python}
round(3.14159)
```

The function has an additional optional parameter giving the number of significant
digits. So, for example, this will round π instead to 2 digits:

```{python}
round(3.14159, 2)
```

An alternative way to call the same function in the same way is to use
*named arguments*, where the values of the two inputs are given by name rather
than position:

```{python}
round(number=3.14159, ndigits=2)
```

How did we know the inputs to each function and what they do? In this
text, we will explain the names and usage of the required inputs to
new functions as they are introduced. In order to learn more about 
all of the possible inputs to a function, we can look at a function's
documentation. Python has excellent built-in documentation that can be
accessed using the `help()` function or, in Jupyter notebooks, by using
a question mark after the function name.

```{python}
#| eval: false
help(round)
# or in Jupyter: round?
```

Many Python libraries also have extensive online documentation. For example,
pandas has comprehensive documentation at [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/).

We will learn how to use numerous functions in the coming chapters, each of
which will help us in exploring and understanding data. In order to do this,
we need to first load our data into Python, which we will show in the next section.

## Loading Data in Python

In this book we will be working with data that is stored in a tabular
format. Fig. 1.2 shows an example of a tabular dataset 
consisting of information about metropolitan regions in the United States
supplied by the US Census Bureau. These regions are called core-based
statistical areas, or CBSA. In the figure we can see rows and
columns. Each row of the dataset represents a particular metropolitan
region. We call each of the rows an *observation*. 
The columns in a tabular dataset represent the measurements that we
record for each observation. These measurements are called *variables*.

In our example dataset, we have five variables which record the name
of the region, the quadrant of the country that the region exists in,
the population of the region in millions of people, the density given in
tens of thousands of people per square kilometer, and the median age
of all people living in the region. More details are given in the following
section.

![Example of a tabular dataset.](img/tidy.png){#fig-img-tidy .lightbox}

A larger version of this dataset, with more regions and variables, is
included in the book's supplemental materials as a comma separated value (CSV)
file. We will make extensive use of this dataset in the following chapters as a
common example for creating visualizations and performing data manipulation. In
order to read in the dataset we use the function `pd.read_csv()` from the
**pandas** package [@pandas]. In order make the functions from **pandas**
available, we need to import it. We will use the standard convention of importing
pandas as `pd` and numpy as `np`.

```{python}
import pandas as pd
import numpy as np
```

We call the `pd.read_csv()` function with the path to
where the file is located relative to where this script is stored. If we
are running the Jupyter notebooks from the supplemental materials, the
data will be called `acs_cbsa.csv` and will be stored in a folder called
`data`. The following code will load the CBSA dataset into Python, save it as
a variable called `cbsa`, and print out the first several rows. The output
dataset is stored as a type of Python object called a *DataFrame*.

```{python}
cbsa = pd.read_csv("data/acs_cbsa.csv")
cbsa
```

Notice that the display shows that there are a total of 934 rows and
13 columns. Or, with our terms defined above, there are 934 observations
and 13 variables. Only the first few and last few observations are shown in the
output, along with information about the shape of the DataFrame.
We can use various methods to explore the data, such as `cbsa.head()` to see
the first few rows. For example, the method `cbsa.describe()` will given
additional summary statistics about the numerical variables.

```{python}
cbsa.describe()
```

The method `cbsa.info()` will give information about the data types in each
column.

```{python}
cbsa.info()
```

The data types shown by pandas tell us the
types of data stored in each column. The type `object` typically indicates
text data (strings), such as the `name`, `quad` (quadrant), and `division`
columns. String data can consist of any sequence of
letters, numbers, spaces, and punctuation marks. String variables are
often used to represent fixed categories, such as the quadrant and
division of each CBSA region. They can also provide unique identifiers 
and descriptions for each row, such as the name of the CBSA region in our
example.

The other data types we see are numeric types like `int64` and `float64`,
which indicate that a column contains integer or floating-point numeric data.
While there are technical differences between these types (integers are whole
numbers, floats can have decimal places), we will refer to any variable of either 
type as *numeric* data for our purposes.

Knowing the types of data for each column is important because, as we
will see throughout the book, they will affect the kinds of visualizations
and analysis that can be applied. The data types in the DataFrame are 
automatically determined by the `pd.read_csv()` function. Optional
arguments like `dtype` can be set to specify alternatives, or we can
modify data types after the DataFrame has been created using techniques
shown in Chapter 3. The string and numeric data types are by
far the most common. Other possible options are explored in Chapter 7
(dates and times), Chapter 9 (spatial variables), and
Chapter 11 (lists and logical values).

## Datasets

Throughout this book, we will use multiple datasets to illustrate different
concepts and show how each approach can be used across multiple application
domains. While we will briefly reintroduce new datasets as they appear, for
readers making their way selectively through the text, we offer
a somewhat more detailed description of the main datasets that we will use in this section.

To introduce the concept of EDA, 
we will make sustained use of the CBSA dataset in Chapters 2-5
to demonstrate new concepts in data visualization and manipulation. As
described above, the data comes from an annual survey conducted by the United
States Census Bureau called the American Community Survey (ACS). The survey 
consists of data collected from a sample of 3.5 million households in the
United States. Outside of the constitutionally mandated decennial census, this
is the largest survey completed by the Census Bureau. It asks several dozen
questions covering topics such as gender, race, income, housing, education,
and transportation. Aggregated data are released on a regular schedule, with
summaries over one-, three-, and five-year periods. Our data comes from the
five-year summary from the most recently published version (2021) at the time of
writing. We selected a small set of measurements that we felt did not
require extensive background knowledge, while capturing variations across the county.
As seen in the table above, we have selected the median age, median household
income (USD), the percentage of households owning their housing, the median
rent for a one-bedroom apartment (USD), and the median household spending on
rent. 

The American Community Survey aggregates data to a variety of different
geographic regions. Most regions correspond to political boundaries, such as
states, counties, and cities. One particularly interesting geographic region 
are the core-based statistical areas, or CBSA. These regions, of which there are
nearly a thousand, are defined by the United States Office of Management and
Budget. Regions are defined in the documentation as "an area containing a
large population nucleus and adjacent communities that have a high degree of
integration with that nucleus." We chose these regions for our dataset because
their social, rather than political, definition makes them particularly well
suited for humanities research questions. Our dataset includes a short, common
name for each CBSA, as well as a unique identifier (`geoid`), and several 
geographic categorizations derived from spatial data provided by the Census Bureau.

The core chapters of the book also make use of a dataset illustrating the 
relative change in the price of various food items for over 140 years in the
United States. This collection was published as-is by Davis S. Jacks for his
publication "From boom to bust: a typology of real commodity prices in the
long run" [@ch02:jacks2019boom]. The data is organized with one observation
per year, and variables capturing the relative price of each of thirteen 
food commodities. We can read this dataset into Python using the same function 
that we used for the CBSA dataset, shown below.

```{python}
food_prices = pd.read_csv("data/food_prices.csv")
food_prices
```

All of the prices are given on a relative scale where $100$ is equal to the
price in 1900. We will use this dataset to show how to build data visualizations
that show change over time. It will also be useful for our study of table pivots
in Chapter 4.

Part II turns to data types. The first three application chapters 
focus on text analysis, temporal analysis, and network analysis, respectively. 
While these three chapters introduce
different methods, we will make use of a consistent core dataset across all 
three that we have created from Wikipedia. Specifically, we have a dataset 
consisting of the text, links, page views, and change histories of a set
of 75 Wikipedia pages sampled from a set of British authors. These data are 
contained in several different tables, each of which will be introduced as 
needed. The main metadata for the set of 75 pages is shown in the data loaded
by the following code.

```{python}
meta = pd.read_csv("data/wiki_uk_meta.csv.gz")
meta
```

We decided to use Wikipedia data because it is freely available and can be 
easily generated in the same format for other collection of pages that 
correspond to nearly any other topic of interest. Wikipedia is also helpful 
because it allows us look at pages in other languages, which will allow us
to demonstrate how to extend our techniques to texts that are not in English.
Finally, we will return to the Wikipedia data in Chapter 12 to demonstrate how
to build a dataset (specifically, this one) by calling an API from within Python 
using the **requests** library.

Several other datasets will be used throughout the book within a single chapter.
For example, Chapter 9 on spatial data makes use of a dataset showing the location
of French cities and Parisian metro stops as a source in our study of geographic data.
Chapter 10 on image data shows
a collection of documentary photographs and associated metadata in our analysis
of images. As these datasets are used only a single point in the book, we will
introduce them in more detail as they are introduced.

## Formatting Python Code

It is very important to properly format Python code in a consistent way.
Even though the code may run without errors and produce the desired results,
keeping the code well-formatted will make it easier to read and debug. We will
follow the following guidelines throughout this book (based on PEP 8, Python's
official style guide):

1. one space before and after an equals sign and around operators
2. except: no spaces around the equals sign in a function argument
3. one space after a comma, but no space before a comma
4. one space around mathematical operations (such as `+` and `*`)
5. if a line of code becomes too long, split it across multiple lines with
proper indentation (typically 4 spaces)
6. use lowercase with underscores for variable names (snake_case)

We have found it makes our life a lot easier if we use these rules right from the
start and whenever we are writing Python code.

## Extensions

Each chapter in this book contains a short, concluding section of extensions
on the main material. These include references for further study, additional
Python packages, and other suggested methods that may be of interest to the study
of each specific type of humanities data.

In this chapter, we will mention a few standard Python references that might
be useful
to use in parallel or in sequence with our text. The classic introduction to the
Python language is *Learning Python* by Mark Lutz [@lutz2013learning]. For those
specifically interested in data science applications, *Python for Data Analysis*
by Wes McKinney (the creator of pandas) provides comprehensive coverage of the
core data science libraries [@mckinney2022python]. This book covers pandas, numpy,
matplotlib, and related tools in great detail.

For the specific approach to data analysis that we follow in this book, 
*Python Data Science Handbook* by Jake VanderPlas is an excellent reference
[@vanderplas2016python]. It covers the full stack of data science tools in Python,
from basic data manipulation through machine learning. The book is also available
freely online.

When working through the code in this book's supplemental materials, as mentioned
above, we will be using Jupyter notebooks. More information about
Jupyter and what can be done with it can be found in the *Jupyter Notebook documentation*
and various online tutorials. The philosophy behind interactive computing can be found
in research on computational notebooks and reproducible research [@perez2007ipython].

For those interested in the grammar of graphics approach to visualization that we
use throughout this book, *The Grammar of Graphics* by Leland Wilkinson provides
the theoretical foundation [@wilkinson2012grammar]. The **plotnine** library
implements these concepts in Python, closely following the **ggplot2** implementation
from R.

## References {-}
